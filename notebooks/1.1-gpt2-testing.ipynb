{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9cd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run if working locally\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "993755fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 148;\n",
       "                var nbb_unformatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\";\n",
       "                var nbb_formatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pickle\n",
    "import os, sys\n",
    "import config\n",
    "\n",
    "config.root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, config.root_path)\n",
    "\n",
    "from db.dbv2 import Table, AugmentedTable, TrainTestTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ab257230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 149;\n",
       "                var nbb_unformatted_code = \"from src.dataset.gpt_augmentor import Augmentor\\nfrom src.dataset.utils import (\\n    truncate_by_token,\\n    avg_segment_length_by_char,\\n    avg_segment_length_by_token,\\n)\\n\\nfrom nltk.tokenize import word_tokenize\";\n",
       "                var nbb_formatted_code = \"from src.dataset.gpt_augmentor import Augmentor\\nfrom src.dataset.utils import (\\n    truncate_by_token,\\n    avg_segment_length_by_char,\\n    avg_segment_length_by_token,\\n)\\n\\nfrom nltk.tokenize import word_tokenize\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.dataset.gpt_augmentor import Augmentor\n",
    "from src.dataset.utils import (\n",
    "    truncate_by_token,\n",
    "    avg_segment_length_by_char,\n",
    "    avg_segment_length_by_token,\n",
    ")\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1bd993d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 150;\n",
       "                var nbb_unformatted_code = \"dataset_type = \\\"committee\\\"\\n\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_formatted_code = \"dataset_type = \\\"committee\\\"\\n\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_type = \"committee\"\n",
    "\n",
    "table = Table(dataset_type)\n",
    "augmented_table = AugmentedTable(dataset_type)\n",
    "train_test_table = TrainTestTable(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "996c5ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 151;\n",
       "                var nbb_unformatted_code = \"target_sentences_original = table.get_target_sentences()\\ntarget_sentences = [s[1] for s in target_sentences_original]\";\n",
       "                var nbb_formatted_code = \"target_sentences_original = table.get_target_sentences()\\ntarget_sentences = [s[1] for s in target_sentences_original]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_sentences_original = table.get_target_sentences()\n",
    "target_sentences = [s[1] for s in target_sentences_original]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "345004b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 152;\n",
       "                var nbb_unformatted_code = \"# Take the first 5 target sentences for testing\\ntest_target_sentences = target_sentences_original[:5]\";\n",
       "                var nbb_formatted_code = \"# Take the first 5 target sentences for testing\\ntest_target_sentences = target_sentences_original[:5]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take the first 5 target sentences for testing\n",
    "test_target_sentences = target_sentences_original[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d73cbb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 153;\n",
       "                var nbb_unformatted_code = \"avg_segment_length_by_char([\\\"one\\\", \\\"two\\\", \\\"to\\\"], floor=True)\";\n",
       "                var nbb_formatted_code = \"avg_segment_length_by_char([\\\"one\\\", \\\"two\\\", \\\"to\\\"], floor=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_segment_length_by_char([\"one\", \"two\", \"to\"], floor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb68118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# example segment\\ntest_segments = []\\n\\nfor target_sentence in test_target_sentences:\\n    segment = table.get_segment(target_sentence[0])\\n    test_segments.append([s[1] for s in segment])\";\n",
       "                var nbb_formatted_code = \"# example segment\\ntest_segments = []\\n\\nfor target_sentence in test_target_sentences:\\n    segment = table.get_segment(target_sentence[0])\\n    test_segments.append([s[1] for s in segment])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example segment\n",
    "test_segments = []\n",
    "\n",
    "for target_sentence in test_target_sentences:\n",
    "    segment = table.get_segment(target_sentence[0])\n",
    "    test_segments.append([s[1] for s in segment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "573092e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"len(test_segments)\";\n",
       "                var nbb_formatted_code = \"len(test_segments)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(test_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4e41de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"test_sentence = target_sentences[0]\\ntest_segment = test_segments[0]\";\n",
       "                var nbb_formatted_code = \"test_sentence = target_sentences[0]\\ntest_segment = test_segments[0]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_sentence = target_sentences[0]\n",
    "test_segment = test_segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aef2ab7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 104;\n",
       "                var nbb_unformatted_code = \"test_segments = [\\n    [\\n        \\\"this is one sentence\\\",\\n        \\\"about donald trump\\\",\\n        \\\"and pakistan\\\",\\n        \\\"i am not a republican\\\",\\n        \\\"rather I am a democrat\\\",\\n    ],\\n#     [\\n#         \\\"welcome to fight club\\\",\\n#         \\\"where there are no rules\\\",\\n#         \\\"except 1 rule\\\",\\n#         \\\"you don't speak about fight club\\\",\\n#     ],\\n#     [\\\"i am not a republican\\\", \\\"rather I am a democrat\\\", \\\"I love Elon Musk\\\"],\\n]\";\n",
       "                var nbb_formatted_code = \"test_segments = [\\n    [\\n        \\\"this is one sentence\\\",\\n        \\\"about donald trump\\\",\\n        \\\"and pakistan\\\",\\n        \\\"i am not a republican\\\",\\n        \\\"rather I am a democrat\\\",\\n    ],\\n    #     [\\n    #         \\\"welcome to fight club\\\",\\n    #         \\\"where there are no rules\\\",\\n    #         \\\"except 1 rule\\\",\\n    #         \\\"you don't speak about fight club\\\",\\n    #     ],\\n    #     [\\\"i am not a republican\\\", \\\"rather I am a democrat\\\", \\\"I love Elon Musk\\\"],\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_segments = [\n",
    "    [\n",
    "        \"this is one sentence\",\n",
    "        \"about donald trump\",\n",
    "        \"and pakistan\",\n",
    "        \"i am not a republican\",\n",
    "        \"rather I am a democrat\",\n",
    "    ],\n",
    "    #     [\n",
    "    #         \"welcome to fight club\",\n",
    "    #         \"where there are no rules\",\n",
    "    #         \"except 1 rule\",\n",
    "    #         \"you don't speak about fight club\",\n",
    "    #     ],\n",
    "    #     [\"i am not a republican\", \"rather I am a democrat\", \"I love Elon Musk\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a0066",
   "metadata": {},
   "source": [
    "## GTA 1\n",
    "\n",
    "- Using GPT-2, we take the first truncated portion of the first sentence in a segment and feed it into the model. The output should be the same size as the the overall dataset average sentence length.\n",
    "- We then take that output sentence as the first sentence in the augmented segment\n",
    "- Using that newly augmented sentence, we feed it into GPT again to generate a new sentence of the same size.\n",
    "- We do this autoregressive process for `n` times.\n",
    "    - For experimentation, we do `n = k/2` where `k` is the average segment size in the dataset.\n",
    "\n",
    "- On average, we will have about half the amount of total data in our augmented dataset than our real dataset\n",
    "\n",
    "Note: By default, GPT is autoregressive, so instead of having to re-run GPT on every sentence that's generated, just run it once and multiply the output_tokens by `n` to get the desired sentences. Afterward, the post-processing will need to chop the initial sentence off and break the complete output sentence into its relative sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5515d999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is a sentence.\\n\\n and another one'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 126;\n",
       "                var nbb_unformatted_code = \"def post_augmentation_processing(sentence, real_sentence_chars, n):\\n    return sentence[real_sentence_chars:]\\n\\n\\npost_augmentation_processing(\\\"this is a sentence.\\\\n\\\\n and another one\\\", 4, 0)\";\n",
       "                var nbb_formatted_code = \"def post_augmentation_processing(sentence, real_sentence_chars, n):\\n    return sentence[real_sentence_chars:]\\n\\n\\npost_augmentation_processing(\\\"this is a sentence.\\\\n\\\\n and another one\\\", 4, 0)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def post_augmentation_processing(sentence, real_sentence_chars, n):\n",
    "    return sentence[real_sentence_chars:]\n",
    "\n",
    "\n",
    "post_augmentation_processing(\"this is a sentence.\\n\\n and another one\", 4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "410afa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed augmentation...\n",
      "augmented_segment [' of the book: \"The']\n",
      "completed augmentation...\n",
      "augmented_segment [' of the book: \"The', ' Book of Mormon, by Joseph']\n",
      "completed augmentation...\n",
      "augmented_segment [' of the book: \"The', ' Book of Mormon, by Joseph', 'Smith.\\n\\n, by']\n",
      "completed augmentation...\n",
      "augmented_segment [' of the book: \"The', ' Book of Mormon, by Joseph', 'Smith.\\n\\n, by', '. L. Sacks']\n",
      "completed augmentation...\n",
      "augmented_segment [' of the book: \"The', ' Book of Mormon, by Joseph', 'Smith.\\n\\n, by', '. L. Sacks', ', R. J. P']\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 127;\n",
       "                var nbb_unformatted_code = \"# GTA 1 POC\\ndataset_avg_sentence_length = sum(\\n    [avg_segment_length_by_token(segment, floor=True) for segment in test_segments]\\n) // len(test_segments)\\n\\navg_segment_length = sum([len(t) for t in test_segments]) // len(\\n    test_segments\\n)  # avg number of sentences per segment in the dataset\\nn = int(avg_segment_length / 2)  # the number of sentences we will generate per segment\\n\\nmax_sent_tokens = 64\\naugmented_segments = []\\n\\nfor segment in test_segments:\\n    first_sentence = truncate_by_token(segment[0], max_sent_tokens)\\n\\n    augmented_segment = []\\n    for i in range(0, avg_segment_length):\\n        next_sentence = (\\n            segment[0] if len(augmented_segment) == 0 else augmented_segment[-1]\\n        )\\n        next_sentence = \\\" \\\".join(word_tokenize(next_sentence)[:max_sent_tokens])\\n        next_sentence_length = len(next_sentence)\\n        sentence_tokens_length = len(word_tokenize(next_sentence))\\n        # create segment\\n        augmented_sentence = Augmentor.augment_gpt2_single(\\n            next_sentence,\\n            fast=True,\\n            # add the length of the current sentence to the dataset avg length of sentence\\n            output_tokens=int(n * int(dataset_avg_sentence_length)),\\n            num_return_sequences=1,\\n        )\\n\\n        augmented_sentence = post_augmentation_processing(\\n            # feed in the first generated sentence\\n            augmented_sentence[0][0],\\n            next_sentence_length,\\n            n,\\n        )\\n\\n        augmented_segment.append(augmented_sentence)\\n        print(\\\"augmented_segment\\\", augmented_segment)\\n\\n    augmented_segments.append(augmented_segment)\";\n",
       "                var nbb_formatted_code = \"# GTA 1 POC\\ndataset_avg_sentence_length = sum(\\n    [avg_segment_length_by_token(segment, floor=True) for segment in test_segments]\\n) // len(test_segments)\\n\\navg_segment_length = sum([len(t) for t in test_segments]) // len(\\n    test_segments\\n)  # avg number of sentences per segment in the dataset\\nn = int(avg_segment_length / 2)  # the number of sentences we will generate per segment\\n\\nmax_sent_tokens = 64\\naugmented_segments = []\\n\\nfor segment in test_segments:\\n    first_sentence = truncate_by_token(segment[0], max_sent_tokens)\\n\\n    augmented_segment = []\\n    for i in range(0, avg_segment_length):\\n        next_sentence = (\\n            segment[0] if len(augmented_segment) == 0 else augmented_segment[-1]\\n        )\\n        next_sentence = \\\" \\\".join(word_tokenize(next_sentence)[:max_sent_tokens])\\n        next_sentence_length = len(next_sentence)\\n        sentence_tokens_length = len(word_tokenize(next_sentence))\\n        # create segment\\n        augmented_sentence = Augmentor.augment_gpt2_single(\\n            next_sentence,\\n            fast=True,\\n            # add the length of the current sentence to the dataset avg length of sentence\\n            output_tokens=int(n * int(dataset_avg_sentence_length)),\\n            num_return_sequences=1,\\n        )\\n\\n        augmented_sentence = post_augmentation_processing(\\n            # feed in the first generated sentence\\n            augmented_sentence[0][0],\\n            next_sentence_length,\\n            n,\\n        )\\n\\n        augmented_segment.append(augmented_sentence)\\n        print(\\\"augmented_segment\\\", augmented_segment)\\n\\n    augmented_segments.append(augmented_segment)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GTA 1 POC\n",
    "dataset_avg_sentence_length = sum(\n",
    "    [avg_segment_length_by_token(segment, floor=True) for segment in test_segments]\n",
    ") // len(test_segments)\n",
    "\n",
    "avg_segment_length = sum([len(t) for t in test_segments]) // len(\n",
    "    test_segments\n",
    ")  # avg number of sentences per segment in the dataset\n",
    "n = int(avg_segment_length / 2)  # the number of sentences we will generate per segment\n",
    "\n",
    "max_sent_tokens = 64\n",
    "augmented_segments = []\n",
    "\n",
    "for segment in test_segments:\n",
    "    first_sentence = truncate_by_token(segment[0], max_sent_tokens)\n",
    "\n",
    "    augmented_segment = []\n",
    "    for i in range(0, avg_segment_length):\n",
    "        next_sentence = (\n",
    "            segment[0] if len(augmented_segment) == 0 else augmented_segment[-1]\n",
    "        )\n",
    "        next_sentence = \" \".join(word_tokenize(next_sentence)[:max_sent_tokens])\n",
    "        next_sentence_length = len(next_sentence)\n",
    "        sentence_tokens_length = len(word_tokenize(next_sentence))\n",
    "        # create segment\n",
    "        augmented_sentence = Augmentor.augment_gpt2_single(\n",
    "            next_sentence,\n",
    "            fast=True,\n",
    "            # add the length of the current sentence to the dataset avg length of sentence\n",
    "            output_tokens=int(n * int(dataset_avg_sentence_length)),\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "\n",
    "        augmented_sentence = post_augmentation_processing(\n",
    "            # feed in the first generated sentence\n",
    "            augmented_sentence[0][0],\n",
    "            next_sentence_length,\n",
    "            n,\n",
    "        )\n",
    "\n",
    "        augmented_segment.append(augmented_sentence)\n",
    "        print(\"augmented_segment\", augmented_segment)\n",
    "\n",
    "    augmented_segments.append(augmented_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "12127b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 128;\n",
       "                var nbb_unformatted_code = \"n * int(dataset_avg_sentence_length), n\";\n",
       "                var nbb_formatted_code = \"n * int(dataset_avg_sentence_length), n\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n * int(dataset_avg_sentence_length), n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "db92da22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' of the book: \"The',\n",
       "  ' Book of Mormon, by Joseph',\n",
       "  'Smith.\\n\\n, by',\n",
       "  '. L. Sacks',\n",
       "  ', R. J. P']]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 129;\n",
       "                var nbb_unformatted_code = \"augmented_segments\";\n",
       "                var nbb_formatted_code = \"augmented_segments\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "augmented_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6a1a093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed augmentation...\n",
      "augmented_segment ['. The whole point of this']\n",
      "completed augmentation...\n",
      "augmented_segment ['. The whole point of this', ' is to give the players a']\n",
      "completed augmentation...\n",
      "augmented_segment ['. The whole point of this', ' is to give the players a', ' chance to win a big game']\n",
      "completed augmentation...\n",
      "augmented_segment ['. The whole point of this', ' is to give the players a', ' chance to win a big game', '.\\n\\nThe only thing']\n",
      "completed augmentation...\n",
      "augmented_segment ['. The whole point of this', ' is to give the players a', ' chance to win a big game', '.\\n\\nThe only thing', ' I can say is that I']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['. The whole point of this',\n",
       "  ' is to give the players a',\n",
       "  ' chance to win a big game',\n",
       "  '.\\n\\nThe only thing',\n",
       "  ' I can say is that I']]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 141;\n",
       "                var nbb_unformatted_code = \"Augmentor.gta1(test_segments)\";\n",
       "                var nbb_formatted_code = \"Augmentor.gta1(test_segments)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Augmentor.gta1(test_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaac744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14d02c0f",
   "metadata": {},
   "source": [
    "## GTA 2\n",
    "\n",
    "- Using GPT-2, we take the first truncated portion of the first sentence in a segment and feed it into the model. The output should be the same size as the first sentence length (for averaging similar segment sizes).\n",
    "- That first outputted sentence becomes the target sentence for the augmented segment.\n",
    "- Using the sentence sentence in the real segment, we repeat the first step. The second sentence in the augmented segment will be the output of the real second sentence fed into GPT-2.\n",
    "- Continuing this process, we will be left with an augmented segment the same exact size as the real segment it’s modeled after with hopefully less variance than GTA 1 toward the end of the segments.\n",
    "\n",
    "Cons:\n",
    "- Possible disjointedness with augmented sentences since they may vary quite a bit from the immediate sentence previously due to relying on an intermediary in-between sentence to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1657b935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed augmentation...\n",
      "sentence this is one sentence augmented_sentence  that is almost impossible to translate\n",
      "completed augmentation...\n",
      "sentence about donald trump augmented_sentence  trumpson, I think.\n",
      "completed augmentation...\n",
      "sentence and pakistan augmented_sentence ) to the US.\n",
      "\n",
      "completed augmentation...\n",
      "sentence i am not a republican augmented_sentence . If I am a democrat\n",
      "completed augmentation...\n",
      "sentence rather I am a democrat augmented_sentence ) and I'm not going\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 134;\n",
       "                var nbb_unformatted_code = \"# GTA 2 POC\\ndataset_avg_sentence_length = sum(\\n    [avg_segment_length_by_token(segment, floor=True) for segment in test_segments]\\n) // len(test_segments)\\n\\navg_segment_length = sum([len(t) for t in test_segments]) // len(\\n    test_segments\\n)  # avg number of sentences per segment in the dataset\\nn = int(avg_segment_length / 2)  # the number of sentences we will generate per segment\\n\\nmax_sent_tokens = 64\\naugmented_segments = []\\n\\nfor segment in test_segments:\\n    augmented_segment = []\\n    for sentence in segment:\\n        sentence = truncate_by_token(sentence, max_sent_tokens)\\n        sentence_length = len(sentence)\\n        sentence_tokens_length = len(word_tokenize(next_sentence))\\n\\n        augmented_sentence = Augmentor.augment_gpt2_single(\\n            sentence,\\n            fast=True,\\n            # add the length of the current sentence to the dataset avg length of sentence\\n            output_tokens=int(n * int(dataset_avg_sentence_length)),\\n            num_return_sequences=1,\\n        )\\n\\n        augmented_sentence = post_augmentation_processing(\\n            # feed in the first generated sentence\\n            augmented_sentence[0][0],\\n            sentence_length,\\n            n,\\n        )\\n\\n        augmented_segment.append(augmented_sentence)\\n        print(\\\"sentence\\\", sentence, \\\"augmented_sentence\\\", augmented_sentence)\\n\\n    augmented_segments.append(augmented_segment)\";\n",
       "                var nbb_formatted_code = \"# GTA 2 POC\\ndataset_avg_sentence_length = sum(\\n    [avg_segment_length_by_token(segment, floor=True) for segment in test_segments]\\n) // len(test_segments)\\n\\navg_segment_length = sum([len(t) for t in test_segments]) // len(\\n    test_segments\\n)  # avg number of sentences per segment in the dataset\\nn = int(avg_segment_length / 2)  # the number of sentences we will generate per segment\\n\\nmax_sent_tokens = 64\\naugmented_segments = []\\n\\nfor segment in test_segments:\\n    augmented_segment = []\\n    for sentence in segment:\\n        sentence = truncate_by_token(sentence, max_sent_tokens)\\n        sentence_length = len(sentence)\\n        sentence_tokens_length = len(word_tokenize(next_sentence))\\n\\n        augmented_sentence = Augmentor.augment_gpt2_single(\\n            sentence,\\n            fast=True,\\n            # add the length of the current sentence to the dataset avg length of sentence\\n            output_tokens=int(n * int(dataset_avg_sentence_length)),\\n            num_return_sequences=1,\\n        )\\n\\n        augmented_sentence = post_augmentation_processing(\\n            # feed in the first generated sentence\\n            augmented_sentence[0][0],\\n            sentence_length,\\n            n,\\n        )\\n\\n        augmented_segment.append(augmented_sentence)\\n        print(\\\"sentence\\\", sentence, \\\"augmented_sentence\\\", augmented_sentence)\\n\\n    augmented_segments.append(augmented_segment)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GTA 2 POC\n",
    "dataset_avg_sentence_length = sum(\n",
    "    [avg_segment_length_by_token(segment, floor=True) for segment in test_segments]\n",
    ") // len(test_segments)\n",
    "\n",
    "avg_segment_length = sum([len(t) for t in test_segments]) // len(\n",
    "    test_segments\n",
    ")  # avg number of sentences per segment in the dataset\n",
    "n = int(avg_segment_length / 2)  # the number of sentences we will generate per segment\n",
    "\n",
    "max_sent_tokens = 64\n",
    "augmented_segments = []\n",
    "\n",
    "for segment in test_segments:\n",
    "    augmented_segment = []\n",
    "    for sentence in segment:\n",
    "        sentence = truncate_by_token(sentence, max_sent_tokens)\n",
    "        sentence_length = len(sentence)\n",
    "        sentence_tokens_length = len(word_tokenize(next_sentence))\n",
    "\n",
    "        augmented_sentence = Augmentor.augment_gpt2_single(\n",
    "            sentence,\n",
    "            fast=True,\n",
    "            # add the length of the current sentence to the dataset avg length of sentence\n",
    "            output_tokens=int(n * int(dataset_avg_sentence_length)),\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "\n",
    "        augmented_sentence = post_augmentation_processing(\n",
    "            # feed in the first generated sentence\n",
    "            augmented_sentence[0][0],\n",
    "            sentence_length,\n",
    "            n,\n",
    "        )\n",
    "\n",
    "        augmented_segment.append(augmented_sentence)\n",
    "        print(\"sentence\", sentence, \"augmented_sentence\", augmented_sentence)\n",
    "\n",
    "    augmented_segments.append(augmented_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "09c3ba2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['this is one sentence',\n",
       "   'about donald trump',\n",
       "   'and pakistan',\n",
       "   'i am not a republican',\n",
       "   'rather I am a democrat']],\n",
       " [[' that is almost impossible to translate',\n",
       "   ' trumpson, I think.',\n",
       "   ') to the US.\\n',\n",
       "   '. If I am a democrat',\n",
       "   \") and I'm not going\"]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 135;\n",
       "                var nbb_unformatted_code = \"test_segments, augmented_segments\";\n",
       "                var nbb_formatted_code = \"test_segments, augmented_segments\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_segments, augmented_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ff416ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed augmentation...\n",
      "completed augmentation...\n",
      "completed augmentation...\n",
      "completed augmentation...\n",
      "completed augmentation...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[', not one', \"eter's first\", '-related activities', ', but I', ', but it']]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 143;\n",
       "                var nbb_unformatted_code = \"Augmentor.gta2(test_segments)\";\n",
       "                var nbb_formatted_code = \"Augmentor.gta2(test_segments)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Augmentor.gta2(test_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741d7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3b0c186",
   "metadata": {},
   "source": [
    "## Testing with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7b73eae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 144;\n",
       "                var nbb_unformatted_code = \"dataset_type = \\\"committee\\\"\\n\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\\n\\ntarget_sentences_original = table.get_target_sentences()\\ntarget_sentences = [s[1] for s in target_sentences_original]\\n\\n# Take the first 5 target sentences for testing\\ntest_target_sentences = target_sentences_original[:5]\\n\\n# example segment\\ntest_segments = []\\n\\nfor target_sentence in test_target_sentences:\\n    segment = table.get_segment(target_sentence[0])\\n    test_segments.append([s[1] for s in segment])\";\n",
       "                var nbb_formatted_code = \"dataset_type = \\\"committee\\\"\\n\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\\n\\ntarget_sentences_original = table.get_target_sentences()\\ntarget_sentences = [s[1] for s in target_sentences_original]\\n\\n# Take the first 5 target sentences for testing\\ntest_target_sentences = target_sentences_original[:5]\\n\\n# example segment\\ntest_segments = []\\n\\nfor target_sentence in test_target_sentences:\\n    segment = table.get_segment(target_sentence[0])\\n    test_segments.append([s[1] for s in segment])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_type = \"committee\"\n",
    "\n",
    "table = Table(dataset_type)\n",
    "augmented_table = AugmentedTable(dataset_type)\n",
    "train_test_table = TrainTestTable(dataset_type)\n",
    "\n",
    "target_sentences_original = table.get_target_sentences()\n",
    "target_sentences = [s[1] for s in target_sentences_original]\n",
    "\n",
    "# Take the first 5 target sentences for testing\n",
    "test_target_sentences = target_sentences_original[:5]\n",
    "\n",
    "# example segment\n",
    "test_segments = []\n",
    "\n",
    "for target_sentence in test_target_sentences:\n",
    "    segment = table.get_segment(target_sentence[0])\n",
    "    test_segments.append([s[1] for s in segment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "75b9da2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed augmentation...\n",
      "augmented_segment [\" are called 'therapeutic' and 'intervention' models. it is a really good start to the next stage, to get to that stage of development where we can really start looking at our own child's health, and the quality of his or her lives.\\n\\nWe are trying to develop new approaches to child mental wellbeing, including the use of 'treatment' or'mental health' as well as 'education'. I think that these approaches are quite important. if you look at the evidence on children's mental wellness and mental illness, there is no evidence that they are better off than children.\\n—\\n. We have a new approach to mental well-being, which is to look to children for help. I mean, what we're doing is looking to a group of young children who have been diagnosed as having a mental disorder and have come to terms with it. They are looking for the right help, so that's what I do. We're looking in this case of a young child who has a problem with substance use, but they don't want to be in an abusive relationship with their parents. It's very hard to find help for them, they're very vulnerable. So we have these young people who are going through a transition period, who will need a counsellor to help them understand their own needs and their circumstances. This is very different from what some of these other groups of people do in terms of providing help with mental disorders, or mental illnesses. but we are working on this. The aim of this is not to create a'reform' of child services, it's to improve it to try to give the best possible service for children. that is really what this programme is about. So that will be part of what is happening in these new programmes. this has been the case for some time.\"]\n",
      "completed augmentation...\n",
      "augmented_segment [\" are called 'therapeutic' and 'intervention' models. it is a really good start to the next stage, to get to that stage of development where we can really start looking at our own child's health, and the quality of his or her lives.\\n\\nWe are trying to develop new approaches to child mental wellbeing, including the use of 'treatment' or'mental health' as well as 'education'. I think that these approaches are quite important. if you look at the evidence on children's mental wellness and mental illness, there is no evidence that they are better off than children.\\n—\\n. We have a new approach to mental well-being, which is to look to children for help. I mean, what we're doing is looking to a group of young children who have been diagnosed as having a mental disorder and have come to terms with it. They are looking for the right help, so that's what I do. We're looking in this case of a young child who has a problem with substance use, but they don't want to be in an abusive relationship with their parents. It's very hard to find help for them, they're very vulnerable. So we have these young people who are going through a transition period, who will need a counsellor to help them understand their own needs and their circumstances. This is very different from what some of these other groups of people do in terms of providing help with mental disorders, or mental illnesses. but we are working on this. The aim of this is not to create a'reform' of child services, it's to improve it to try to give the best possible service for children. that is really what this programme is about. So that will be part of what is happening in these new programmes. this has been the case for some time.\", \"ological' tools such as 'behavioural' interventions, which will help us develop our child's'social' skills. These are often not as well known as other forms of therapy, such 'behavioral therapy', or'mental health therapy'.\\n\\nWe need to be very careful about the way we approach 'treatment' in this context. We have not developed a system to provide 'evidence-based' evidence to justify the treatment of children. The evidence is very limited and it can be quite hard to assess the evidence in any given case. It is also difficult to know what 'problems' may have been caused to a child by this 'diagnostic' therapy. It may also be difficult for us to identify the 'cause' of the problem and provide a'reasonableness' to it.\\n. it may be hard for we to define 'patient' 'needs' and how 'they' are to 'them'.\\n and. In the absence of a coherent'model' or a model for 'child mental health' that can provide such an explanation, we need a way of thinking about what is happening in a specific child and what the child needs, what needs are being addressed and where those needs might be identified. A 'clinical model' is one that offers a more holistic view of what child is experiencing, and that is based on an understanding of how the human body and brain 'work' together. The model is often based upon the idea that 'we' have to look at the whole picture, to understand what's going on in the individual and to respond in ways that we may not normally respond to. There is an important distinction between the two. For example, a treatment model can only be developed by developing a set of models, which may or may may do not provide an 'adequate' explanation for how a given child or child has been affected\"]\n",
      "completed augmentation...\n",
      "augmented_segment [\" are called 'therapeutic' and 'intervention' models. it is a really good start to the next stage, to get to that stage of development where we can really start looking at our own child's health, and the quality of his or her lives.\\n\\nWe are trying to develop new approaches to child mental wellbeing, including the use of 'treatment' or'mental health' as well as 'education'. I think that these approaches are quite important. if you look at the evidence on children's mental wellness and mental illness, there is no evidence that they are better off than children.\\n—\\n. We have a new approach to mental well-being, which is to look to children for help. I mean, what we're doing is looking to a group of young children who have been diagnosed as having a mental disorder and have come to terms with it. They are looking for the right help, so that's what I do. We're looking in this case of a young child who has a problem with substance use, but they don't want to be in an abusive relationship with their parents. It's very hard to find help for them, they're very vulnerable. So we have these young people who are going through a transition period, who will need a counsellor to help them understand their own needs and their circumstances. This is very different from what some of these other groups of people do in terms of providing help with mental disorders, or mental illnesses. but we are working on this. The aim of this is not to create a'reform' of child services, it's to improve it to try to give the best possible service for children. that is really what this programme is about. So that will be part of what is happening in these new programmes. this has been the case for some time.\", \"ological' tools such as 'behavioural' interventions, which will help us develop our child's'social' skills. These are often not as well known as other forms of therapy, such 'behavioral therapy', or'mental health therapy'.\\n\\nWe need to be very careful about the way we approach 'treatment' in this context. We have not developed a system to provide 'evidence-based' evidence to justify the treatment of children. The evidence is very limited and it can be quite hard to assess the evidence in any given case. It is also difficult to know what 'problems' may have been caused to a child by this 'diagnostic' therapy. It may also be difficult for us to identify the 'cause' of the problem and provide a'reasonableness' to it.\\n. it may be hard for we to define 'patient' 'needs' and how 'they' are to 'them'.\\n and. In the absence of a coherent'model' or a model for 'child mental health' that can provide such an explanation, we need a way of thinking about what is happening in a specific child and what the child needs, what needs are being addressed and where those needs might be identified. A 'clinical model' is one that offers a more holistic view of what child is experiencing, and that is based on an understanding of how the human body and brain 'work' together. The model is often based upon the idea that 'we' have to look at the whole picture, to understand what's going on in the individual and to respond in ways that we may not normally respond to. There is an important distinction between the two. For example, a treatment model can only be developed by developing a set of models, which may or may may do not provide an 'adequate' explanation for how a given child or child has been affected\", \"'counselor'model, and we have only been trying to 'help' children. We also do not have a child'who is being treated'by a therapist.\\n\\nIn this article, we discuss the different approaches to child-care for a wide range of mental disorders. Our aim is to provide an overview of these approaches. This is not to say that we can't apply them, but that they are important. The focus will be on the ways in which children are treated by their parents. Some of the more important elements will include:\\n- The child is treated with a structured approach, with an understanding of child psychology, to help them learn and develop their own child development. In addition, it will give them the opportunity to learn about their needs, their interests, their behaviours, how to behave and what to do in a positive way. Children are given the ability to make decisions and make choices in the context of their environment. It is important to note that children may not be able to fully understand their surroundings. For example, they may have difficulty interacting with people or with their peers. They may be unable to understand how they feel or what is going on in their life (e.g. what they think they need, what their partner wants, etc.). It may also be difficult for children to relate to their family or friends. These issues are not always as clear cut as they seem, so it is best to consider how these children will interact with the adult world. There may well be difficulties for parents who have to deal with issues with family members or other people. Parents may find it difficult to interact and learn from their children's experiences. And, parents may feel they have no control over their child. A child who has been treated in an abusive environment will often feel like they can no longer cope with his or her needs. Many of us have been very\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed augmentation...\n",
      "augmented_segment [\" are called 'therapeutic' and 'intervention' models. it is a really good start to the next stage, to get to that stage of development where we can really start looking at our own child's health, and the quality of his or her lives.\\n\\nWe are trying to develop new approaches to child mental wellbeing, including the use of 'treatment' or'mental health' as well as 'education'. I think that these approaches are quite important. if you look at the evidence on children's mental wellness and mental illness, there is no evidence that they are better off than children.\\n—\\n. We have a new approach to mental well-being, which is to look to children for help. I mean, what we're doing is looking to a group of young children who have been diagnosed as having a mental disorder and have come to terms with it. They are looking for the right help, so that's what I do. We're looking in this case of a young child who has a problem with substance use, but they don't want to be in an abusive relationship with their parents. It's very hard to find help for them, they're very vulnerable. So we have these young people who are going through a transition period, who will need a counsellor to help them understand their own needs and their circumstances. This is very different from what some of these other groups of people do in terms of providing help with mental disorders, or mental illnesses. but we are working on this. The aim of this is not to create a'reform' of child services, it's to improve it to try to give the best possible service for children. that is really what this programme is about. So that will be part of what is happening in these new programmes. this has been the case for some time.\", \"ological' tools such as 'behavioural' interventions, which will help us develop our child's'social' skills. These are often not as well known as other forms of therapy, such 'behavioral therapy', or'mental health therapy'.\\n\\nWe need to be very careful about the way we approach 'treatment' in this context. We have not developed a system to provide 'evidence-based' evidence to justify the treatment of children. The evidence is very limited and it can be quite hard to assess the evidence in any given case. It is also difficult to know what 'problems' may have been caused to a child by this 'diagnostic' therapy. It may also be difficult for us to identify the 'cause' of the problem and provide a'reasonableness' to it.\\n. it may be hard for we to define 'patient' 'needs' and how 'they' are to 'them'.\\n and. In the absence of a coherent'model' or a model for 'child mental health' that can provide such an explanation, we need a way of thinking about what is happening in a specific child and what the child needs, what needs are being addressed and where those needs might be identified. A 'clinical model' is one that offers a more holistic view of what child is experiencing, and that is based on an understanding of how the human body and brain 'work' together. The model is often based upon the idea that 'we' have to look at the whole picture, to understand what's going on in the individual and to respond in ways that we may not normally respond to. There is an important distinction between the two. For example, a treatment model can only be developed by developing a set of models, which may or may may do not provide an 'adequate' explanation for how a given child or child has been affected\", \"'counselor'model, and we have only been trying to 'help' children. We also do not have a child'who is being treated'by a therapist.\\n\\nIn this article, we discuss the different approaches to child-care for a wide range of mental disorders. Our aim is to provide an overview of these approaches. This is not to say that we can't apply them, but that they are important. The focus will be on the ways in which children are treated by their parents. Some of the more important elements will include:\\n- The child is treated with a structured approach, with an understanding of child psychology, to help them learn and develop their own child development. In addition, it will give them the opportunity to learn about their needs, their interests, their behaviours, how to behave and what to do in a positive way. Children are given the ability to make decisions and make choices in the context of their environment. It is important to note that children may not be able to fully understand their surroundings. For example, they may have difficulty interacting with people or with their peers. They may be unable to understand how they feel or what is going on in their life (e.g. what they think they need, what their partner wants, etc.). It may also be difficult for children to relate to their family or friends. These issues are not always as clear cut as they seem, so it is best to consider how these children will interact with the adult world. There may well be difficulties for parents who have to deal with issues with family members or other people. Parents may find it difficult to interact and learn from their children's experiences. And, parents may feel they have no control over their child. A child who has been treated in an abusive environment will often feel like they can no longer cope with his or her needs. Many of us have been very\", \"or 'cure' the condition, rather, we are trying in the same way to help children.\\n\\nThe following is an excerpt from the article entitled, 'Child Care for Children' by Dr. Michael J. Schumacher, Ph.D., author of 'Cognitive and Affective Disorders: An Overview and Treatment Recommendations for the Treatment of Children With Autism Spectrum Disorders (CASD) and Other Disorders' published in Psychiatry & Psychopharmacology (2006). It is a work in progress, but the gist is this:\\n... we know that some children are being placed in care and others are not. We know this by looking at the symptoms of autism spectrum disorder (ASDS) in our own children and by using the diagnostic criteria of ASD (Autism Spectrum Disorder, Asperger's, Developmental Disabilities and Tourette's Syndrome).\\n and.\\n, and are. The symptoms are often described as a variety of problems, including, for example, anorexia nervosa or post-traumatic stress disorder. In addition, many children with ASD have had an experience of being physically abused, neglected, or bullied. This has been described in detail in this book as well. The authors also say in their book that they are'very concerned' that children's needs are sometimes 'inconvenient' and 'unnecessary' to be placed into this situation. It should be noted that the 'child' in question is often a girl or boy, so we do know a number of boys with autism and that there is some research to show that this is true. (The authors have not made any specific recommendations about which types of children should receive care, although it is possible to use their own personal experiences to make this decision.)\\n (This article has a link to a discussion of the topic in a previous article.) We do have some information that\"]\n",
      "completed augmentation...\n",
      "augmented_segment [\" are called 'therapeutic' and 'intervention' models. it is a really good start to the next stage, to get to that stage of development where we can really start looking at our own child's health, and the quality of his or her lives.\\n\\nWe are trying to develop new approaches to child mental wellbeing, including the use of 'treatment' or'mental health' as well as 'education'. I think that these approaches are quite important. if you look at the evidence on children's mental wellness and mental illness, there is no evidence that they are better off than children.\\n—\\n. We have a new approach to mental well-being, which is to look to children for help. I mean, what we're doing is looking to a group of young children who have been diagnosed as having a mental disorder and have come to terms with it. They are looking for the right help, so that's what I do. We're looking in this case of a young child who has a problem with substance use, but they don't want to be in an abusive relationship with their parents. It's very hard to find help for them, they're very vulnerable. So we have these young people who are going through a transition period, who will need a counsellor to help them understand their own needs and their circumstances. This is very different from what some of these other groups of people do in terms of providing help with mental disorders, or mental illnesses. but we are working on this. The aim of this is not to create a'reform' of child services, it's to improve it to try to give the best possible service for children. that is really what this programme is about. So that will be part of what is happening in these new programmes. this has been the case for some time.\", \"ological' tools such as 'behavioural' interventions, which will help us develop our child's'social' skills. These are often not as well known as other forms of therapy, such 'behavioral therapy', or'mental health therapy'.\\n\\nWe need to be very careful about the way we approach 'treatment' in this context. We have not developed a system to provide 'evidence-based' evidence to justify the treatment of children. The evidence is very limited and it can be quite hard to assess the evidence in any given case. It is also difficult to know what 'problems' may have been caused to a child by this 'diagnostic' therapy. It may also be difficult for us to identify the 'cause' of the problem and provide a'reasonableness' to it.\\n. it may be hard for we to define 'patient' 'needs' and how 'they' are to 'them'.\\n and. In the absence of a coherent'model' or a model for 'child mental health' that can provide such an explanation, we need a way of thinking about what is happening in a specific child and what the child needs, what needs are being addressed and where those needs might be identified. A 'clinical model' is one that offers a more holistic view of what child is experiencing, and that is based on an understanding of how the human body and brain 'work' together. The model is often based upon the idea that 'we' have to look at the whole picture, to understand what's going on in the individual and to respond in ways that we may not normally respond to. There is an important distinction between the two. For example, a treatment model can only be developed by developing a set of models, which may or may may do not provide an 'adequate' explanation for how a given child or child has been affected\", \"'counselor'model, and we have only been trying to 'help' children. We also do not have a child'who is being treated'by a therapist.\\n\\nIn this article, we discuss the different approaches to child-care for a wide range of mental disorders. Our aim is to provide an overview of these approaches. This is not to say that we can't apply them, but that they are important. The focus will be on the ways in which children are treated by their parents. Some of the more important elements will include:\\n- The child is treated with a structured approach, with an understanding of child psychology, to help them learn and develop their own child development. In addition, it will give them the opportunity to learn about their needs, their interests, their behaviours, how to behave and what to do in a positive way. Children are given the ability to make decisions and make choices in the context of their environment. It is important to note that children may not be able to fully understand their surroundings. For example, they may have difficulty interacting with people or with their peers. They may be unable to understand how they feel or what is going on in their life (e.g. what they think they need, what their partner wants, etc.). It may also be difficult for children to relate to their family or friends. These issues are not always as clear cut as they seem, so it is best to consider how these children will interact with the adult world. There may well be difficulties for parents who have to deal with issues with family members or other people. Parents may find it difficult to interact and learn from their children's experiences. And, parents may feel they have no control over their child. A child who has been treated in an abusive environment will often feel like they can no longer cope with his or her needs. Many of us have been very\", \"or 'cure' the condition, rather, we are trying in the same way to help children.\\n\\nThe following is an excerpt from the article entitled, 'Child Care for Children' by Dr. Michael J. Schumacher, Ph.D., author of 'Cognitive and Affective Disorders: An Overview and Treatment Recommendations for the Treatment of Children With Autism Spectrum Disorders (CASD) and Other Disorders' published in Psychiatry & Psychopharmacology (2006). It is a work in progress, but the gist is this:\\n... we know that some children are being placed in care and others are not. We know this by looking at the symptoms of autism spectrum disorder (ASDS) in our own children and by using the diagnostic criteria of ASD (Autism Spectrum Disorder, Asperger's, Developmental Disabilities and Tourette's Syndrome).\\n and.\\n, and are. The symptoms are often described as a variety of problems, including, for example, anorexia nervosa or post-traumatic stress disorder. In addition, many children with ASD have had an experience of being physically abused, neglected, or bullied. This has been described in detail in this book as well. The authors also say in their book that they are'very concerned' that children's needs are sometimes 'inconvenient' and 'unnecessary' to be placed into this situation. It should be noted that the 'child' in question is often a girl or boy, so we do know a number of boys with autism and that there is some research to show that this is true. (The authors have not made any specific recommendations about which types of children should receive care, although it is possible to use their own personal experiences to make this decision.)\\n (This article has a link to a discussion of the topic in a previous article.) We do have some information that\", 'he idea that a child is better off if she is cared for in a caring home is a common one. This is because in general, children are more likely to be in care if they are cared in an environment that is not in danger of being attacked, neglected, or neglected.\\n (See the previous article, Children with Autism, by Michael Schumann, Ph, author. See also Child Care, and by Susan M. K. Johnson, M., author.)\\n,\\'Child care for children \\', by\\'Dr.\\' Schuman, in \\'The Effects of Childcare on Children\\' by Jens Mørge, PhD., Ph., \\'Children with C.V.I., CASd., and Children\\'s Mental Health: A Review and Meta-Analysis,\\' by Peter B. Burch and Peter M.-H. Jansen, eds., Child Development and Social Development,\\', (1-5), pp. 1-9, p. 476-487. (see also, Child Welfare, Categorization and the Child-Care System, (2-6), by R.A. H. Männer, A.C.R.K. and Järvin Jansson, \"The Effect on the Developmental Quality of the Coding Behavior of Adolescents with Children and Adolescent Children: The Role of COD,\",,(1), Vol. 3, No. 2, pp., pp 8-17.)']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed augmentation...\n",
      "augmented_segment ['mittee, we have been waiting for a long time for the confirmation of members for this day so i would like you to send us a brief statement. it is important to keep in mind that members of this committee are the representatives of our children and we want to be sure that their parents are fully aware of what is going on and that they are not being bullied or harassed by members. if you have a question about any of these things please do not hesitate to contact us. and if we can give you an answer please let us know. the rest of your questions can be sent to your member.\\n\\nThank you for your time,\\n.\\n (The Chairperson of Siâne Gwenillian )']\n",
      "completed augmentation...\n",
      "augmented_segment ['mittee, we have been waiting for a long time for the confirmation of members for this day so i would like you to send us a brief statement. it is important to keep in mind that members of this committee are the representatives of our children and we want to be sure that their parents are fully aware of what is going on and that they are not being bullied or harassed by members. if you have a question about any of these things please do not hesitate to contact us. and if we can give you an answer please let us know. the rest of your questions can be sent to your member.\\n\\nThank you for your time,\\n.\\n (The Chairperson of Siâne Gwenillian )', 'n. This is the first time that we would want our members to have such information. Please feel free to contact your representatives and tell them what you think. We will be in touch with your representative and if they wish to take the time to explain what we are doing, i will send you an email with the details of the meeting.']\n",
      "completed augmentation...\n",
      "augmented_segment ['mittee, we have been waiting for a long time for the confirmation of members for this day so i would like you to send us a brief statement. it is important to keep in mind that members of this committee are the representatives of our children and we want to be sure that their parents are fully aware of what is going on and that they are not being bullied or harassed by members. if you have a question about any of these things please do not hesitate to contact us. and if we can give you an answer please let us know. the rest of your questions can be sent to your member.\\n\\nThank you for your time,\\n.\\n (The Chairperson of Siâne Gwenillian )', 'n. This is the first time that we would want our members to have such information. Please feel free to contact your representatives and tell them what you think. We will be in touch with your representative and if they wish to take the time to explain what we are doing, i will send you an email with the details of the meeting.', 'at they are going to want to know.']\n",
      "completed augmentation...\n",
      "augmented_segment ['mittee, we have been waiting for a long time for the confirmation of members for this day so i would like you to send us a brief statement. it is important to keep in mind that members of this committee are the representatives of our children and we want to be sure that their parents are fully aware of what is going on and that they are not being bullied or harassed by members. if you have a question about any of these things please do not hesitate to contact us. and if we can give you an answer please let us know. the rest of your questions can be sent to your member.\\n\\nThank you for your time,\\n.\\n (The Chairperson of Siâne Gwenillian )', 'n. This is the first time that we would want our members to have such information. Please feel free to contact your representatives and tell them what you think. We will be in touch with your representative and if they wish to take the time to explain what we are doing, i will send you an email with the details of the meeting.', 'at they are going to want to know.', '.\" He went on to say, \"The only people who really care about this are the ones who can afford it. And that\\'s why they\\'re going for a new plan.\"\\n\\nIn the end, the plan was a disaster, not least because it was not designed to work. The idea was to put an end to the government\\'s monopoly of health care, and it worked. But it also meant that the federal government had to provide health insurance to everyone.\\n..']\n",
      "completed augmentation...\n",
      "augmented_segment ['mittee, we have been waiting for a long time for the confirmation of members for this day so i would like you to send us a brief statement. it is important to keep in mind that members of this committee are the representatives of our children and we want to be sure that their parents are fully aware of what is going on and that they are not being bullied or harassed by members. if you have a question about any of these things please do not hesitate to contact us. and if we can give you an answer please let us know. the rest of your questions can be sent to your member.\\n\\nThank you for your time,\\n.\\n (The Chairperson of Siâne Gwenillian )', 'n. This is the first time that we would want our members to have such information. Please feel free to contact your representatives and tell them what you think. We will be in touch with your representative and if they wish to take the time to explain what we are doing, i will send you an email with the details of the meeting.', 'at they are going to want to know.', '.\" He went on to say, \"The only people who really care about this are the ones who can afford it. And that\\'s why they\\'re going for a new plan.\"\\n\\nIn the end, the plan was a disaster, not least because it was not designed to work. The idea was to put an end to the government\\'s monopoly of health care, and it worked. But it also meant that the federal government had to provide health insurance to everyone.\\n..', 'the government\\'s ability to provide for the families of the homeless who had been forced out by the city and the police. It was like a giant government program, a program to bring people to a place where they could get on the streets and be treated. This plan would have been so good for everyone that it would not have happened. But the people of New York who were displaced by this plan, they were not able to afford to buy a car or a house. They were homeless. And this program was so bad for them, that they couldn\\'t afford a job. The program they had, the program that was going to help them buy their car, was the one that really helped them. That program is called \"Housing for All\". It is a system that is designed for people whose lives were ruined by a government that has been unable to deliver on its promises. People who are homeless are being forced in the most desperate, desperate way imaginable. There are no jobs. No housing. A lot of people are in shelters. Many are not getting a second chance. In New Jersey, there were more than 3,000 homeless people in 2013, a number that had not been able either to get their homes or to find a decent job because of homelessness. New Yorkers who lived in New Brunswick were in terrible shape. They lost their jobs, their housing, and they lost all their hope. Their only hope was that their kids would be able go out and get a good job. New Hampshire, which was one of those states that were the first to pass a law requiring everyone in poverty to have a home, had a similar story. People were getting out of their houses because they didn\\'t have enough money to pay for their school, or because their parents couldn`t afford school. These were people that lost everything. Some of them were working, some of \\'em had kids, but they weren']\n",
      "completed augmentation...\n",
      "augmented_segment ['u. you mentioned that the focus is on the brain, in your view you are not going back to a place in biology that is as well endowed with the capacity to make decisions as you would expect from a human being. so you have to do a lot of things that are very specific to your individual situation. in other words, what you need to be able to deal with are your thoughts and feelings. if you feel that something is wrong, then you\\'re going through an internal process that will take some time, it will be a difficult process, you can\\'t get over it. but in the long run, if it\\'s done right you will have achieved what your parents had wanted, so it is possible to move on to other things. i guess the problem with being a psychologist is that it can be quite difficult to understand how things are being done in our society. and if we can do that we will improve our ability to communicate, that makes us better at understanding our own situation than we would be if the world were just a little bit more structured.. as i said, I was very disappointed in what the group said about this, even though it was a very positive thing to see them do something that they had never done before. it may not sound like it, but there are so many things to talk about that need some discussion, as far as I know, there is a whole lot more to learn from the groups. they did not seem to want to hear anything negative about what they were doing. the idea that there was some sort of \"problem\" with this is very disturbing., to me it seems like they have a different view of human nature. there may be something very wrong with our behavior, which is why they are doing this. this may or may no one else in society have that problem, this could be an issue that has to go away, or maybe we might be']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [147], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m augmented_test_segments_gta1 \u001b[38;5;241m=\u001b[39m Augmentor\u001b[38;5;241m.\u001b[39mgta1(test_segments)\n\u001b[1;32m      2\u001b[0m augmented_test_segments_gta2 \u001b[38;5;241m=\u001b[39m Augmentor\u001b[38;5;241m.\u001b[39mgta2(test_segments)\n",
      "File \u001b[0;32m~/Documents/PhD/context-encoder-qmsum/src/dataset/gpt_augmentor.py:181\u001b[0m, in \u001b[0;36mAugmentor.gta1\u001b[0;34m(cls, segments, max_sent_tokens)\u001b[0m\n\u001b[1;32m    179\u001b[0m sentence_tokens_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_tokenize(next_sentence))\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# create segment\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m augmented_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mAugmentor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment_gpt2_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_sentence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# add the length of the current sentence to the dataset avg length of sentence\u001b[39;49;00m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_avg_sentence_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m augmented_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mpost_augmentation_processing(\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# feed in the first generated sentence\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     augmented_sentence[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    192\u001b[0m     next_sentence_length,\n\u001b[1;32m    193\u001b[0m )\n\u001b[1;32m    195\u001b[0m augmented_segment\u001b[38;5;241m.\u001b[39mappend(augmented_sentence)\n",
      "File \u001b[0;32m~/Documents/PhD/context-encoder-qmsum/src/dataset/gpt_augmentor.py:116\u001b[0m, in \u001b[0;36mAugmentor.augment_gpt2_single\u001b[0;34m(cls, sentence, fast, num_return_sequences, output_tokens)\u001b[0m\n\u001b[1;32m    113\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m sample_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# corresponds to all the new tokens appended to the input\u001b[39;49;00m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m generated_segments\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    128\u001b[0m     [tokenizer\u001b[38;5;241m.\u001b[39mdecode(x, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    129\u001b[0m      \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m sample_outputs]\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted augmentation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/transformers/generation_tf_utils.py:583\u001b[0m, in \u001b[0;36mTFGenerationMixin.generate\u001b[0;34m(self, input_ids, max_length, max_new_tokens, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, output_scores, output_attentions, output_hidden_states, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, **model_kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_beams \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    582\u001b[0m     seed \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbad_words_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbad_words_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_start_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_start_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforced_bos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforced_bos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforced_eos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforced_eos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;66;03m# We cannot generate if the model does not have a LM head\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_output_embeddings() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/transformers/generation_tf_utils.py:1678\u001b[0m, in \u001b[0;36mTFGenerationMixin._generate\u001b[0;34m(self, input_ids, max_length, max_new_tokens, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, seed, output_scores, output_attentions, output_hidden_states, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, **model_kwargs)\u001b[0m\n\u001b[1;32m   1670\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1671\u001b[0m         input_ids,\n\u001b[1;32m   1672\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mnum_return_sequences,\n\u001b[1;32m   1673\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1675\u001b[0m     )\n\u001b[1;32m   1677\u001b[0m     \u001b[38;5;66;03m# 11. run sample\u001b[39;00m\n\u001b[0;32m-> 1678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_beams \u001b[38;5;241m<\u001b[39m num_return_sequences:\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/transformers/generation_tf_utils.py:2560\u001b[0m, in \u001b[0;36mTFGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, logits_warper, max_length, pad_token_id, eos_token_id, seed, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_cond_fn(generated, finished_sequences, cur_len, model_kwargs):\n\u001b[1;32m   2559\u001b[0m     maximum_iterations \u001b[38;5;241m=\u001b[39m max_length \u001b[38;5;241m-\u001b[39m cur_len\n\u001b[0;32m-> 2560\u001b[0m     generated, _, cur_len, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2561\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_cond_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_body_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2563\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinished_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;66;03m# 6. prepare outputs\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_xla:\n\u001b[1;32m   2569\u001b[0m     \u001b[38;5;66;03m# cut for backward compatibility\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:629\u001b[0m, in \u001b[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m           _PRINTED_WARNING[(func, arg_name)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    623\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    624\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    625\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwill be removed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstructions for updating:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    626\u001b[0m             _call_location(), decorator_utils\u001b[38;5;241m.\u001b[39mget_qualified_name(func),\n\u001b[1;32m    627\u001b[0m             func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m, arg_name, arg_value, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min a future version\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    628\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m date), instructions)\n\u001b[0;32m--> 629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py:2513\u001b[0m, in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m   2337\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   2338\u001b[0m \u001b[38;5;129m@deprecation\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated_arg_values(\n\u001b[1;32m   2339\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2354\u001b[0m                   maximum_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2355\u001b[0m                   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2356\u001b[0m   \u001b[38;5;124;03m\"\"\"Repeat `body` while the condition `cond` is true.\u001b[39;00m\n\u001b[1;32m   2357\u001b[0m \n\u001b[1;32m   2358\u001b[0m \u001b[38;5;124;03m  `cond` is a callable returning a boolean scalar tensor. `body` is a callable\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \n\u001b[1;32m   2512\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2513\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2514\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2515\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2516\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2517\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape_invariants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape_invariants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2518\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparallel_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[43m      \u001b[49m\u001b[43mback_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mback_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2520\u001b[0m \u001b[43m      \u001b[49m\u001b[43mswap_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswap_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2521\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2522\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2523\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreturn_same_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py:2762\u001b[0m, in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2759\u001b[0m loop_var_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(type_spec\u001b[38;5;241m.\u001b[39mtype_spec_from_value,\n\u001b[1;32m   2760\u001b[0m                                         \u001b[38;5;28mlist\u001b[39m(loop_vars))\n\u001b[1;32m   2761\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cond(\u001b[38;5;241m*\u001b[39mloop_vars):\n\u001b[0;32m-> 2762\u001b[0m   loop_vars \u001b[38;5;241m=\u001b[39m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2763\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m try_to_pack \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loop_vars, (\u001b[38;5;28mlist\u001b[39m, _basetuple)):\n\u001b[1;32m   2764\u001b[0m     packed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py:2753\u001b[0m, in \u001b[0;36mwhile_loop.<locals>.<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   2750\u001b[0m     loop_vars \u001b[38;5;241m=\u001b[39m (counter, loop_vars)\n\u001b[1;32m   2751\u001b[0m     cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m i, lv: (  \u001b[38;5;66;03m# pylint: disable=g-long-lambda\u001b[39;00m\n\u001b[1;32m   2752\u001b[0m         math_ops\u001b[38;5;241m.\u001b[39mlogical_and(i \u001b[38;5;241m<\u001b[39m maximum_iterations, orig_cond(\u001b[38;5;241m*\u001b[39mlv)))\n\u001b[0;32m-> 2753\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m i, lv: (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[43morig_body\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlv\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2754\u001b[0m   try_to_pack \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executing_eagerly:\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/transformers/generation_tf_utils.py:2476\u001b[0m, in \u001b[0;36mTFGenerationMixin.sample.<locals>.sample_body_fn\u001b[0;34m(generated, finished_sequences, cur_len, model_kwargs)\u001b[0m\n\u001b[1;32m   2474\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;66;03m# forward pass to get next token logits\u001b[39;00m\n\u001b[0;32m-> 2476\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2477\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2481\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m model_outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/keras/engine/training.py:557\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39mcopied_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    555\u001b[0m     layout_map_lib\u001b[38;5;241m.\u001b[39m_map_subclass_model_variable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout_map)\n\u001b[0;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/keras/engine/base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1096\u001b[0m ):\n\u001b[0;32m-> 1097\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:409\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    408\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:895\u001b[0m, in \u001b[0;36mTFGPT2LMHeadModel.call\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(GPT2_INPUTS_DOCSTRING)\n\u001b[1;32m    847\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    869\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    870\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFCausalLMOutputWithCrossAttentions, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;124;03m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;124;03m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;124;03m        config.vocab_size - 1]`.\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 895\u001b[0m     transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mwte(hidden_states, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/keras/engine/base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1096\u001b[0m ):\n\u001b[0;32m-> 1097\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:409\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    408\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:406\u001b[0m, in \u001b[0;36mTFGPT2MainLayer.call\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    404\u001b[0m     one_cst \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    405\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(attention_mask, dtype\u001b[38;5;241m=\u001b[39mone_cst\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 406\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubtract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_cst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m10000.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Copied from `modeling_tf_t5.py` with -1e9 -> -10000\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39madd_cross_attention \u001b[38;5;129;01mand\u001b[39;00m encoder_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# If a 2D ou 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# we need to make broadcastable to [batch_size, num_heads, mask_seq_length, mask_seq_length]\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:529\u001b[0m, in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.multiply\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    481\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39mregister_binary_elementwise_api\n\u001b[1;32m    482\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmultiply\u001b[39m(x, y, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    484\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns an element-wise x * y.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03m  For example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;124;03m   * InvalidArgumentError: When `x` and `y` have incompatible shapes or types.\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/phd/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py:6576\u001b[0m, in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   6575\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 6576\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6577\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMul\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   6579\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 147;\n",
       "                var nbb_unformatted_code = \"augmented_test_segments_gta1 = Augmentor.gta1(test_segments)\\naugmented_test_segments_gta2 = Augmentor.gta2(test_segments)\";\n",
       "                var nbb_formatted_code = \"augmented_test_segments_gta1 = Augmentor.gta1(test_segments)\\naugmented_test_segments_gta2 = Augmentor.gta2(test_segments)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "augmented_test_segments_gta1 = Augmentor.gta1(test_segments)\n",
    "augmented_test_segments_gta2 = Augmentor.gta2(test_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df3126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
