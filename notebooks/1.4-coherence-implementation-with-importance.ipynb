{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc7697a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run if working locally\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ee84b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_formatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pickle\n",
    "import os, sys\n",
    "import config\n",
    "\n",
    "config.root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, config.root_path)\n",
    "\n",
    "from src.dataset.dataset import RawData\n",
    "from src.dataset.wikisection_preprocessing import (\n",
    "    tokenize,\n",
    "    clean_sentence,\n",
    "    preprocess_text_segmentation,\n",
    "    format_data_for_db_insertion,\n",
    ")\n",
    "from src.dataset.utils import truncate_by_token\n",
    "from db.dbv2 import Table, AugmentedTable, TrainTestTable\n",
    "import pprint\n",
    "\n",
    "from src.bertkeywords.src.similarities import Embedding, Similarities\n",
    "from src.bertkeywords.src.keywords import Keywords\n",
    "from src.encoders.coherence import Coherence\n",
    "from src.dataset.utils import flatten, dedupe_list, truncate_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e4dcd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 22:54:13.646091: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-04-09 22:54:13.646253: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-04-09 22:54:16.660175: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-04-09 22:54:16.693570: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-09 22:54:17.469533: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# initialize the coherence library\\ncoherence = Coherence(max_words_per_step=4)\";\n",
       "                var nbb_formatted_code = \"# initialize the coherence library\\ncoherence = Coherence(max_words_per_step=4)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the coherence library\n",
    "coherence = Coherence(max_words_per_step=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "226021b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-04-09 22:54:24.018911: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-09 22:54:24.793058: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# initialize the keywords and embeddings library\\npp = pprint.PrettyPrinter(indent=4)\\nsimilarities_lib = Similarities(\\\"bert-base-uncased\\\")\\nkeywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\\nembedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)\";\n",
       "                var nbb_formatted_code = \"# initialize the keywords and embeddings library\\npp = pprint.PrettyPrinter(indent=4)\\nsimilarities_lib = Similarities(\\\"bert-base-uncased\\\")\\nkeywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\\nembedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the keywords and embeddings library\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "similarities_lib = Similarities(\"bert-base-uncased\")\n",
    "keywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\n",
    "embedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb2458b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_formatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_type = \"city\"\n",
    "table = Table(dataset_type)\n",
    "augmented_table = AugmentedTable(dataset_type)\n",
    "train_test_table = TrainTestTable(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd59c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_formatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = table.get_all()\n",
    "\n",
    "text_data = [x[1] for x in data]\n",
    "text_labels = [x[2] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcccad5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_formatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_segments = table.get_all_segments()\n",
    "text_segments = [[y[1] for y in x] for x in all_segments]\n",
    "segments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "295657fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_formatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = 5\n",
    "max_tokens = 400\n",
    "\n",
    "for i, (segment, labels) in enumerate(\n",
    "    zip(text_segments[:samples], segments_labels[:samples])\n",
    "):\n",
    "    for sentence, label in zip(segment, labels):\n",
    "        # this is the training case. During inference, we will have no idea\n",
    "        # when segments start and when they end.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "467789d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"text_labels[:25]\";\n",
       "                var nbb_formatted_code = \"text_labels[:25]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_labels[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6870992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"pruning = 4  # remove the lowest n important words from coherence map\\npruning_min = 10  # only prune after n words in the coherence map\\n\\n\\n# importance testing\\ndef compare_coherent_words(\\n    coherence_map, keywords_prev, keywords_current, suppress_errors=False\\n):\\n    word_comparisons = []\\n    print(f\\\"comparing coherent words: {[[x[0] for x in c] for c in coherence_map]}\\\")\\n    coherence_map.append(keywords_prev)\\n    for i, keywords in enumerate(coherence_map[::-1]):\\n        for word_tuple in keywords:\\n            word = word_tuple[0]\\n            for second_word_tuple in keywords_current:\\n                second_word = second_word_tuple[0]\\n\\n                try:\\n                    word_one_emb = word_tuple[2]\\n                    word_two_emb = second_word_tuple[2]\\n\\n                    word_comparisons.append(\\n                        (\\n                            word,\\n                            second_word,\\n                            (1 / (i + 1))\\n                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\\n                        )\\n                    )\\n                except AssertionError as e:\\n                    if not suppress_errors:\\n                        print(e, word, second_word)\\n\\n    return word_comparisons\\n\\n\\ndef coherence_tester(\\n    text_data, text_labels, max_tokens=400, max_str_length=30, prediction_thresh=0.2\\n):\\n    coherence_map = []\\n    predictions = []\\n    for i, (row, label) in enumerate(zip(text_data, text_labels)):\\n        # compare the current sentence to the previous one\\n        if i == 0:\\n            pass\\n        else:\\n            prev_row = text_data[i - 1]\\n\\n            row = truncate_by_token(row, max_tokens)\\n            prev_row = truncate_by_token(prev_row, max_tokens)\\n\\n            # add the keywords to the coherence map\\n            coherence_map.append(\\n                coherence.get_coherence([row, prev_row], coherence_threshold=0.2)\\n            )\\n            print(f\\\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}\\\")\\n            if pruning > 0 and len(coherence_map) >= pruning_min:\\n                print(\\\"pruning...\\\", len(coherence_map))\\n                sorted_map = sorted(\\n                    coherence_map, key=lambda tup: tup[1]\\n                )  # sort asc by importance based on keybert\\n                coherence_map = sorted_map[pruning:][\\n                    ::-1\\n                ]  # get the last n - pruning values and reverse the list\\n                print(\\\"done pruning...\\\", len(coherence_map))\\n\\n            # truncate the strings for printing\\n            truncated_row = truncate_string(row, max_str_length)\\n            truncated_prev_row = truncate_string(prev_row, max_str_length)\\n\\n            # get the keywords for the current sentences\\n            keywords_current = keywords_lib.get_keywords_with_embeddings(row)\\n            keywords_prev = keywords_lib.get_keywords_with_embeddings(prev_row)\\n\\n            # compute the word comparisons between the previous (with the coherence map)\\n            # and the current (possibly the first sentence in a new segment)\\n            word_comparisons_with_coherence = compare_coherent_words(\\n                coherence_map, keywords_prev, keywords_current\\n            )\\n\\n            similarities_with_coherence = [\\n                comparison[2] for comparison in word_comparisons_with_coherence\\n            ]\\n            avg_similarity_with_coherence = sum(similarities_with_coherence) / len(\\n                similarities_with_coherence\\n            )\\n\\n            # if the two sentences are similar, create a cohesive prediction\\n            # otherwise, predict a new segment\\n            if avg_similarity_with_coherence[0] > prediction_thresh:\\n                print(\\n                    f\\\"Label: {label}, Prediction: {0}, logit: {avg_similarity_with_coherence[0]}\\\"\\n                )\\n                predictions.append((avg_similarity_with_coherence[0], 0))\\n            else:\\n                # start of a new segment, empty the map\\n                coherence_map = []\\n                print(\\n                    f\\\"Label: {label}, Prediction: {1}, logit: {avg_similarity_with_coherence[0]}\\\"\\n                )\\n                predictions.append((avg_similarity_with_coherence[0], 1))\\n\\n    return predictions\";\n",
       "                var nbb_formatted_code = \"pruning = 4  # remove the lowest n important words from coherence map\\npruning_min = 10  # only prune after n words in the coherence map\\n\\n\\n# importance testing\\ndef compare_coherent_words(\\n    coherence_map, keywords_prev, keywords_current, suppress_errors=False\\n):\\n    word_comparisons = []\\n    print(f\\\"comparing coherent words: {[[x[0] for x in c] for c in coherence_map]}\\\")\\n    coherence_map.append(keywords_prev)\\n    for i, keywords in enumerate(coherence_map[::-1]):\\n        for word_tuple in keywords:\\n            word = word_tuple[0]\\n            for second_word_tuple in keywords_current:\\n                second_word = second_word_tuple[0]\\n\\n                try:\\n                    word_one_emb = word_tuple[2]\\n                    word_two_emb = second_word_tuple[2]\\n\\n                    word_comparisons.append(\\n                        (\\n                            word,\\n                            second_word,\\n                            (1 / (i + 1))\\n                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\\n                        )\\n                    )\\n                except AssertionError as e:\\n                    if not suppress_errors:\\n                        print(e, word, second_word)\\n\\n    return word_comparisons\\n\\n\\ndef coherence_tester(\\n    text_data, text_labels, max_tokens=400, max_str_length=30, prediction_thresh=0.2\\n):\\n    coherence_map = []\\n    predictions = []\\n    for i, (row, label) in enumerate(zip(text_data, text_labels)):\\n        # compare the current sentence to the previous one\\n        if i == 0:\\n            pass\\n        else:\\n            prev_row = text_data[i - 1]\\n\\n            row = truncate_by_token(row, max_tokens)\\n            prev_row = truncate_by_token(prev_row, max_tokens)\\n\\n            # add the keywords to the coherence map\\n            coherence_map.append(\\n                coherence.get_coherence([row, prev_row], coherence_threshold=0.2)\\n            )\\n            print(f\\\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}\\\")\\n            if pruning > 0 and len(coherence_map) >= pruning_min:\\n                print(\\\"pruning...\\\", len(coherence_map))\\n                sorted_map = sorted(\\n                    coherence_map, key=lambda tup: tup[1]\\n                )  # sort asc by importance based on keybert\\n                coherence_map = sorted_map[pruning:][\\n                    ::-1\\n                ]  # get the last n - pruning values and reverse the list\\n                print(\\\"done pruning...\\\", len(coherence_map))\\n\\n            # truncate the strings for printing\\n            truncated_row = truncate_string(row, max_str_length)\\n            truncated_prev_row = truncate_string(prev_row, max_str_length)\\n\\n            # get the keywords for the current sentences\\n            keywords_current = keywords_lib.get_keywords_with_embeddings(row)\\n            keywords_prev = keywords_lib.get_keywords_with_embeddings(prev_row)\\n\\n            # compute the word comparisons between the previous (with the coherence map)\\n            # and the current (possibly the first sentence in a new segment)\\n            word_comparisons_with_coherence = compare_coherent_words(\\n                coherence_map, keywords_prev, keywords_current\\n            )\\n\\n            similarities_with_coherence = [\\n                comparison[2] for comparison in word_comparisons_with_coherence\\n            ]\\n            avg_similarity_with_coherence = sum(similarities_with_coherence) / len(\\n                similarities_with_coherence\\n            )\\n\\n            # if the two sentences are similar, create a cohesive prediction\\n            # otherwise, predict a new segment\\n            if avg_similarity_with_coherence[0] > prediction_thresh:\\n                print(\\n                    f\\\"Label: {label}, Prediction: {0}, logit: {avg_similarity_with_coherence[0]}\\\"\\n                )\\n                predictions.append((avg_similarity_with_coherence[0], 0))\\n            else:\\n                # start of a new segment, empty the map\\n                coherence_map = []\\n                print(\\n                    f\\\"Label: {label}, Prediction: {1}, logit: {avg_similarity_with_coherence[0]}\\\"\\n                )\\n                predictions.append((avg_similarity_with_coherence[0], 1))\\n\\n    return predictions\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pruning = 4  # remove the lowest n important words from coherence map\n",
    "pruning_min = 10  # only prune after n words in the coherence map\n",
    "\n",
    "\n",
    "# importance testing\n",
    "def compare_coherent_words(\n",
    "    coherence_map, keywords_prev, keywords_current, suppress_errors=False\n",
    "):\n",
    "    word_comparisons = []\n",
    "    print(f\"comparing coherent words: {[[x[0] for x in c] for c in coherence_map]}\")\n",
    "    coherence_map.append(keywords_prev)\n",
    "    for i, keywords in enumerate(coherence_map[::-1]):\n",
    "        for word_tuple in keywords:\n",
    "            word = word_tuple[0]\n",
    "            for second_word_tuple in keywords_current:\n",
    "                second_word = second_word_tuple[0]\n",
    "\n",
    "                try:\n",
    "                    word_one_emb = word_tuple[2]\n",
    "                    word_two_emb = second_word_tuple[2]\n",
    "\n",
    "                    word_comparisons.append(\n",
    "                        (\n",
    "                            word,\n",
    "                            second_word,\n",
    "                            (1 / (i + 1))\n",
    "                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\n",
    "                        )\n",
    "                    )\n",
    "                except AssertionError as e:\n",
    "                    if not suppress_errors:\n",
    "                        print(e, word, second_word)\n",
    "\n",
    "    return word_comparisons\n",
    "\n",
    "\n",
    "# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\n",
    "def coherence_tester(\n",
    "    text_data, text_labels, max_tokens=400, max_str_length=30, prediction_thresh=0.2\n",
    "):\n",
    "    coherence_map = []\n",
    "    predictions = []\n",
    "    for i, (row, label) in enumerate(zip(text_data, text_labels)):\n",
    "        # compare the current sentence to the previous one\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            prev_row = text_data[i - 1]\n",
    "\n",
    "            row = truncate_by_token(row, max_tokens)\n",
    "            prev_row = truncate_by_token(prev_row, max_tokens)\n",
    "\n",
    "            # add the keywords to the coherence map\n",
    "            coherence_map.append(\n",
    "                coherence.get_coherence([row, prev_row], coherence_threshold=0.2)\n",
    "            )\n",
    "            print(f\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}\")\n",
    "            if pruning > 0 and len(coherence_map) >= pruning_min:\n",
    "                print(\"pruning...\", len(coherence_map))\n",
    "                sorted_map = sorted(\n",
    "                    coherence_map, key=lambda tup: tup[1]\n",
    "                )  # sort asc by importance based on keybert\n",
    "                coherence_map = sorted_map[pruning:][\n",
    "                    ::-1\n",
    "                ]  # get the last n - pruning values and reverse the list\n",
    "                print(\"done pruning...\", len(coherence_map))\n",
    "\n",
    "            # truncate the strings for printing\n",
    "            truncated_row = truncate_string(row, max_str_length)\n",
    "            truncated_prev_row = truncate_string(prev_row, max_str_length)\n",
    "\n",
    "            # get the keywords for the current sentences\n",
    "            keywords_current = keywords_lib.get_keywords_with_embeddings(row)\n",
    "            keywords_prev = keywords_lib.get_keywords_with_embeddings(prev_row)\n",
    "\n",
    "            # compute the word comparisons between the previous (with the coherence map)\n",
    "            # and the current (possibly the first sentence in a new segment)\n",
    "            word_comparisons_with_coherence = compare_coherent_words(\n",
    "                coherence_map, keywords_prev, keywords_current\n",
    "            )\n",
    "\n",
    "            similarities_with_coherence = [\n",
    "                comparison[2] for comparison in word_comparisons_with_coherence\n",
    "            ]\n",
    "            avg_similarity_with_coherence = sum(similarities_with_coherence) / len(\n",
    "                similarities_with_coherence\n",
    "            )\n",
    "\n",
    "            # if the two sentences are similar, create a cohesive prediction\n",
    "            # otherwise, predict a new segment\n",
    "            if avg_similarity_with_coherence[0] > prediction_thresh:\n",
    "                print(\n",
    "                    f\"Label: {label}, Prediction: {0}, logit: {avg_similarity_with_coherence[0]}\"\n",
    "                )\n",
    "                predictions.append((avg_similarity_with_coherence[0], 0))\n",
    "            else:\n",
    "                # start of a new segment, empty the map\n",
    "                coherence_map = []\n",
    "                print(\n",
    "                    f\"Label: {label}, Prediction: {1}, logit: {avg_similarity_with_coherence[0]}\"\n",
    "                )\n",
    "                predictions.append((avg_similarity_with_coherence[0], 1))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca71b4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Map: [['shoreline', 'basque', 'seashore', 'basque']]\n",
      "comparing coherent words: [['shoreline', 'basque', 'seashore', 'basque']]\n",
      "Label: 0, Prediction: 0, logit: 0.23786942660808563\n",
      "Coherence Map: [['shoreline', 'basque', 'seashore', 'basque'], ['basque', 'sebastián', 'saint', 'san', 'latin'], ['precipitation', 'shoreline', 'climate', 'shoreline']]\n",
      "comparing coherent words: [['shoreline', 'basque', 'seashore', 'basque'], ['basque', 'sebastián', 'saint', 'san', 'latin'], ['precipitation', 'shoreline', 'climate', 'shoreline']]\n",
      "Label: 0, Prediction: 1, logit: 0.16597723960876465\n",
      "Coherence Map: [['paleolithic', 'precipitation', 'evidence', 'precipitation']]\n",
      "comparing coherent words: [['paleolithic', 'precipitation', 'evidence', 'precipitation']]\n",
      "Label: 0, Prediction: 0, logit: 0.22200337052345276\n",
      "Coherence Map: [['paleolithic', 'precipitation', 'evidence', 'precipitation'], ['precipitation', 'climate', 'winters', 'overcast', 'cloudy'], ['basque', 'paleolithic', 'roman', 'paleolithic']]\n",
      "comparing coherent words: [['paleolithic', 'precipitation', 'evidence', 'precipitation'], ['precipitation', 'climate', 'winters', 'overcast', 'cloudy'], ['basque', 'paleolithic', 'roman', 'paleolithic']]\n",
      "Label: 0, Prediction: 1, logit: 0.14119142293930054\n",
      "Coherence Map: [['castile', 'san', 'colonizers', 'san']]\n",
      "comparing coherent words: [['castile', 'san', 'colonizers', 'san']]\n",
      "Label: 0, Prediction: 0, logit: 0.2757745385169983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(tensor(0.2379), 0),\n",
       " (tensor(0.1660), 1),\n",
       " (tensor(0.2220), 0),\n",
       " (tensor(0.1412), 1),\n",
       " (tensor(0.2758), 0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"samples = 6\\nmax_tokens = 400  # want to keep this under 512\\nmax_str_length = 30\\n\\ncoherence_tester(\\n    text_data[:samples],\\n    text_labels[:samples],\\n    max_tokens=max_tokens,\\n    max_str_length=max_str_length,\\n)\";\n",
       "                var nbb_formatted_code = \"samples = 6\\nmax_tokens = 400  # want to keep this under 512\\nmax_str_length = 30\\n\\ncoherence_tester(\\n    text_data[:samples],\\n    text_labels[:samples],\\n    max_tokens=max_tokens,\\n    max_str_length=max_str_length,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = 6\n",
    "max_tokens = 400  # want to keep this under 512\n",
    "max_str_length = 30\n",
    "\n",
    "coherence_tester(\n",
    "    text_data[:samples],\n",
    "    text_labels[:samples],\n",
    "    max_tokens=max_tokens,\n",
    "    max_str_length=max_str_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a93727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a18a8471",
   "metadata": {},
   "source": [
    "## Sample Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a97f25cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"segments_to_test = 10\\n\\ntext_segments_to_check = [\\n    [truncate_by_token(s, max_tokens) for s in segment]\\n    for segment in text_segments[:segments_to_test]\\n]\\ntext_labels_to_check = segments_labels[:segments_to_test]\";\n",
       "                var nbb_formatted_code = \"segments_to_test = 10\\n\\ntext_segments_to_check = [\\n    [truncate_by_token(s, max_tokens) for s in segment]\\n    for segment in text_segments[:segments_to_test]\\n]\\ntext_labels_to_check = segments_labels[:segments_to_test]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "segments_to_test = 10\n",
    "\n",
    "text_segments_to_check = [\n",
    "    [truncate_by_token(s, max_tokens) for s in segment]\n",
    "    for segment in text_segments[:segments_to_test]\n",
    "]\n",
    "text_labels_to_check = segments_labels[:segments_to_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "583ba1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 36)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"len(text_segments_to_check[0]), len(text_labels_to_check[0])\";\n",
       "                var nbb_formatted_code = \"len(text_segments_to_check[0]), len(text_labels_to_check[0])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(text_segments_to_check[0]), len(text_labels_to_check[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d1c6816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 41;\n",
       "                var nbb_unformatted_code = \"different_segment_1 = [\\n    text_segments_to_check[0][-3],\\n    text_segments_to_check[0][-2],\\n    text_segments_to_check[0][-1],\\n    text_segments_to_check[1][0],\\n    text_segments_to_check[1][1],\\n    text_segments_to_check[1][2],\\n]\\ndifferent_segment_2 = [\\n    text_segments_to_check[1][-3],\\n    text_segments_to_check[1][-2],\\n    text_segments_to_check[1][-1],\\n    text_segments_to_check[2][0],\\n    text_segments_to_check[2][1],\\n    text_segments_to_check[2][2],\\n]\\ndifferent_segment_3 = [\\n    text_segments_to_check[2][-3],\\n    text_segments_to_check[2][-2],\\n    text_segments_to_check[2][-1],\\n    text_segments_to_check[3][0],\\n    text_segments_to_check[3][1],\\n    text_segments_to_check[3][2],\\n]\\ndifferent_segment_4 = [\\n    text_segments_to_check[3][-3],\\n    text_segments_to_check[3][-2],\\n    text_segments_to_check[3][-1],\\n    text_segments_to_check[4][0],\\n    text_segments_to_check[4][1],\\n    text_segments_to_check[4][2],\\n]\\ndifferent_segment_5 = [\\n    text_segments_to_check[4][-3],\\n    text_segments_to_check[4][-2],\\n    text_segments_to_check[4][-1],\\n    text_segments_to_check[5][0],\\n    text_segments_to_check[5][1],\\n    text_segments_to_check[5][2],\\n]\\n\\nsame_segment_1 = [\\n    text_segments_to_check[0][0],\\n    text_segments_to_check[0][1],\\n    text_segments_to_check[0][2],\\n]\\nsame_segment_2 = [\\n    text_segments_to_check[1][0],\\n    text_segments_to_check[1][1],\\n    text_segments_to_check[1][2],\\n]\\nsame_segment_3 = [\\n    text_segments_to_check[2][0],\\n    text_segments_to_check[2][1],\\n    text_segments_to_check[2][2],\\n]\\nsame_segment_4 = [\\n    text_segments_to_check[3][0],\\n    text_segments_to_check[3][1],\\n    text_segments_to_check[3][2],\\n]\\nsame_segment_5 = [\\n    text_segments_to_check[4][0],\\n    text_segments_to_check[4][1],\\n    text_segments_to_check[4][2],\\n]\";\n",
       "                var nbb_formatted_code = \"different_segment_1 = [\\n    text_segments_to_check[0][-3],\\n    text_segments_to_check[0][-2],\\n    text_segments_to_check[0][-1],\\n    text_segments_to_check[1][0],\\n    text_segments_to_check[1][1],\\n    text_segments_to_check[1][2],\\n]\\ndifferent_segment_2 = [\\n    text_segments_to_check[1][-3],\\n    text_segments_to_check[1][-2],\\n    text_segments_to_check[1][-1],\\n    text_segments_to_check[2][0],\\n    text_segments_to_check[2][1],\\n    text_segments_to_check[2][2],\\n]\\ndifferent_segment_3 = [\\n    text_segments_to_check[2][-3],\\n    text_segments_to_check[2][-2],\\n    text_segments_to_check[2][-1],\\n    text_segments_to_check[3][0],\\n    text_segments_to_check[3][1],\\n    text_segments_to_check[3][2],\\n]\\ndifferent_segment_4 = [\\n    text_segments_to_check[3][-3],\\n    text_segments_to_check[3][-2],\\n    text_segments_to_check[3][-1],\\n    text_segments_to_check[4][0],\\n    text_segments_to_check[4][1],\\n    text_segments_to_check[4][2],\\n]\\ndifferent_segment_5 = [\\n    text_segments_to_check[4][-3],\\n    text_segments_to_check[4][-2],\\n    text_segments_to_check[4][-1],\\n    text_segments_to_check[5][0],\\n    text_segments_to_check[5][1],\\n    text_segments_to_check[5][2],\\n]\\n\\nsame_segment_1 = [\\n    text_segments_to_check[0][0],\\n    text_segments_to_check[0][1],\\n    text_segments_to_check[0][2],\\n]\\nsame_segment_2 = [\\n    text_segments_to_check[1][0],\\n    text_segments_to_check[1][1],\\n    text_segments_to_check[1][2],\\n]\\nsame_segment_3 = [\\n    text_segments_to_check[2][0],\\n    text_segments_to_check[2][1],\\n    text_segments_to_check[2][2],\\n]\\nsame_segment_4 = [\\n    text_segments_to_check[3][0],\\n    text_segments_to_check[3][1],\\n    text_segments_to_check[3][2],\\n]\\nsame_segment_5 = [\\n    text_segments_to_check[4][0],\\n    text_segments_to_check[4][1],\\n    text_segments_to_check[4][2],\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "different_segment_1 = [\n",
    "    text_segments_to_check[0][-3],\n",
    "    text_segments_to_check[0][-2],\n",
    "    text_segments_to_check[0][-1],\n",
    "    text_segments_to_check[1][0],\n",
    "    text_segments_to_check[1][1],\n",
    "    text_segments_to_check[1][2],\n",
    "]\n",
    "different_segment_2 = [\n",
    "    text_segments_to_check[1][-3],\n",
    "    text_segments_to_check[1][-2],\n",
    "    text_segments_to_check[1][-1],\n",
    "    text_segments_to_check[2][0],\n",
    "    text_segments_to_check[2][1],\n",
    "    text_segments_to_check[2][2],\n",
    "]\n",
    "different_segment_3 = [\n",
    "    text_segments_to_check[2][-3],\n",
    "    text_segments_to_check[2][-2],\n",
    "    text_segments_to_check[2][-1],\n",
    "    text_segments_to_check[3][0],\n",
    "    text_segments_to_check[3][1],\n",
    "    text_segments_to_check[3][2],\n",
    "]\n",
    "different_segment_4 = [\n",
    "    text_segments_to_check[3][-3],\n",
    "    text_segments_to_check[3][-2],\n",
    "    text_segments_to_check[3][-1],\n",
    "    text_segments_to_check[4][0],\n",
    "    text_segments_to_check[4][1],\n",
    "    text_segments_to_check[4][2],\n",
    "]\n",
    "different_segment_5 = [\n",
    "    text_segments_to_check[4][-3],\n",
    "    text_segments_to_check[4][-2],\n",
    "    text_segments_to_check[4][-1],\n",
    "    text_segments_to_check[5][0],\n",
    "    text_segments_to_check[5][1],\n",
    "    text_segments_to_check[5][2],\n",
    "]\n",
    "\n",
    "same_segment_1 = [\n",
    "    text_segments_to_check[0][0],\n",
    "    text_segments_to_check[0][1],\n",
    "    text_segments_to_check[0][2],\n",
    "]\n",
    "same_segment_2 = [\n",
    "    text_segments_to_check[1][0],\n",
    "    text_segments_to_check[1][1],\n",
    "    text_segments_to_check[1][2],\n",
    "]\n",
    "same_segment_3 = [\n",
    "    text_segments_to_check[2][0],\n",
    "    text_segments_to_check[2][1],\n",
    "    text_segments_to_check[2][2],\n",
    "]\n",
    "same_segment_4 = [\n",
    "    text_segments_to_check[3][0],\n",
    "    text_segments_to_check[3][1],\n",
    "    text_segments_to_check[3][2],\n",
    "]\n",
    "same_segment_5 = [\n",
    "    text_segments_to_check[4][0],\n",
    "    text_segments_to_check[4][1],\n",
    "    text_segments_to_check[4][2],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed7cac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Map: ['universidad', 'michelin', 'universities', 'michelin']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['universidad', 'michelin', 'universities', 'michelin', 'uci', 'universidad', 'sociedad', 'universidad']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['universidad', 'michelin', 'universities', 'michelin', 'uci', 'universidad', 'sociedad', 'universidad', 'sioux', 'uci', 'railroad', 'uci']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 1, Prediction: 1\n",
      "Coherence Map: ['census', 'sioux', 'area', 'sioux']\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['households', 'census', 'census', 'census']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['households', 'households', 'median', 'households']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['households', 'households', 'median', 'households', 'elementary', 'households', 'elementary', 'median']\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['cathedral', 'elementary', 'buildings', 'elementary']\n",
      "Label: 1, Prediction: 0\n",
      "Coherence Map: ['cathedral', 'elementary', 'buildings', 'elementary', '1943', 'cossacks', '1942', 'cossacks']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['cathedral', 'elementary', 'buildings', 'elementary', '1943', 'cossacks', '1942', 'cossacks', 'separatists', '1943', 'ukrainian', '1943']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['lennon', 'separatists', 'square', 'separatists']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['lennon', 'separatists', 'square', 'separatists', 'climate', 'lennon', 'classification', 'lennon']\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['census', 'climate', 'area', 'climate']\n",
      "Label: 1, Prediction: 1\n",
      "Coherence Map: ['households', 'census', 'census', 'census']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['households', 'census', 'census', 'census', 'households', 'households', 'median', 'households']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['households', 'census', 'census', 'census']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['households', 'census', 'census', 'census', 'households', 'households', 'median', 'households']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['households', 'census', 'census', 'census', 'households', 'households', 'median', 'households', 'complexes', 'households', 'gora', 'median']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 1, Prediction: 1\n",
      "Coherence Map: ['springs', 'winery', 'sources', 'winery']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['springs', 'winery', 'sources', 'winery', 'boris', 'springs', 'iii', 'springs']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['springs', 'winery', 'sources', 'winery']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['springs', 'winery', 'sources', 'winery', 'boris', 'springs', 'iii', 'springs']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['springs', 'winery', 'sources', 'winery', 'boris', 'springs', 'iii', 'springs', 'massachusetts', 'boris', 'railroad', 'boris']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 1, Prediction: 1\n",
      "Coherence Map: ['harvard', 'harvard', 'harvard', 'founded']\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['households', 'census', 'census', 'census']\n",
      "Label: 0, Prediction: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(tensor(0.3717), 0),\n",
       " (tensor(0.3243), 0),\n",
       " (tensor(0.2591), 1),\n",
       " (tensor(0.2496), 1),\n",
       " (tensor(0.3721), 0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 47;\n",
       "                var nbb_unformatted_code = \"num_sentences_to_check = 6  # test only the first n in the segment\\ntarget_labels = [0, 0, 0, 1, 0, 0]\\n\\n# test coherence on different segments\\ncoherence_tester(\\n    different_segment_1[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_2[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_3[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_4[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_5[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\";\n",
       "                var nbb_formatted_code = \"num_sentences_to_check = 6  # test only the first n in the segment\\ntarget_labels = [0, 0, 0, 1, 0, 0]\\n\\n# test coherence on different segments\\ncoherence_tester(\\n    different_segment_1[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_2[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_3[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_4[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_5[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_sentences_to_check = 6  # test only the first n in the segment\n",
    "target_labels = [0, 0, 0, 1, 0, 0]\n",
    "\n",
    "# test coherence on different segments\n",
    "coherence_tester(\n",
    "    different_segment_1[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")\n",
    "coherence_tester(\n",
    "    different_segment_2[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")\n",
    "coherence_tester(\n",
    "    different_segment_3[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")\n",
    "coherence_tester(\n",
    "    different_segment_4[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")\n",
    "coherence_tester(\n",
    "    different_segment_5[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f34ae8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence: ['shoreline', 'basque', 'seashore', 'basque']\n",
      "0, tensor([0.2451]), In spite of appearances both t.. <> The city is in the north of th..\n",
      "0, tensor([0.3396]), In spite of appearances both t.. <> The city is in the north of th..\n",
      "Coherence: ['precipitation', 'shoreline', 'climate', 'shoreline']\n",
      "0, tensor([0.2880]), The city is in the north of th.. <> San Sebastián features an ocea..\n",
      "0, tensor([0.3303]), The city is in the north of th.. <> San Sebastián features an ocea..\n",
      "Coherence: ['census', 'sioux', 'area', 'sioux']\n",
      "0, tensor([0.2409]), Hospers was founded in 1872 wh.. <> Hospers is located at 43 07103..\n",
      "0, tensor([0.2987]), Hospers was founded in 1872 wh.. <> Hospers is located at 43 07103..\n",
      "Coherence: ['households', 'census', 'census', 'census']\n",
      "0, tensor([0.2941]), Hospers is located at 43 07103.. <> As of the census of 2010 there..\n",
      "0, tensor([0.3422]), Hospers is located at 43 07103.. <> As of the census of 2010 there..\n",
      "Coherence: ['1943', 'cossacks', '1942', 'cossacks']\n",
      "0, tensor([0.3146]), First mentioned in 1571 in con.. <> During World War II the Red Ar..\n",
      "0, tensor([0.3753]), First mentioned in 1571 in con.. <> During World War II the Red Ar..\n",
      "Coherence: ['separatists', '1943', 'ukrainian', '1943']\n",
      "0, tensor([0.3053]), During World War II the Red Ar.. <> The town was the site of spora..\n",
      "0, tensor([0.3648]), During World War II the Red Ar.. <> The town was the site of spora..\n",
      "Coherence: ['households', 'census', 'census', 'census']\n",
      "0, tensor([0.2690]), Wallingford is located at 43 3.. <> As of the census of 2010 there..\n",
      "0, tensor([0.3508]), Wallingford is located at 43 3.. <> As of the census of 2010 there..\n",
      "Coherence: ['households', 'households', 'median', 'households']\n",
      "0, tensor([0.5013]), As of the census of 2010 there.. <> As of the census of 2000 there..\n",
      "0, tensor([0.4821]), As of the census of 2010 there.. <> As of the census of 2000 there..\n",
      "Coherence: ['springs', 'winery', 'sources', 'winery']\n",
      "0, tensor([0.3112]), The health resort village of B.. <> The remains of an ancient vill..\n",
      "0, tensor([0.3717]), The health resort village of B.. <> The remains of an ancient vill..\n",
      "Coherence: ['boris', 'springs', 'iii', 'springs']\n",
      "0, tensor([0.3035]), The remains of an ancient vill.. <> The Banya Palace summerhouse o..\n",
      "0, tensor([0.3243]), The remains of an ancient vill.. <> The Banya Palace summerhouse o..\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# test coherence on same segments\\ncoherence_tester(same_segment_1, [0,0,0])\\ncoherence_tester(same_segment_2, [0,0,0])\\ncoherence_tester(same_segment_3, [0,0,0])\\ncoherence_tester(same_segment_4, [0,0,0])\\ncoherence_tester(same_segment_5, [0,0,0])\";\n",
       "                var nbb_formatted_code = \"# test coherence on same segments\\ncoherence_tester(same_segment_1, [0, 0, 0])\\ncoherence_tester(same_segment_2, [0, 0, 0])\\ncoherence_tester(same_segment_3, [0, 0, 0])\\ncoherence_tester(same_segment_4, [0, 0, 0])\\ncoherence_tester(same_segment_5, [0, 0, 0])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test coherence on same segments\n",
    "coherence_tester(same_segment_1, [0, 0, 0])\n",
    "coherence_tester(same_segment_2, [0, 0, 0])\n",
    "coherence_tester(same_segment_3, [0, 0, 0])\n",
    "coherence_tester(same_segment_4, [0, 0, 0])\n",
    "coherence_tester(same_segment_5, [0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "810d05f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Map: ['shoreline', 'basque', 'seashore', 'basque']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['shoreline', 'basque', 'seashore', 'basque', 'precipitation', 'shoreline', 'climate', 'shoreline']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['shoreline', 'basque', 'seashore', 'basque', 'precipitation', 'shoreline', 'climate', 'shoreline', 'paleolithic', 'precipitation', 'evidence', 'precipitation']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['basque', 'paleolithic', 'roman', 'paleolithic']\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['castile', 'san', 'colonizers', 'san']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['castile', 'san', 'colonizers', 'san', 'navarre', 'castile', 'iberian', 'castile']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['castile', 'san', 'colonizers', 'san', 'navarre', 'castile', 'iberian', 'castile', 'neoclassical', 'navarre', 'bourgeois', 'navarre']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['railway', 'neoclassical', 'bridge', 'neoclassical']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['railway', 'neoclassical', 'bridge', 'neoclassical', 'residents', 'railway', 'walls', 'railway']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['railway', 'neoclassical', 'bridge', 'neoclassical', 'residents', 'railway', 'walls', 'railway', 'industry', 'residents', 'nucleus', 'residents']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['railway', 'railway', 'railway', 'neoclassical', 'neoclassical', 'residents', 'residents', 'residents', 'terminal', 'industry', 'centre', 'industry']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['railway', 'railway', 'railway', 'neoclassical', 'neoclassical', 'residents', 'residents', 'residents', 'district', 'terminal', 'avenida', 'terminal']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'avenida', 'railway', 'railway', 'railway', 'neoclassical', 'neoclassical', 'residents', 'district', 'district', 'beach', 'district']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'avenida', 'railway', 'railway', 'railway', 'neoclassical', 'postwar', 'district', 'dictator', 'district']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'avenida', 'railway', 'railway', 'railway', 'railway', 'postwar', 'former', 'postwar']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'railway', 'railway', 'civil', 'railway']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'inhabitants', 'railway', 'immigrants', 'railway']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'nanotechnology', 'inhabitants', 'campus', 'inhabitants']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'downtown', 'nanotechnology', 'axis', 'nanotechnology']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'bridge', 'district', 'pedestrian', 'district']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['bridge', 'district', 'district', 'district', 'district', 'district', 'postwar', 'postwar', 'municipality', 'bridge', 'training', 'bridge']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['bridge', 'bridge', 'bridge', 'district', 'district', 'district', 'district', 'district', 'nursery', 'municipality', 'nurseries', 'municipality']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['bridge', 'bridge', 'bridge', 'district', 'district', 'district', 'district', 'district', 'village', 'nursery', 'reconstruction', 'nursery']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['bridge', 'bridge', 'bridge', 'district', 'district', 'district', 'district', 'district', 'festivals', 'village', 'festival', 'village']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festival', 'bridge', 'bridge', 'bridge', 'district', 'district', 'district', 'celebrations', 'festivals', 'parades', 'festivals']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'fireworks', 'celebrations', 'festival', 'celebrations']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'basque', 'fireworks', 'festival', 'fireworks']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'basque', 'basque', 'traditional', 'basque']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'festival', 'basque', 'gypsy', 'basque']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'festival', 'festival', 'stalls', 'festival']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'festival', 'bridge', 'bridge', 'bridge', 'basque', 'festival', 'villages', 'festival']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'festival', 'festival', 'festival', 'bridge', 'san', 'basque', 'tourism', 'basque']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'festival', 'festival', 'festival', 'bridge', 'michelin', 'san', 'gastronomy', 'san']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['universidad', 'michelin', 'universities', 'michelin']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['universidad', 'michelin', 'universities', 'michelin', 'uci', 'universidad', 'sociedad', 'universidad']\n",
      "Label: 0, Prediction: 0\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 48;\n",
       "                var nbb_unformatted_code = \"# test on a full segment\\npredictions = coherence_tester(text_segments_to_check[0], text_labels_to_check[0])\";\n",
       "                var nbb_formatted_code = \"# test on a full segment\\npredictions = coherence_tester(text_segments_to_check[0], text_labels_to_check[0])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test on a full segment\n",
    "predictions = coherence_tester(text_segments_to_check[0], text_labels_to_check[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a3613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5121f953",
   "metadata": {},
   "source": [
    "# pruning test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e541d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 44;\n",
       "                var nbb_unformatted_code = \"data = [(\\\"word1\\\", 0.3), (\\\"word2\\\", 0.1), (\\\"word3\\\", 0.7)]\\n\\nsorted_arr = sorted(data, key=lambda tup: tup[1])\";\n",
       "                var nbb_formatted_code = \"data = [(\\\"word1\\\", 0.3), (\\\"word2\\\", 0.1), (\\\"word3\\\", 0.7)]\\n\\nsorted_arr = sorted(data, key=lambda tup: tup[1])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"word1\", 0.3), (\"word2\", 0.1), (\"word3\", 0.7)]\n",
    "\n",
    "sorted_arr = sorted(data, key=lambda tup: tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0188f9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('word2', 0.1), ('word1', 0.3), ('word3', 0.7)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 45;\n",
       "                var nbb_unformatted_code = \"sorted_arr\";\n",
       "                var nbb_formatted_code = \"sorted_arr\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00458200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
