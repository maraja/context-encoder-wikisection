{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc7697a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run if working locally\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ee84b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom utils.metrics import windowdiff, pk\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_formatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom utils.metrics import windowdiff, pk\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pickle\n",
    "import os, sys\n",
    "import config\n",
    "\n",
    "config.root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, config.root_path)\n",
    "\n",
    "from src.dataset.dataset import RawData\n",
    "from src.dataset.wikisection_preprocessing import (\n",
    "    tokenize,\n",
    "    clean_sentence,\n",
    "    preprocess_text_segmentation,\n",
    "    format_data_for_db_insertion,\n",
    ")\n",
    "from src.dataset.utils import truncate_by_token\n",
    "from db.dbv2 import Table, AugmentedTable, TrainTestTable\n",
    "import pprint\n",
    "\n",
    "from utils.metrics import windowdiff, pk\n",
    "\n",
    "from src.bertkeywords.src.similarities import Embedding, Similarities\n",
    "from src.bertkeywords.src.keywords import Keywords\n",
    "from src.encoders.coherence import Coherence\n",
    "from src.dataset.utils import flatten, dedupe_list, truncate_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e4dcd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-04-11 23:14:07.597737: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-11 23:14:08.355450: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-11 23:14:08.491838: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# initialize the coherence library\\ncoherence = Coherence(max_words_per_step=5)\";\n",
       "                var nbb_formatted_code = \"# initialize the coherence library\\ncoherence = Coherence(max_words_per_step=5)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the coherence library\n",
    "coherence = Coherence(max_words_per_step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "226021b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-04-11 23:14:11.640808: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-11 23:14:12.380027: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# initialize the keywords and embeddings library\\npp = pprint.PrettyPrinter(indent=4)\\nsimilarities_lib = Similarities(\\\"bert-base-uncased\\\")\\nkeywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\\nembedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)\";\n",
       "                var nbb_formatted_code = \"# initialize the keywords and embeddings library\\npp = pprint.PrettyPrinter(indent=4)\\nsimilarities_lib = Similarities(\\\"bert-base-uncased\\\")\\nkeywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\\nembedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the keywords and embeddings library\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "similarities_lib = Similarities(\"bert-base-uncased\")\n",
    "keywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\n",
    "embedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb2458b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_formatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_type = \"city\"\n",
    "table = Table(dataset_type)\n",
    "augmented_table = AugmentedTable(dataset_type)\n",
    "train_test_table = TrainTestTable(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd59c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_formatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = table.get_all()\n",
    "\n",
    "text_data = [x[1] for x in data]\n",
    "text_labels = [x[2] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcccad5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_formatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_segments = table.get_all_segments()\n",
    "text_segments = [[y[1] for y in x] for x in all_segments]\n",
    "segments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "295657fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_formatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = 5\n",
    "max_tokens = 400\n",
    "\n",
    "for i, (segment, labels) in enumerate(\n",
    "    zip(text_segments[:samples], segments_labels[:samples])\n",
    "):\n",
    "    for sentence, label in zip(segment, labels):\n",
    "        # this is the training case. During inference, we will have no idea\n",
    "        # when segments start and when they end.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "467789d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"text_labels[:25]\";\n",
       "                var nbb_formatted_code = \"text_labels[:25]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_labels[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f6870992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 79;\n",
       "                var nbb_unformatted_code = \"pruning = 0  # remove the lowest n important words from coherence map\\npruning_min = 10  # only prune after n words in the coherence map\\n\\n\\ndef get_weighted_average(weighted_similarities, weights):\\n    return sum(weighted_similarities) / sum(weights)\\n\\n\\n# importance testing\\ndef compare_coherent_words(coherence_map, keywords_current, suppress_errors=False):\\n    word_comparisons = []\\n    weights = []\\n    for i, keywords in enumerate(coherence_map[::-1]):\\n        for word_tuple in keywords:\\n            word = word_tuple[0]\\n            for second_word_tuple in keywords_current:\\n                second_word = second_word_tuple[0]\\n\\n                try:\\n                    word_one_emb = word_tuple[2]\\n                    word_two_emb = second_word_tuple[2]\\n                    weight = 1 / (\\n                        i + 1\\n                    )  # this weight is a recipricol function that will grow smaller the further the keywords are away\\n\\n                    word_comparisons.append(\\n                        (\\n                            word,\\n                            second_word,\\n                            weight\\n                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\\n                        )\\n                    )\\n                    weights.append(weight)\\n                except AssertionError as e:\\n                    if not suppress_errors:\\n                        print(e, word, second_word)\\n\\n    return word_comparisons, weights\\n\\n\\n# Adding dynamic threshold\\ndef coherence_tester(\\n    text_data,\\n    text_labels,\\n    max_tokens=400,\\n    max_str_length=30,\\n    prediction_thresh=0.25,\\n    dynamic_threshold=True,\\n    threshold_warmup=15,  # number of iterations before using dynamic threshold\\n    last_n_threshold=5,  # will only consider the last n thresholds for dynamic threshold\\n):\\n    coherence_map = []\\n    predictions = []\\n    thresholds = []\\n    for i, (row, label) in enumerate(zip(text_data, text_labels)):\\n        threshold = prediction_thresh\\n        if dynamic_threshold and (i + 1) > threshold_warmup:\\n            last_n_thresholds = thresholds[(0 - last_n_threshold) :]\\n            last_n_thresholds.sort()\\n            mid = len(last_n_thresholds) // 2\\n            threshold = (last_n_thresholds[mid] + last_n_thresholds[~mid]) / 2\\n            print(f\\\"median threshold: {threshold}\\\")\\n        # compare the current sentence to the previous one\\n        if i == 0:\\n            predictions.append((0, 0))\\n        else:\\n            prev_row = text_data[i - 1]\\n\\n            row = truncate_by_token(row, max_tokens)\\n            prev_row = truncate_by_token(prev_row, max_tokens)\\n\\n            # add the keywords to the coherence map\\n            coherence_map.append(\\n                coherence.get_coherence([row, prev_row], coherence_threshold=0.2)\\n            )\\n            print(f\\\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}\\\")\\n            if pruning > 0 and len(coherence_map) >= pruning_min:\\n                print(\\\"pruning...\\\", len(coherence_map))\\n                sorted_map = sorted(\\n                    coherence_map, key=lambda tup: tup[1]\\n                )  # sort asc by importance based on keybert\\n                coherence_map = sorted_map[pruning:][\\n                    ::-1\\n                ]  # get the last n - pruning values and reverse the list\\n                print(\\\"done pruning...\\\", len(coherence_map))\\n\\n            # truncate the strings for printing\\n            truncated_row = truncate_string(row, max_str_length)\\n            truncated_prev_row = truncate_string(prev_row, max_str_length)\\n\\n            # get the keywords for the current sentences\\n            keywords_current = keywords_lib.get_keywords_with_embeddings(row)\\n            keywords_prev = keywords_lib.get_keywords_with_embeddings(prev_row)\\n\\n            # compute the word comparisons between the previous (with the coherence map)\\n            # and the current (possibly the first sentence in a new segment)\\n            word_comparisons_with_coherence, weights = compare_coherent_words(\\n                [*coherence_map, keywords_prev], keywords_current\\n            )\\n\\n            similarities_with_coherence = [\\n                comparison[2] for comparison in word_comparisons_with_coherence\\n            ]\\n            avg_similarity_with_coherence = sum(similarities_with_coherence) / len(\\n                similarities_with_coherence\\n            )\\n            weighted_avg_similarity_with_coherence = get_weighted_average(\\n                similarities_with_coherence, weights\\n            )\\n            print(f\\\"weighted: {weighted_avg_similarity_with_coherence}\\\")\\n\\n            # if the two sentences are similar, create a cohesive prediction\\n            # otherwise, predict a new segment\\n            if weighted_avg_similarity_with_coherence > threshold:\\n                print(\\n                    f\\\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 0))\\n            else:\\n                # start of a new segment, empty the map\\n                coherence_map = []\\n                print(\\n                    f\\\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 1))\\n\\n            thresholds.append(weighted_avg_similarity_with_coherence)\\n\\n            print(\\\"===============================================\\\")\\n\\n    return predictions\";\n",
       "                var nbb_formatted_code = \"pruning = 0  # remove the lowest n important words from coherence map\\npruning_min = 10  # only prune after n words in the coherence map\\n\\n\\ndef get_weighted_average(weighted_similarities, weights):\\n    return sum(weighted_similarities) / sum(weights)\\n\\n\\n# importance testing\\ndef compare_coherent_words(coherence_map, keywords_current, suppress_errors=False):\\n    word_comparisons = []\\n    weights = []\\n    for i, keywords in enumerate(coherence_map[::-1]):\\n        for word_tuple in keywords:\\n            word = word_tuple[0]\\n            for second_word_tuple in keywords_current:\\n                second_word = second_word_tuple[0]\\n\\n                try:\\n                    word_one_emb = word_tuple[2]\\n                    word_two_emb = second_word_tuple[2]\\n                    weight = 1 / (\\n                        i + 1\\n                    )  # this weight is a recipricol function that will grow smaller the further the keywords are away\\n\\n                    word_comparisons.append(\\n                        (\\n                            word,\\n                            second_word,\\n                            weight\\n                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\\n                        )\\n                    )\\n                    weights.append(weight)\\n                except AssertionError as e:\\n                    if not suppress_errors:\\n                        print(e, word, second_word)\\n\\n    return word_comparisons, weights\\n\\n\\n# Adding dynamic threshold\\ndef coherence_tester(\\n    text_data,\\n    text_labels,\\n    max_tokens=400,\\n    max_str_length=30,\\n    prediction_thresh=0.25,\\n    dynamic_threshold=True,\\n    threshold_warmup=15,  # number of iterations before using dynamic threshold\\n    last_n_threshold=5,  # will only consider the last n thresholds for dynamic threshold\\n):\\n    coherence_map = []\\n    predictions = []\\n    thresholds = []\\n    for i, (row, label) in enumerate(zip(text_data, text_labels)):\\n        threshold = prediction_thresh\\n        if dynamic_threshold and (i + 1) > threshold_warmup:\\n            last_n_thresholds = thresholds[(0 - last_n_threshold) :]\\n            last_n_thresholds.sort()\\n            mid = len(last_n_thresholds) // 2\\n            threshold = (last_n_thresholds[mid] + last_n_thresholds[~mid]) / 2\\n            print(f\\\"median threshold: {threshold}\\\")\\n        # compare the current sentence to the previous one\\n        if i == 0:\\n            predictions.append((0, 0))\\n        else:\\n            prev_row = text_data[i - 1]\\n\\n            row = truncate_by_token(row, max_tokens)\\n            prev_row = truncate_by_token(prev_row, max_tokens)\\n\\n            # add the keywords to the coherence map\\n            coherence_map.append(\\n                coherence.get_coherence([row, prev_row], coherence_threshold=0.2)\\n            )\\n            print(f\\\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}\\\")\\n            if pruning > 0 and len(coherence_map) >= pruning_min:\\n                print(\\\"pruning...\\\", len(coherence_map))\\n                sorted_map = sorted(\\n                    coherence_map, key=lambda tup: tup[1]\\n                )  # sort asc by importance based on keybert\\n                coherence_map = sorted_map[pruning:][\\n                    ::-1\\n                ]  # get the last n - pruning values and reverse the list\\n                print(\\\"done pruning...\\\", len(coherence_map))\\n\\n            # truncate the strings for printing\\n            truncated_row = truncate_string(row, max_str_length)\\n            truncated_prev_row = truncate_string(prev_row, max_str_length)\\n\\n            # get the keywords for the current sentences\\n            keywords_current = keywords_lib.get_keywords_with_embeddings(row)\\n            keywords_prev = keywords_lib.get_keywords_with_embeddings(prev_row)\\n\\n            # compute the word comparisons between the previous (with the coherence map)\\n            # and the current (possibly the first sentence in a new segment)\\n            word_comparisons_with_coherence, weights = compare_coherent_words(\\n                [*coherence_map, keywords_prev], keywords_current\\n            )\\n\\n            similarities_with_coherence = [\\n                comparison[2] for comparison in word_comparisons_with_coherence\\n            ]\\n            avg_similarity_with_coherence = sum(similarities_with_coherence) / len(\\n                similarities_with_coherence\\n            )\\n            weighted_avg_similarity_with_coherence = get_weighted_average(\\n                similarities_with_coherence, weights\\n            )\\n            print(f\\\"weighted: {weighted_avg_similarity_with_coherence}\\\")\\n\\n            # if the two sentences are similar, create a cohesive prediction\\n            # otherwise, predict a new segment\\n            if weighted_avg_similarity_with_coherence > threshold:\\n                print(\\n                    f\\\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 0))\\n            else:\\n                # start of a new segment, empty the map\\n                coherence_map = []\\n                print(\\n                    f\\\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 1))\\n\\n            thresholds.append(weighted_avg_similarity_with_coherence)\\n\\n            print(\\\"===============================================\\\")\\n\\n    return predictions\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pruning = 0  # remove the lowest n important words from coherence map\n",
    "pruning_min = 10  # only prune after n words in the coherence map\n",
    "\n",
    "\n",
    "def get_weighted_average(weighted_similarities, weights):\n",
    "    return sum(weighted_similarities) / sum(weights)\n",
    "\n",
    "\n",
    "# importance testing\n",
    "def compare_coherent_words(coherence_map, keywords_current, suppress_errors=False):\n",
    "    word_comparisons = []\n",
    "    weights = []\n",
    "    for i, keywords in enumerate(coherence_map[::-1]):\n",
    "        for word_tuple in keywords:\n",
    "            word = word_tuple[0]\n",
    "            for second_word_tuple in keywords_current:\n",
    "                second_word = second_word_tuple[0]\n",
    "\n",
    "                try:\n",
    "                    word_one_emb = word_tuple[2]\n",
    "                    word_two_emb = second_word_tuple[2]\n",
    "                    weight = 1 / (\n",
    "                        i + 1\n",
    "                    )  # this weight is a recipricol function that will grow smaller the further the keywords are away\n",
    "\n",
    "                    word_comparisons.append(\n",
    "                        (\n",
    "                            word,\n",
    "                            second_word,\n",
    "                            weight\n",
    "                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\n",
    "                        )\n",
    "                    )\n",
    "                    weights.append(weight)\n",
    "                except AssertionError as e:\n",
    "                    if not suppress_errors:\n",
    "                        print(e, word, second_word)\n",
    "\n",
    "    return word_comparisons, weights\n",
    "\n",
    "\n",
    "# Adding dynamic threshold\n",
    "def coherence_tester(\n",
    "    text_data,\n",
    "    text_labels,\n",
    "    max_tokens=400,\n",
    "    max_str_length=30,\n",
    "    prediction_thresh=0.25,\n",
    "    dynamic_threshold=True,\n",
    "    threshold_warmup=15,  # number of iterations before using dynamic threshold\n",
    "    last_n_threshold=5,  # will only consider the last n thresholds for dynamic threshold\n",
    "):\n",
    "    coherence_map = []\n",
    "    predictions = []\n",
    "    thresholds = []\n",
    "    for i, (row, label) in enumerate(zip(text_data, text_labels)):\n",
    "        threshold = prediction_thresh\n",
    "        if dynamic_threshold and (i + 1) > threshold_warmup:\n",
    "            last_n_thresholds = thresholds[(0 - last_n_threshold) :]\n",
    "            last_n_thresholds.sort()\n",
    "            mid = len(last_n_thresholds) // 2\n",
    "            threshold = (last_n_thresholds[mid] + last_n_thresholds[~mid]) / 2\n",
    "            print(f\"median threshold: {threshold}\")\n",
    "        # compare the current sentence to the previous one\n",
    "        if i == 0:\n",
    "            predictions.append((0, 0))\n",
    "        else:\n",
    "            prev_row = text_data[i - 1]\n",
    "\n",
    "            row = truncate_by_token(row, max_tokens)\n",
    "            prev_row = truncate_by_token(prev_row, max_tokens)\n",
    "\n",
    "            # add the keywords to the coherence map\n",
    "            coherence_map.append(\n",
    "                coherence.get_coherence([row, prev_row], coherence_threshold=0.2)\n",
    "            )\n",
    "            print(f\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}\")\n",
    "            if pruning > 0 and len(coherence_map) >= pruning_min:\n",
    "                print(\"pruning...\", len(coherence_map))\n",
    "                sorted_map = sorted(\n",
    "                    coherence_map, key=lambda tup: tup[1]\n",
    "                )  # sort asc by importance based on keybert\n",
    "                coherence_map = sorted_map[pruning:][\n",
    "                    ::-1\n",
    "                ]  # get the last n - pruning values and reverse the list\n",
    "                print(\"done pruning...\", len(coherence_map))\n",
    "\n",
    "            # truncate the strings for printing\n",
    "            truncated_row = truncate_string(row, max_str_length)\n",
    "            truncated_prev_row = truncate_string(prev_row, max_str_length)\n",
    "\n",
    "            # get the keywords for the current sentences\n",
    "            keywords_current = keywords_lib.get_keywords_with_embeddings(row)\n",
    "            keywords_prev = keywords_lib.get_keywords_with_embeddings(prev_row)\n",
    "\n",
    "            # compute the word comparisons between the previous (with the coherence map)\n",
    "            # and the current (possibly the first sentence in a new segment)\n",
    "            word_comparisons_with_coherence, weights = compare_coherent_words(\n",
    "                [*coherence_map, keywords_prev], keywords_current\n",
    "            )\n",
    "\n",
    "            similarities_with_coherence = [\n",
    "                comparison[2] for comparison in word_comparisons_with_coherence\n",
    "            ]\n",
    "            avg_similarity_with_coherence = sum(similarities_with_coherence) / len(\n",
    "                similarities_with_coherence\n",
    "            )\n",
    "            weighted_avg_similarity_with_coherence = get_weighted_average(\n",
    "                similarities_with_coherence, weights\n",
    "            )\n",
    "            print(f\"weighted: {weighted_avg_similarity_with_coherence}\")\n",
    "\n",
    "            # if the two sentences are similar, create a cohesive prediction\n",
    "            # otherwise, predict a new segment\n",
    "            if weighted_avg_similarity_with_coherence > threshold:\n",
    "                print(\n",
    "                    f\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\"\n",
    "                )\n",
    "                predictions.append((weighted_avg_similarity_with_coherence, 0))\n",
    "            else:\n",
    "                # start of a new segment, empty the map\n",
    "                coherence_map = []\n",
    "                print(\n",
    "                    f\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\"\n",
    "                )\n",
    "                predictions.append((weighted_avg_similarity_with_coherence, 1))\n",
    "\n",
    "            thresholds.append(weighted_avg_similarity_with_coherence)\n",
    "\n",
    "            print(\"===============================================\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ca71b4cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Map: [['accredited', 'ada', 'accreditation', 'ada', 'program']]\n",
      "weighted: tensor([0.3225])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3225])\n",
      "===============================================\n",
      "Coherence Map: [['accredited', 'ada', 'accreditation', 'ada', 'program'], ['ada', 'accredited', 'center', 'accredited', 'primary']]\n",
      "weighted: tensor([0.2896])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2896])\n",
      "===============================================\n",
      "Coherence Map: [['accredited', 'ada', 'accreditation', 'ada', 'program'], ['ada', 'accredited', 'center', 'accredited', 'primary'], ['technology', 'ada', 'ada', 'ada', 'located']]\n",
      "weighted: tensor([0.3280])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3280])\n",
      "===============================================\n",
      "Coherence Map: [['accredited', 'ada', 'accreditation', 'ada', 'program'], ['ada', 'accredited', 'center', 'accredited', 'primary'], ['technology', 'ada', 'ada', 'ada', 'located'], ['prosecutor', 'tech', 'prosecutors', 'tech', 'prosecutor']]\n",
      "weighted: tensor([0.2445])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2445])\n",
      "===============================================\n",
      "Coherence Map: [['jorge', 'convicted', 'anchoring', 'convicted', 'nacional']]\n",
      "weighted: tensor([0.2413])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.2413])\n",
      "===============================================\n",
      "Coherence Map: [['settlers', 'patagonia', 'settlement', 'patagonia', 'mineral']]\n",
      "weighted: tensor([0.3147])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3147])\n",
      "===============================================\n",
      "Coherence Map: [['settlers', 'patagonia', 'settlement', 'patagonia', 'mineral'], ['precipitation', 'settlers', 'winter', 'settlers', 'rain']]\n",
      "weighted: tensor([0.3123])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3123])\n",
      "===============================================\n",
      "Coherence Map: [['settlers', 'patagonia', 'settlement', 'patagonia', 'mineral'], ['precipitation', 'settlers', 'winter', 'settlers', 'rain'], ['2001', 'precipitation', 'inhabitants', 'precipitation', 'population']]\n",
      "weighted: tensor([0.3061])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3061])\n",
      "===============================================\n",
      "Coherence Map: [['settlers', 'patagonia', 'settlement', 'patagonia', 'mineral'], ['precipitation', 'settlers', 'winter', 'settlers', 'rain'], ['2001', 'precipitation', 'inhabitants', 'precipitation', 'population'], ['isidro', 'patagonia', 'julio', 'patagonia', 'san']]\n",
      "weighted: tensor([0.3031])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3031])\n",
      "===============================================\n",
      "Coherence Map: [['settlers', 'patagonia', 'settlement', 'patagonia', 'mineral'], ['precipitation', 'settlers', 'winter', 'settlers', 'rain'], ['2001', 'precipitation', 'inhabitants', 'precipitation', 'population'], ['isidro', 'patagonia', 'julio', 'patagonia', 'san'], ['factories', 'isidro', 'industrial', 'isidro', 'shipyard']]\n",
      "weighted: tensor([0.2917])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2917])\n",
      "===============================================\n",
      "Coherence Map: [['settlers', 'patagonia', 'settlement', 'patagonia', 'mineral'], ['precipitation', 'settlers', 'winter', 'settlers', 'rain'], ['2001', 'precipitation', 'inhabitants', 'precipitation', 'population'], ['isidro', 'patagonia', 'julio', 'patagonia', 'san'], ['factories', 'isidro', 'industrial', 'isidro', 'shipyard'], ['drilling', 'factories', 'oil', 'factories', 'gulf']]\n",
      "weighted: tensor([0.3311])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3311])\n",
      "===============================================\n",
      "Coherence Map: [['settlers', 'patagonia', 'settlement', 'patagonia', 'mineral'], ['precipitation', 'settlers', 'winter', 'settlers', 'rain'], ['2001', 'precipitation', 'inhabitants', 'precipitation', 'population'], ['isidro', 'patagonia', 'julio', 'patagonia', 'san'], ['factories', 'isidro', 'industrial', 'isidro', 'shipyard'], ['drilling', 'factories', 'oil', 'factories', 'gulf'], ['jorge', 'drilling', 'argentina', 'drilling', 'san']]\n",
      "weighted: tensor([0.2998])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2998])\n",
      "===============================================\n",
      "Coherence Map: [['settlers', 'patagonia', 'settlement', 'patagonia', 'mineral'], ['precipitation', 'settlers', 'winter', 'settlers', 'rain'], ['2001', 'precipitation', 'inhabitants', 'precipitation', 'population'], ['isidro', 'patagonia', 'julio', 'patagonia', 'san'], ['factories', 'isidro', 'industrial', 'isidro', 'shipyard'], ['drilling', 'factories', 'oil', 'factories', 'gulf'], ['jorge', 'drilling', 'argentina', 'drilling', 'san'], ['latitude', 'jorge', 'cartography', 'jorge', 'gmt']]\n",
      "weighted: tensor([0.2397])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2397])\n",
      "===============================================\n",
      "Coherence Map: [['port', 'longitude', '1908', 'longitude', 'port']]\n",
      "weighted: tensor([0.2526])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2526])\n",
      "===============================================\n",
      "median threshold: tensor([0.2917])\n",
      "Coherence Map: [['port', 'longitude', '1908', 'longitude', 'port'], ['shipyard', 'port', 'naval', 'port', 'buoy']]\n",
      "weighted: tensor([0.3697])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3697])\n",
      "===============================================\n",
      "median threshold: tensor([0.2998])\n",
      "Coherence Map: [['port', 'longitude', '1908', 'longitude', 'port'], ['shipyard', 'port', 'naval', 'port', 'buoy'], ['concrete', 'shipyard', 'property', 'shipyard', 'tons']]\n",
      "weighted: tensor([0.3147])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3147])\n",
      "===============================================\n",
      "median threshold: tensor([0.2998])\n",
      "Coherence Map: [['port', 'longitude', '1908', 'longitude', 'port'], ['shipyard', 'port', 'naval', 'port', 'buoy'], ['concrete', 'shipyard', 'property', 'shipyard', 'tons'], ['energy', 'concrete', 'wind', 'concrete', 'capacity']]\n",
      "weighted: tensor([0.3354])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3354])\n",
      "===============================================\n",
      "median threshold: tensor([0.3147])\n",
      "Coherence Map: [['port', 'longitude', '1908', 'longitude', 'port'], ['shipyard', 'port', 'naval', 'port', 'buoy'], ['concrete', 'shipyard', 'property', 'shipyard', 'tons'], ['energy', 'concrete', 'wind', 'concrete', 'capacity'], ['football', 'energy', 'basketball', 'energy', 'sport']]\n",
      "weighted: tensor([0.3094])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3094])\n",
      "===============================================\n",
      "median threshold: tensor([0.3147])\n",
      "Coherence Map: [['soviet', 'football', 'ukrainian', 'football', 'jews']]\n",
      "weighted: tensor([0.2866])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.2866])\n",
      "===============================================\n",
      "median threshold: tensor([0.3147])\n",
      "Coherence Map: [['chattanooga', 'soviet', 'founded', 'ukrainian', 'nashville']]\n",
      "weighted: tensor([0.2603])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.2603])\n",
      "===============================================\n",
      "median threshold: tensor([0.3094])\n",
      "Coherence Map: [['hohenwald', 'swiss', 'census', 'founded', 'area']]\n",
      "weighted: tensor([0.2795])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2795])\n",
      "===============================================\n",
      "median threshold: tensor([0.2866])\n",
      "Coherence Map: [['households', 'census', 'median', 'census', 'census']]\n",
      "weighted: tensor([0.3453])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3453])\n",
      "===============================================\n",
      "median threshold: tensor([0.2866])\n",
      "Coherence Map: [['households', 'census', 'median', 'census', 'census'], ['km²', 'households', 'census', 'households', 'land']]\n",
      "weighted: tensor([0.3488])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.3488])\n",
      "===============================================\n",
      "median threshold: tensor([0.2866])\n",
      "Coherence Map: [['households', 'census', 'median', 'census', 'census'], ['km²', 'households', 'census', 'households', 'land'], ['households', 'km²', 'median', 'km²', 'census']]\n",
      "weighted: tensor([0.3826])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3826])\n",
      "===============================================\n",
      "median threshold: tensor([0.3453])\n",
      "Coherence Map: [['households', 'census', 'median', 'census', 'census'], ['km²', 'households', 'census', 'households', 'land'], ['households', 'km²', 'median', 'km²', 'census'], ['manufacturing', 'households', 'plant', 'households', 'community']]\n",
      "weighted: tensor([0.2790])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2790])\n",
      "===============================================\n",
      "median threshold: tensor([0.3453])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Map: [['academy', 'county', 'springs', 'county', 'bay']]\n",
      "weighted: tensor([0.3357])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3357])\n",
      "===============================================\n",
      "median threshold: tensor([0.3453])\n",
      "Coherence Map: [['climate', 'springs', 'humid', 'springs', 'subtropical']]\n",
      "weighted: tensor([0.2988])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2988])\n",
      "===============================================\n",
      "median threshold: tensor([0.3357])\n",
      "Coherence Map: [['canal', 'climate', 'heritage', 'climate', 'courthouse']]\n",
      "weighted: tensor([0.2241])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.2241])\n",
      "===============================================\n",
      "median threshold: tensor([0.2988])\n",
      "Coherence Map: [['delphi', 'delphi', 'census', 'canal', 'area']]\n",
      "weighted: tensor([0.2641])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2641])\n",
      "===============================================\n",
      "median threshold: tensor([0.2790])\n",
      "Coherence Map: [['households', 'census', 'census', 'census', 'household']]\n",
      "weighted: tensor([0.3071])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3071])\n",
      "===============================================\n",
      "median threshold: tensor([0.2988])\n",
      "Coherence Map: [['households', 'census', 'census', 'census', 'household'], ['households', 'households', 'census', 'households', 'median']]\n",
      "weighted: tensor([0.4617])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4617])\n",
      "===============================================\n",
      "median threshold: tensor([0.2988])\n",
      "Coherence Map: [['households', 'census', 'census', 'census', 'household'], ['households', 'households', 'census', 'households', 'median'], ['corporation', 'households', 'public', 'households', 'delphi']]\n",
      "weighted: tensor([0.2521])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2521])\n",
      "===============================================\n",
      "median threshold: tensor([0.2641])\n",
      "Coherence Map: [['winter', 'community', 'rain', 'community', 'summer']]\n",
      "weighted: tensor([0.2822])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.2822])\n",
      "===============================================\n",
      "median threshold: tensor([0.2822])\n",
      "Coherence Map: [['winter', 'community', 'rain', 'community', 'summer'], ['dialect', 'winter', 'language', 'winter', 'inhabitants']]\n",
      "weighted: tensor([0.3501])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3501])\n",
      "===============================================\n",
      "median threshold: tensor([0.3071])\n",
      "Coherence Map: [['winter', 'community', 'rain', 'community', 'summer'], ['dialect', 'winter', 'language', 'winter', 'inhabitants'], ['helix', 'dialect', 'ear', 'dialect', 'named']]\n",
      "weighted: tensor([0.3093])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.3093])\n",
      "===============================================\n",
      "median threshold: tensor([0.3093])\n",
      "Coherence Map: [['winter', 'community', 'rain', 'community', 'summer'], ['dialect', 'winter', 'language', 'winter', 'inhabitants'], ['helix', 'dialect', 'ear', 'dialect', 'named'], ['census', 'named', 'area', 'named', 'total']]\n",
      "weighted: tensor([0.2636])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2636])\n",
      "===============================================\n",
      "median threshold: tensor([0.2822])\n",
      "Coherence Map: [['households', 'census', 'census', 'census', 'residents']]\n",
      "weighted: tensor([0.3708])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3708])\n",
      "===============================================\n",
      "median threshold: tensor([0.3093])\n",
      "Coherence Map: [['households', 'census', 'census', 'census', 'residents'], ['households', 'households', 'median', 'households', 'census']]\n",
      "weighted: tensor([0.4944])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4944])\n",
      "===============================================\n",
      "median threshold: tensor([0.3501])\n",
      "Coherence Map: [['households', 'census', 'census', 'census', 'residents'], ['households', 'households', 'median', 'households', 'census'], ['farming', 'households', 'farm', 'households', 'crop']]\n",
      "weighted: tensor([0.2767])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2767])\n",
      "===============================================\n",
      "median threshold: tensor([0.3093])\n",
      "Coherence Map: [['rodeo', 'crop', 'festival', 'crop', 'annual']]\n",
      "weighted: tensor([0.4041])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4041])\n",
      "===============================================\n",
      "median threshold: tensor([0.3708])\n",
      "Coherence Map: [['rodeo', 'crop', 'festival', 'crop', 'annual'], ['altitude', 'rodeo', 'region', 'rodeo', 'altitude']]\n",
      "weighted: tensor([0.2589])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.2589])\n",
      "===============================================\n",
      "median threshold: tensor([0.3708])\n",
      "Coherence Map: [['agricultural', 'paulo', 'corn', 'paulo', 'soybeans']]\n",
      "weighted: tensor([0.3638])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3638])\n",
      "===============================================\n",
      "median threshold: tensor([0.3638])\n",
      "Coherence Map: [['founded', 'agricultural', 'guarani', 'agricultural', 'waterfall']]\n",
      "weighted: tensor([0.3103])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3103])\n",
      "===============================================\n",
      "median threshold: tensor([0.3103])\n",
      "Coherence Map: [['etymology', 'founded', 'albanian', 'founded', 'croatian']]\n",
      "weighted: tensor([0.2844])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.2844])\n",
      "===============================================\n",
      "median threshold: tensor([0.3103])\n",
      "Coherence Map: [['albania', 'etymology', 'rivers', 'etymology', 'adriatic']]\n",
      "weighted: tensor([0.3251])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3251])\n",
      "===============================================\n",
      "median threshold: tensor([0.3103])\n",
      "Coherence Map: [['albania', 'etymology', 'rivers', 'etymology', 'adriatic'], ['climate', 'albania', 'humid', 'rivers', 'precipitation']]\n",
      "weighted: tensor([0.2647])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2647])\n",
      "===============================================\n",
      "median threshold: tensor([0.3103])\n",
      "Coherence Map: [['byzantine', 'climate', 'macedonia', 'climate', 'serbian']]\n",
      "weighted: tensor([0.2856])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2856])\n",
      "===============================================\n",
      "median threshold: tensor([0.2856])\n",
      "Coherence Map: [['ottomans', 'byzantine', 'ottoman', 'byzantine', 'sieges']]\n",
      "weighted: tensor([0.3806])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3806])\n",
      "===============================================\n",
      "median threshold: tensor([0.2856])\n",
      "Coherence Map: [['ottomans', 'byzantine', 'ottoman', 'byzantine', 'sieges'], ['albanian', 'ottomans', 'albania', 'ottomans', 'serbian']]\n",
      "weighted: tensor([0.4264])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4264])\n",
      "===============================================\n",
      "median threshold: tensor([0.3251])\n",
      "Coherence Map: [['ottomans', 'byzantine', 'ottoman', 'byzantine', 'sieges'], ['albanian', 'ottomans', 'albania', 'ottomans', 'serbian'], ['albanians', 'albanian', 'albania', 'albanian', 'archdiocese']]\n",
      "weighted: tensor([0.3929])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3929])\n",
      "===============================================\n",
      "median threshold: tensor([0.3806])\n",
      "Coherence Map: [['ottomans', 'byzantine', 'ottoman', 'byzantine', 'sieges'], ['albanian', 'ottomans', 'albania', 'ottomans', 'serbian'], ['albanians', 'albanian', 'albania', 'albanian', 'archdiocese'], ['municipality', 'albanians', 'municipalities', 'albanians', 'municipal']]\n",
      "weighted: tensor([0.3000])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3000])\n",
      "===============================================\n",
      "median threshold: tensor([0.3806])\n",
      "Coherence Map: [['serbia', 'municipality', 'production', 'municipality', 'manufacture']]\n",
      "weighted: tensor([0.3189])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3189])\n",
      "===============================================\n",
      "median threshold: tensor([0.3806])\n",
      "Coherence Map: [['albania', 'manufacturing', 'tirana', 'manufacturing', 'albanian']]\n",
      "weighted: tensor([0.3432])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3432])\n",
      "===============================================\n",
      "median threshold: tensor([0.3432])\n",
      "Coherence Map: [['balkan', 'albania', 'albania', 'albania', 'pasha']]\n",
      "weighted: tensor([0.3571])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3571])\n",
      "===============================================\n",
      "median threshold: tensor([0.3432])\n",
      "Coherence Map: [['balkan', 'albania', 'albania', 'albania', 'pasha'], ['accordion', 'albania', 'violin', 'albania', 'accordion']]\n",
      "weighted: tensor([0.2796])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2796])\n",
      "===============================================\n",
      "median threshold: tensor([0.3189])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Map: [['citadel', 'accordion', 'buildings', 'accordion', 'rebuilt']]\n",
      "weighted: tensor([0.3313])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3313])\n",
      "===============================================\n",
      "median threshold: tensor([0.3313])\n",
      "Coherence Map: [['citadel', 'accordion', 'buildings', 'accordion', 'rebuilt'], ['personalities', 'citadel', 'born', 'citadel', 'residents']]\n",
      "weighted: tensor([0.3830])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3830])\n",
      "===============================================\n",
      "median threshold: tensor([0.3432])\n",
      "Coherence Map: [['citadel', 'accordion', 'buildings', 'accordion', 'rebuilt'], ['personalities', 'citadel', 'born', 'citadel', 'residents'], ['officials', 'personalities', 'caused', 'personalities', 'teachers']]\n",
      "weighted: tensor([0.3611])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.3611])\n",
      "===============================================\n",
      "median threshold: tensor([0.3571])\n",
      "Coherence Map: [['citadel', 'accordion', 'buildings', 'accordion', 'rebuilt'], ['personalities', 'citadel', 'born', 'citadel', 'residents'], ['officials', 'personalities', 'caused', 'personalities', 'teachers'], ['districts', 'officials', 'district', 'officials', 'enclave']]\n",
      "weighted: tensor([0.3099])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3099])\n",
      "===============================================\n",
      "median threshold: tensor([0.3313])\n",
      "Coherence Map: [['rivers', 'districts', 'northwest', 'districts', 'canal']]\n",
      "weighted: tensor([0.3242])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3242])\n",
      "===============================================\n",
      "median threshold: tensor([0.3313])\n",
      "Coherence Map: [['winters', 'rivers', 'climate', 'rivers', 'rainfall']]\n",
      "weighted: tensor([0.3757])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3757])\n",
      "===============================================\n",
      "median threshold: tensor([0.3611])\n",
      "Coherence Map: [['winters', 'rivers', 'climate', 'rivers', 'rainfall'], ['census', 'winters', 'population', 'winters', 'inhabitants']]\n",
      "weighted: tensor([0.3516])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3516])\n",
      "===============================================\n",
      "median threshold: tensor([0.3516])\n",
      "Coherence Map: [['gdp', 'census', 'billion', 'census', 'china']]\n",
      "weighted: tensor([0.3270])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3270])\n",
      "===============================================\n",
      "median threshold: tensor([0.3270])\n",
      "Coherence Map: [['railroad', 'gdp', '1871', 'gdp', 'built']]\n",
      "weighted: tensor([0.2998])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.2998])\n",
      "===============================================\n",
      "median threshold: tensor([0.3270])\n",
      "Coherence Map: [['census', 'railroad', 'area', 'railroad', 'bureau']]\n",
      "weighted: tensor([0.2886])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2886])\n",
      "===============================================\n",
      "median threshold: tensor([0.3270])\n",
      "Coherence Map: [['households', 'census', 'census', 'census', 'household']]\n",
      "weighted: tensor([0.3451])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3451])\n",
      "===============================================\n",
      "median threshold: tensor([0.3270])\n",
      "Coherence Map: [['households', 'census', 'census', 'census', 'household'], ['households', 'households', 'median', 'households', 'population']]\n",
      "weighted: tensor([0.5050])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5050])\n",
      "===============================================\n",
      "median threshold: tensor([0.3270])\n",
      "Coherence Map: [['households', 'census', 'census', 'census', 'household'], ['households', 'households', 'median', 'households', 'population'], ['railroad', 'households', 'unsolved', 'households', 'railroad']]\n",
      "weighted: tensor([0.1857])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.1857])\n",
      "===============================================\n",
      "median threshold: tensor([0.2998])\n",
      "Coherence Map: [['grid', 'railroad', 'highway', 'railroad', 'routes']]\n",
      "weighted: tensor([0.2888])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2888])\n",
      "===============================================\n",
      "median threshold: tensor([0.2888])\n",
      "Coherence Map: [['households', 'grid', 'residents', 'grid', 'population']]\n",
      "weighted: tensor([0.2805])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2805])\n",
      "===============================================\n",
      "median threshold: tensor([0.2888])\n",
      "Coherence Map: [['households', 'households', 'median', 'households', 'census']]\n",
      "weighted: tensor([0.5068])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5068])\n",
      "===============================================\n",
      "median threshold: tensor([0.2888])\n",
      "Coherence Map: [['households', 'households', 'median', 'households', 'census'], ['lutheran', 'households', 'churches', 'households', 'church']]\n",
      "weighted: tensor([0.3005])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3005])\n",
      "===============================================\n",
      "median threshold: tensor([0.2888])\n",
      "Coherence Map: [['households', 'households', 'median', 'households', 'census'], ['lutheran', 'households', 'churches', 'households', 'church'], ['agriculture', 'lutheran', 'agribusiness', 'lutheran', 'farming']]\n",
      "weighted: tensor([0.2915])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2915])\n",
      "===============================================\n",
      "median threshold: tensor([0.2915])\n",
      "Coherence Map: [['households', 'households', 'median', 'households', 'census'], ['lutheran', 'households', 'churches', 'households', 'church'], ['agriculture', 'lutheran', 'agribusiness', 'lutheran', 'farming'], ['classrooms', 'agriculture', 'district', 'agriculture', 'library']]\n",
      "weighted: tensor([0.2713])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2713])\n",
      "===============================================\n",
      "median threshold: tensor([0.2915])\n",
      "Coherence Map: [['warren', 'warren', 'polk', 'warren', 'warren']]\n",
      "weighted: tensor([0.2753])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2753])\n",
      "===============================================\n",
      "median threshold: tensor([0.2915])\n",
      "Coherence Map: [['renamed', 'polk', 'deadwood', 'polk', 'established']]\n",
      "weighted: tensor([0.2779])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.2779])\n",
      "===============================================\n",
      "median threshold: tensor([0.2779])\n",
      "Coherence Map: [['confederate', 'immigrants', 'general', 'immigrants', 'general']]\n",
      "weighted: tensor([0.2740])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2740])\n",
      "===============================================\n",
      "median threshold: tensor([0.2753])\n",
      "Coherence Map: [['jones', 'confederate', 'census', 'confederate', 'area']]\n",
      "weighted: tensor([0.2551])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2551])\n",
      "===============================================\n",
      "median threshold: tensor([0.2740])\n",
      "Coherence Map: [['climate', 'census', 'temperatures', 'census', 'warm']]\n",
      "weighted: tensor([0.3322])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3322])\n",
      "===============================================\n",
      "median threshold: tensor([0.2753])\n",
      "Coherence Map: [['climate', 'census', 'temperatures', 'census', 'warm'], ['census', 'climate', 'household', 'climate', 'individuals']]\n",
      "weighted: tensor([0.3276])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3276])\n",
      "===============================================\n",
      "median threshold: tensor([0.2779])\n",
      "Coherence Map: [['climate', 'census', 'temperatures', 'census', 'warm'], ['census', 'climate', 'household', 'climate', 'individuals'], ['households', 'households', 'household', 'households', 'population']]\n",
      "weighted: tensor([0.4242])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4242])\n",
      "===============================================\n",
      "median threshold: tensor([0.3276])\n",
      "Coherence Map: [['climate', 'census', 'temperatures', 'census', 'warm'], ['census', 'climate', 'household', 'climate', 'individuals'], ['households', 'households', 'household', 'households', 'population'], ['jones', 'census', 'state', 'census', 'federally']]\n",
      "weighted: tensor([0.2680])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2680])\n",
      "===============================================\n",
      "median threshold: tensor([0.3276])\n",
      "Coherence Map: [['brandt', 'state', 'provinces', 'federally', 'bohemia']]\n",
      "weighted: tensor([0.2223])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.2223])\n",
      "===============================================\n",
      "median threshold: tensor([0.3276])\n",
      "Coherence Map: [['northwestern', 'brandt', 'ridge', 'brandt', 'county']]\n",
      "weighted: tensor([0.2481])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.2481])\n",
      "===============================================\n",
      "median threshold: tensor([0.2680])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Map: [['population', 'northwest', 'census', 'county', 'household']]\n",
      "weighted: tensor([0.2720])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2720])\n",
      "===============================================\n",
      "median threshold: tensor([0.2680])\n",
      "Coherence Map: [['population', 'northwest', 'census', 'county', 'household'], ['district', 'households', 'district', 'census', 'residents']]\n",
      "weighted: tensor([0.2137])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2137])\n",
      "===============================================\n",
      "median threshold: tensor([0.2481])\n",
      "Coherence Map: [['humid', 'district', 'subtropical', 'district', 'precipitation']]\n",
      "weighted: tensor([0.2569])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2569])\n",
      "===============================================\n",
      "median threshold: tensor([0.2481])\n",
      "Coherence Map: [['humid', 'district', 'subtropical', 'district', 'precipitation'], ['climate', 'climate', 'savanna', 'climate', 'climate']]\n",
      "weighted: tensor([0.3798])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.3798])\n",
      "===============================================\n",
      "median threshold: tensor([0.2569])\n",
      "Coherence Map: [['humid', 'district', 'subtropical', 'district', 'precipitation'], ['climate', 'climate', 'savanna', 'climate', 'climate'], ['settlements', 'climate', 'settlers', 'climate', 'colony']]\n",
      "weighted: tensor([0.2397])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.2397])\n",
      "===============================================\n",
      "median threshold: tensor([0.2569])\n",
      "Coherence Map: [['isle', 'settlements', 'île', 'settlements', 'webster']]\n",
      "weighted: tensor([0.3692])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3692])\n",
      "===============================================\n",
      "median threshold: tensor([0.2569])\n",
      "Coherence Map: [['isle', 'settlements', 'île', 'settlements', 'webster'], ['factory', 'isle', 'mill', 'isle', 'lumber']]\n",
      "weighted: tensor([0.3307])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3307])\n",
      "===============================================\n",
      "median threshold: tensor([0.3307])\n",
      "Coherence Map: [['isle', 'settlements', 'île', 'settlements', 'webster'], ['factory', 'isle', 'mill', 'isle', 'lumber'], ['agricultural', 'factory', 'agriculture', 'factory', 'potatoes']]\n",
      "weighted: tensor([0.3949])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3949])\n",
      "===============================================\n",
      "median threshold: tensor([0.3692])\n",
      "Coherence Map: [['isle', 'settlements', 'île', 'settlements', 'webster'], ['factory', 'isle', 'mill', 'isle', 'lumber'], ['agricultural', 'factory', 'agriculture', 'factory', 'potatoes'], ['railway', 'agricultural', 'railroad', 'agricultural', 'rail']]\n",
      "weighted: tensor([0.3882])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3882])\n",
      "===============================================\n",
      "median threshold: tensor([0.3692])\n",
      "Coherence Map: [['isle', 'settlements', 'île', 'settlements', 'webster'], ['factory', 'isle', 'mill', 'isle', 'lumber'], ['agricultural', 'factory', 'agriculture', 'factory', 'potatoes'], ['railway', 'agricultural', 'railroad', 'agricultural', 'rail'], ['military', 'railway', 'airport', 'railway', 'maine']]\n",
      "weighted: tensor([0.3233])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3233])\n",
      "===============================================\n",
      "median threshold: tensor([0.3692])\n",
      "Coherence Map: [['college', 'military', 'university', 'military', 'students']]\n",
      "weighted: tensor([0.3176])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3176])\n",
      "===============================================\n",
      "median threshold: tensor([0.3307])\n",
      "Coherence Map: [['hospital', 'maine', 'nurses', 'maine', 'construction']]\n",
      "weighted: tensor([0.3317])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3317])\n",
      "===============================================\n",
      "median threshold: tensor([0.3317])\n",
      "Coherence Map: [['hospital', 'maine', 'nurses', 'maine', 'construction'], ['balloon', 'hospital', 'celebration', 'hospital', 'event']]\n",
      "weighted: tensor([0.3727])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3727])\n",
      "===============================================\n",
      "median threshold: tensor([0.3317])\n",
      "Coherence Map: [['hospital', 'maine', 'nurses', 'maine', 'construction'], ['balloon', 'hospital', 'celebration', 'hospital', 'event'], ['isle', 'balloon', 'humid', 'balloon', 'climate']]\n",
      "weighted: tensor([0.2705])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2705])\n",
      "===============================================\n",
      "median threshold: tensor([0.3233])\n",
      "Coherence Map: [['census', 'isle', 'household', 'isle', 'residents']]\n",
      "weighted: tensor([0.3223])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3223])\n",
      "===============================================\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 80;\n",
       "                var nbb_unformatted_code = \"start = 100\\nnum_samples = 100\\nmax_tokens = 256  # want to keep this under 512\\nmax_str_length = 30\\n\\ntrue_labels = text_labels[start : start + num_samples]\\n\\npredictions = coherence_tester(\\n    text_data[start : start + num_samples],\\n    true_labels,\\n    max_tokens=max_tokens,\\n    max_str_length=max_str_length,\\n)\";\n",
       "                var nbb_formatted_code = \"start = 100\\nnum_samples = 100\\nmax_tokens = 256  # want to keep this under 512\\nmax_str_length = 30\\n\\ntrue_labels = text_labels[start : start + num_samples]\\n\\npredictions = coherence_tester(\\n    text_data[start : start + num_samples],\\n    true_labels,\\n    max_tokens=max_tokens,\\n    max_str_length=max_str_length,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = 100\n",
    "num_samples = 100\n",
    "max_tokens = 256  # want to keep this under 512\n",
    "max_str_length = 30\n",
    "\n",
    "true_labels = text_labels[start : start + num_samples]\n",
    "\n",
    "predictions = coherence_tester(\n",
    "    text_data[start : start + num_samples],\n",
    "    true_labels,\n",
    "    max_tokens=max_tokens,\n",
    "    max_str_length=max_str_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d3a93727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 81;\n",
       "                var nbb_unformatted_code = \"print([x[1] for x in predictions])\\nprint(true_labels)\";\n",
       "                var nbb_formatted_code = \"print([x[1] for x in predictions])\\nprint(true_labels)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print([x[1] for x in predictions])\n",
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "61e7863f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 82;\n",
       "                var nbb_unformatted_code = \"pred_string = \\\"\\\".join(str([x[1] for x in predictions]))\\ntrue_string = \\\"\\\".join(str(true_labels))\";\n",
       "                var nbb_formatted_code = \"pred_string = \\\"\\\".join(str([x[1] for x in predictions]))\\ntrue_string = \\\"\\\".join(str(true_labels))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_string = \"\".join(str([x[1] for x in predictions]))\n",
    "true_string = \"\".join(str(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "19af16ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 83;\n",
       "                var nbb_unformatted_code = \"avg_k = len(true_labels) // true_labels.count(1)  # get avg segment size\";\n",
       "                var nbb_formatted_code = \"avg_k = len(true_labels) // true_labels.count(1)  # get avg segment size\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_k = len(true_labels) // true_labels.count(1)  # get avg segment size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "db43c420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 5\n",
      "wd = 0.5540540540540541\n",
      "pk = 0.4594594594594595\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 84;\n",
       "                var nbb_unformatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_formatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wd_score = windowdiff(pred_string, true_string, avg_k)\n",
    "pk_score = pk(pred_string, true_string, avg_k)\n",
    "\n",
    "print(f\"k = {avg_k}\")\n",
    "print(f\"wd = {wd_score}\")\n",
    "print(f\"pk = {pk_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb16194",
   "metadata": {},
   "source": [
    "## Prediction Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af208503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 66;\n",
       "                var nbb_unformatted_code = \"pred_thresh = 0.26\";\n",
       "                var nbb_formatted_code = \"pred_thresh = 0.26\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_thresh = 0.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9cebcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 67;\n",
       "                var nbb_unformatted_code = \"modified_predictions = [\\n    1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\\n]\\n\\npred_string = \\\"\\\".join(str(modified_predictions))\\ntrue_string = \\\"\\\".join(str(true_labels))\\n\\navg_k = len(true_labels) // true_labels.count(1)  # get avg segment size\";\n",
       "                var nbb_formatted_code = \"modified_predictions = [\\n    1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\\n]\\n\\npred_string = \\\"\\\".join(str(modified_predictions))\\ntrue_string = \\\"\\\".join(str(true_labels))\\n\\navg_k = len(true_labels) // true_labels.count(1)  # get avg segment size\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modified_predictions = [\n",
    "    1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\n",
    "]\n",
    "\n",
    "pred_string = \"\".join(str(modified_predictions))\n",
    "true_string = \"\".join(str(true_labels))\n",
    "\n",
    "avg_k = len(true_labels) // true_labels.count(1)  # get avg segment size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b9dbb752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 5\n",
      "wd = 0.23648648648648649\n",
      "pk = 0.21621621621621623\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 68;\n",
       "                var nbb_unformatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_formatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wd_score = windowdiff(pred_string, true_string, avg_k)\n",
    "pk_score = pk(pred_string, true_string, avg_k)\n",
    "\n",
    "print(f\"k = {avg_k}\")\n",
    "print(f\"wd = {wd_score}\")\n",
    "print(f\"pk = {pk_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a8471",
   "metadata": {},
   "source": [
    "## Sample Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a97f25cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"segments_to_test = 10\\n\\ntext_segments_to_check = [\\n    [truncate_by_token(s, max_tokens) for s in segment]\\n    for segment in text_segments[:segments_to_test]\\n]\\ntext_labels_to_check = segments_labels[:segments_to_test]\";\n",
       "                var nbb_formatted_code = \"segments_to_test = 10\\n\\ntext_segments_to_check = [\\n    [truncate_by_token(s, max_tokens) for s in segment]\\n    for segment in text_segments[:segments_to_test]\\n]\\ntext_labels_to_check = segments_labels[:segments_to_test]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "segments_to_test = 10\n",
    "\n",
    "text_segments_to_check = [\n",
    "    [truncate_by_token(s, max_tokens) for s in segment]\n",
    "    for segment in text_segments[:segments_to_test]\n",
    "]\n",
    "text_labels_to_check = segments_labels[:segments_to_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "583ba1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 36)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"len(text_segments_to_check[0]), len(text_labels_to_check[0])\";\n",
       "                var nbb_formatted_code = \"len(text_segments_to_check[0]), len(text_labels_to_check[0])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(text_segments_to_check[0]), len(text_labels_to_check[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d1c6816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 41;\n",
       "                var nbb_unformatted_code = \"different_segment_1 = [\\n    text_segments_to_check[0][-3],\\n    text_segments_to_check[0][-2],\\n    text_segments_to_check[0][-1],\\n    text_segments_to_check[1][0],\\n    text_segments_to_check[1][1],\\n    text_segments_to_check[1][2],\\n]\\ndifferent_segment_2 = [\\n    text_segments_to_check[1][-3],\\n    text_segments_to_check[1][-2],\\n    text_segments_to_check[1][-1],\\n    text_segments_to_check[2][0],\\n    text_segments_to_check[2][1],\\n    text_segments_to_check[2][2],\\n]\\ndifferent_segment_3 = [\\n    text_segments_to_check[2][-3],\\n    text_segments_to_check[2][-2],\\n    text_segments_to_check[2][-1],\\n    text_segments_to_check[3][0],\\n    text_segments_to_check[3][1],\\n    text_segments_to_check[3][2],\\n]\\ndifferent_segment_4 = [\\n    text_segments_to_check[3][-3],\\n    text_segments_to_check[3][-2],\\n    text_segments_to_check[3][-1],\\n    text_segments_to_check[4][0],\\n    text_segments_to_check[4][1],\\n    text_segments_to_check[4][2],\\n]\\ndifferent_segment_5 = [\\n    text_segments_to_check[4][-3],\\n    text_segments_to_check[4][-2],\\n    text_segments_to_check[4][-1],\\n    text_segments_to_check[5][0],\\n    text_segments_to_check[5][1],\\n    text_segments_to_check[5][2],\\n]\\n\\nsame_segment_1 = [\\n    text_segments_to_check[0][0],\\n    text_segments_to_check[0][1],\\n    text_segments_to_check[0][2],\\n]\\nsame_segment_2 = [\\n    text_segments_to_check[1][0],\\n    text_segments_to_check[1][1],\\n    text_segments_to_check[1][2],\\n]\\nsame_segment_3 = [\\n    text_segments_to_check[2][0],\\n    text_segments_to_check[2][1],\\n    text_segments_to_check[2][2],\\n]\\nsame_segment_4 = [\\n    text_segments_to_check[3][0],\\n    text_segments_to_check[3][1],\\n    text_segments_to_check[3][2],\\n]\\nsame_segment_5 = [\\n    text_segments_to_check[4][0],\\n    text_segments_to_check[4][1],\\n    text_segments_to_check[4][2],\\n]\";\n",
       "                var nbb_formatted_code = \"different_segment_1 = [\\n    text_segments_to_check[0][-3],\\n    text_segments_to_check[0][-2],\\n    text_segments_to_check[0][-1],\\n    text_segments_to_check[1][0],\\n    text_segments_to_check[1][1],\\n    text_segments_to_check[1][2],\\n]\\ndifferent_segment_2 = [\\n    text_segments_to_check[1][-3],\\n    text_segments_to_check[1][-2],\\n    text_segments_to_check[1][-1],\\n    text_segments_to_check[2][0],\\n    text_segments_to_check[2][1],\\n    text_segments_to_check[2][2],\\n]\\ndifferent_segment_3 = [\\n    text_segments_to_check[2][-3],\\n    text_segments_to_check[2][-2],\\n    text_segments_to_check[2][-1],\\n    text_segments_to_check[3][0],\\n    text_segments_to_check[3][1],\\n    text_segments_to_check[3][2],\\n]\\ndifferent_segment_4 = [\\n    text_segments_to_check[3][-3],\\n    text_segments_to_check[3][-2],\\n    text_segments_to_check[3][-1],\\n    text_segments_to_check[4][0],\\n    text_segments_to_check[4][1],\\n    text_segments_to_check[4][2],\\n]\\ndifferent_segment_5 = [\\n    text_segments_to_check[4][-3],\\n    text_segments_to_check[4][-2],\\n    text_segments_to_check[4][-1],\\n    text_segments_to_check[5][0],\\n    text_segments_to_check[5][1],\\n    text_segments_to_check[5][2],\\n]\\n\\nsame_segment_1 = [\\n    text_segments_to_check[0][0],\\n    text_segments_to_check[0][1],\\n    text_segments_to_check[0][2],\\n]\\nsame_segment_2 = [\\n    text_segments_to_check[1][0],\\n    text_segments_to_check[1][1],\\n    text_segments_to_check[1][2],\\n]\\nsame_segment_3 = [\\n    text_segments_to_check[2][0],\\n    text_segments_to_check[2][1],\\n    text_segments_to_check[2][2],\\n]\\nsame_segment_4 = [\\n    text_segments_to_check[3][0],\\n    text_segments_to_check[3][1],\\n    text_segments_to_check[3][2],\\n]\\nsame_segment_5 = [\\n    text_segments_to_check[4][0],\\n    text_segments_to_check[4][1],\\n    text_segments_to_check[4][2],\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "different_segment_1 = [\n",
    "    text_segments_to_check[0][-3],\n",
    "    text_segments_to_check[0][-2],\n",
    "    text_segments_to_check[0][-1],\n",
    "    text_segments_to_check[1][0],\n",
    "    text_segments_to_check[1][1],\n",
    "    text_segments_to_check[1][2],\n",
    "]\n",
    "different_segment_2 = [\n",
    "    text_segments_to_check[1][-3],\n",
    "    text_segments_to_check[1][-2],\n",
    "    text_segments_to_check[1][-1],\n",
    "    text_segments_to_check[2][0],\n",
    "    text_segments_to_check[2][1],\n",
    "    text_segments_to_check[2][2],\n",
    "]\n",
    "different_segment_3 = [\n",
    "    text_segments_to_check[2][-3],\n",
    "    text_segments_to_check[2][-2],\n",
    "    text_segments_to_check[2][-1],\n",
    "    text_segments_to_check[3][0],\n",
    "    text_segments_to_check[3][1],\n",
    "    text_segments_to_check[3][2],\n",
    "]\n",
    "different_segment_4 = [\n",
    "    text_segments_to_check[3][-3],\n",
    "    text_segments_to_check[3][-2],\n",
    "    text_segments_to_check[3][-1],\n",
    "    text_segments_to_check[4][0],\n",
    "    text_segments_to_check[4][1],\n",
    "    text_segments_to_check[4][2],\n",
    "]\n",
    "different_segment_5 = [\n",
    "    text_segments_to_check[4][-3],\n",
    "    text_segments_to_check[4][-2],\n",
    "    text_segments_to_check[4][-1],\n",
    "    text_segments_to_check[5][0],\n",
    "    text_segments_to_check[5][1],\n",
    "    text_segments_to_check[5][2],\n",
    "]\n",
    "\n",
    "same_segment_1 = [\n",
    "    text_segments_to_check[0][0],\n",
    "    text_segments_to_check[0][1],\n",
    "    text_segments_to_check[0][2],\n",
    "]\n",
    "same_segment_2 = [\n",
    "    text_segments_to_check[1][0],\n",
    "    text_segments_to_check[1][1],\n",
    "    text_segments_to_check[1][2],\n",
    "]\n",
    "same_segment_3 = [\n",
    "    text_segments_to_check[2][0],\n",
    "    text_segments_to_check[2][1],\n",
    "    text_segments_to_check[2][2],\n",
    "]\n",
    "same_segment_4 = [\n",
    "    text_segments_to_check[3][0],\n",
    "    text_segments_to_check[3][1],\n",
    "    text_segments_to_check[3][2],\n",
    "]\n",
    "same_segment_5 = [\n",
    "    text_segments_to_check[4][0],\n",
    "    text_segments_to_check[4][1],\n",
    "    text_segments_to_check[4][2],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed7cac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Map: ['universidad', 'michelin', 'universities', 'michelin']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['universidad', 'michelin', 'universities', 'michelin', 'uci', 'universidad', 'sociedad', 'universidad']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['universidad', 'michelin', 'universities', 'michelin', 'uci', 'universidad', 'sociedad', 'universidad', 'sioux', 'uci', 'railroad', 'uci']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 1, Prediction: 1\n",
      "Coherence Map: ['census', 'sioux', 'area', 'sioux']\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['households', 'census', 'census', 'census']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['households', 'households', 'median', 'households']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['households', 'households', 'median', 'households', 'elementary', 'households', 'elementary', 'median']\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['cathedral', 'elementary', 'buildings', 'elementary']\n",
      "Label: 1, Prediction: 0\n",
      "Coherence Map: ['cathedral', 'elementary', 'buildings', 'elementary', '1943', 'cossacks', '1942', 'cossacks']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['cathedral', 'elementary', 'buildings', 'elementary', '1943', 'cossacks', '1942', 'cossacks', 'separatists', '1943', 'ukrainian', '1943']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['lennon', 'separatists', 'square', 'separatists']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['lennon', 'separatists', 'square', 'separatists', 'climate', 'lennon', 'classification', 'lennon']\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['census', 'climate', 'area', 'climate']\n",
      "Label: 1, Prediction: 1\n",
      "Coherence Map: ['households', 'census', 'census', 'census']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['households', 'census', 'census', 'census', 'households', 'households', 'median', 'households']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['households', 'census', 'census', 'census']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['households', 'census', 'census', 'census', 'households', 'households', 'median', 'households']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['households', 'census', 'census', 'census', 'households', 'households', 'median', 'households', 'complexes', 'households', 'gora', 'median']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 1, Prediction: 1\n",
      "Coherence Map: ['springs', 'winery', 'sources', 'winery']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['springs', 'winery', 'sources', 'winery', 'boris', 'springs', 'iii', 'springs']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['springs', 'winery', 'sources', 'winery']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['springs', 'winery', 'sources', 'winery', 'boris', 'springs', 'iii', 'springs']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['springs', 'winery', 'sources', 'winery', 'boris', 'springs', 'iii', 'springs', 'massachusetts', 'boris', 'railroad', 'boris']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 1, Prediction: 1\n",
      "Coherence Map: ['harvard', 'harvard', 'harvard', 'founded']\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['households', 'census', 'census', 'census']\n",
      "Label: 0, Prediction: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(tensor(0.3717), 0),\n",
       " (tensor(0.3243), 0),\n",
       " (tensor(0.2591), 1),\n",
       " (tensor(0.2496), 1),\n",
       " (tensor(0.3721), 0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 47;\n",
       "                var nbb_unformatted_code = \"num_sentences_to_check = 6  # test only the first n in the segment\\ntarget_labels = [0, 0, 0, 1, 0, 0]\\n\\n# test coherence on different segments\\ncoherence_tester(\\n    different_segment_1[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_2[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_3[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_4[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_5[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\";\n",
       "                var nbb_formatted_code = \"num_sentences_to_check = 6  # test only the first n in the segment\\ntarget_labels = [0, 0, 0, 1, 0, 0]\\n\\n# test coherence on different segments\\ncoherence_tester(\\n    different_segment_1[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_2[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_3[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_4[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_5[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_sentences_to_check = 6  # test only the first n in the segment\n",
    "target_labels = [0, 0, 0, 1, 0, 0]\n",
    "\n",
    "# test coherence on different segments\n",
    "coherence_tester(\n",
    "    different_segment_1[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")\n",
    "coherence_tester(\n",
    "    different_segment_2[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")\n",
    "coherence_tester(\n",
    "    different_segment_3[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")\n",
    "coherence_tester(\n",
    "    different_segment_4[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")\n",
    "coherence_tester(\n",
    "    different_segment_5[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f34ae8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence: ['shoreline', 'basque', 'seashore', 'basque']\n",
      "0, tensor([0.2451]), In spite of appearances both t.. <> The city is in the north of th..\n",
      "0, tensor([0.3396]), In spite of appearances both t.. <> The city is in the north of th..\n",
      "Coherence: ['precipitation', 'shoreline', 'climate', 'shoreline']\n",
      "0, tensor([0.2880]), The city is in the north of th.. <> San Sebastián features an ocea..\n",
      "0, tensor([0.3303]), The city is in the north of th.. <> San Sebastián features an ocea..\n",
      "Coherence: ['census', 'sioux', 'area', 'sioux']\n",
      "0, tensor([0.2409]), Hospers was founded in 1872 wh.. <> Hospers is located at 43 07103..\n",
      "0, tensor([0.2987]), Hospers was founded in 1872 wh.. <> Hospers is located at 43 07103..\n",
      "Coherence: ['households', 'census', 'census', 'census']\n",
      "0, tensor([0.2941]), Hospers is located at 43 07103.. <> As of the census of 2010 there..\n",
      "0, tensor([0.3422]), Hospers is located at 43 07103.. <> As of the census of 2010 there..\n",
      "Coherence: ['1943', 'cossacks', '1942', 'cossacks']\n",
      "0, tensor([0.3146]), First mentioned in 1571 in con.. <> During World War II the Red Ar..\n",
      "0, tensor([0.3753]), First mentioned in 1571 in con.. <> During World War II the Red Ar..\n",
      "Coherence: ['separatists', '1943', 'ukrainian', '1943']\n",
      "0, tensor([0.3053]), During World War II the Red Ar.. <> The town was the site of spora..\n",
      "0, tensor([0.3648]), During World War II the Red Ar.. <> The town was the site of spora..\n",
      "Coherence: ['households', 'census', 'census', 'census']\n",
      "0, tensor([0.2690]), Wallingford is located at 43 3.. <> As of the census of 2010 there..\n",
      "0, tensor([0.3508]), Wallingford is located at 43 3.. <> As of the census of 2010 there..\n",
      "Coherence: ['households', 'households', 'median', 'households']\n",
      "0, tensor([0.5013]), As of the census of 2010 there.. <> As of the census of 2000 there..\n",
      "0, tensor([0.4821]), As of the census of 2010 there.. <> As of the census of 2000 there..\n",
      "Coherence: ['springs', 'winery', 'sources', 'winery']\n",
      "0, tensor([0.3112]), The health resort village of B.. <> The remains of an ancient vill..\n",
      "0, tensor([0.3717]), The health resort village of B.. <> The remains of an ancient vill..\n",
      "Coherence: ['boris', 'springs', 'iii', 'springs']\n",
      "0, tensor([0.3035]), The remains of an ancient vill.. <> The Banya Palace summerhouse o..\n",
      "0, tensor([0.3243]), The remains of an ancient vill.. <> The Banya Palace summerhouse o..\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# test coherence on same segments\\ncoherence_tester(same_segment_1, [0,0,0])\\ncoherence_tester(same_segment_2, [0,0,0])\\ncoherence_tester(same_segment_3, [0,0,0])\\ncoherence_tester(same_segment_4, [0,0,0])\\ncoherence_tester(same_segment_5, [0,0,0])\";\n",
       "                var nbb_formatted_code = \"# test coherence on same segments\\ncoherence_tester(same_segment_1, [0, 0, 0])\\ncoherence_tester(same_segment_2, [0, 0, 0])\\ncoherence_tester(same_segment_3, [0, 0, 0])\\ncoherence_tester(same_segment_4, [0, 0, 0])\\ncoherence_tester(same_segment_5, [0, 0, 0])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test coherence on same segments\n",
    "coherence_tester(same_segment_1, [0, 0, 0])\n",
    "coherence_tester(same_segment_2, [0, 0, 0])\n",
    "coherence_tester(same_segment_3, [0, 0, 0])\n",
    "coherence_tester(same_segment_4, [0, 0, 0])\n",
    "coherence_tester(same_segment_5, [0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "810d05f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Map: ['shoreline', 'basque', 'seashore', 'basque']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['shoreline', 'basque', 'seashore', 'basque', 'precipitation', 'shoreline', 'climate', 'shoreline']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['shoreline', 'basque', 'seashore', 'basque', 'precipitation', 'shoreline', 'climate', 'shoreline', 'paleolithic', 'precipitation', 'evidence', 'precipitation']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['basque', 'paleolithic', 'roman', 'paleolithic']\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['castile', 'san', 'colonizers', 'san']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['castile', 'san', 'colonizers', 'san', 'navarre', 'castile', 'iberian', 'castile']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['castile', 'san', 'colonizers', 'san', 'navarre', 'castile', 'iberian', 'castile', 'neoclassical', 'navarre', 'bourgeois', 'navarre']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['railway', 'neoclassical', 'bridge', 'neoclassical']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['railway', 'neoclassical', 'bridge', 'neoclassical', 'residents', 'railway', 'walls', 'railway']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['railway', 'neoclassical', 'bridge', 'neoclassical', 'residents', 'railway', 'walls', 'railway', 'industry', 'residents', 'nucleus', 'residents']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['railway', 'railway', 'railway', 'neoclassical', 'neoclassical', 'residents', 'residents', 'residents', 'terminal', 'industry', 'centre', 'industry']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['railway', 'railway', 'railway', 'neoclassical', 'neoclassical', 'residents', 'residents', 'residents', 'district', 'terminal', 'avenida', 'terminal']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'avenida', 'railway', 'railway', 'railway', 'neoclassical', 'neoclassical', 'residents', 'district', 'district', 'beach', 'district']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'avenida', 'railway', 'railway', 'railway', 'neoclassical', 'postwar', 'district', 'dictator', 'district']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'avenida', 'railway', 'railway', 'railway', 'railway', 'postwar', 'former', 'postwar']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'railway', 'railway', 'civil', 'railway']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'inhabitants', 'railway', 'immigrants', 'railway']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'nanotechnology', 'inhabitants', 'campus', 'inhabitants']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'downtown', 'nanotechnology', 'axis', 'nanotechnology']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'bridge', 'district', 'pedestrian', 'district']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['bridge', 'district', 'district', 'district', 'district', 'district', 'postwar', 'postwar', 'municipality', 'bridge', 'training', 'bridge']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['bridge', 'bridge', 'bridge', 'district', 'district', 'district', 'district', 'district', 'nursery', 'municipality', 'nurseries', 'municipality']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['bridge', 'bridge', 'bridge', 'district', 'district', 'district', 'district', 'district', 'village', 'nursery', 'reconstruction', 'nursery']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['bridge', 'bridge', 'bridge', 'district', 'district', 'district', 'district', 'district', 'festivals', 'village', 'festival', 'village']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festival', 'bridge', 'bridge', 'bridge', 'district', 'district', 'district', 'celebrations', 'festivals', 'parades', 'festivals']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'fireworks', 'celebrations', 'festival', 'celebrations']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'basque', 'fireworks', 'festival', 'fireworks']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'basque', 'basque', 'traditional', 'basque']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'festival', 'basque', 'gypsy', 'basque']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'festival', 'festival', 'stalls', 'festival']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'festival', 'bridge', 'bridge', 'bridge', 'basque', 'festival', 'villages', 'festival']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'festival', 'festival', 'festival', 'bridge', 'san', 'basque', 'tourism', 'basque']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'festival', 'festival', 'festival', 'bridge', 'michelin', 'san', 'gastronomy', 'san']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['universidad', 'michelin', 'universities', 'michelin']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['universidad', 'michelin', 'universities', 'michelin', 'uci', 'universidad', 'sociedad', 'universidad']\n",
      "Label: 0, Prediction: 0\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 48;\n",
       "                var nbb_unformatted_code = \"# test on a full segment\\npredictions = coherence_tester(text_segments_to_check[0], text_labels_to_check[0])\";\n",
       "                var nbb_formatted_code = \"# test on a full segment\\npredictions = coherence_tester(text_segments_to_check[0], text_labels_to_check[0])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test on a full segment\n",
    "predictions = coherence_tester(text_segments_to_check[0], text_labels_to_check[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a3613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5121f953",
   "metadata": {},
   "source": [
    "# pruning test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e541d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 44;\n",
       "                var nbb_unformatted_code = \"data = [(\\\"word1\\\", 0.3), (\\\"word2\\\", 0.1), (\\\"word3\\\", 0.7)]\\n\\nsorted_arr = sorted(data, key=lambda tup: tup[1])\";\n",
       "                var nbb_formatted_code = \"data = [(\\\"word1\\\", 0.3), (\\\"word2\\\", 0.1), (\\\"word3\\\", 0.7)]\\n\\nsorted_arr = sorted(data, key=lambda tup: tup[1])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"word1\", 0.3), (\"word2\", 0.1), (\"word3\", 0.7)]\n",
    "\n",
    "sorted_arr = sorted(data, key=lambda tup: tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0188f9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('word2', 0.1), ('word1', 0.3), ('word3', 0.7)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 45;\n",
       "                var nbb_unformatted_code = \"sorted_arr\";\n",
       "                var nbb_formatted_code = \"sorted_arr\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00458200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
