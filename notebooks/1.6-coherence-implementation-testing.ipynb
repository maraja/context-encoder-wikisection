{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc7697a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run if working locally\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ee84b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom utils.metrics import windowdiff, pk\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_formatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom utils.metrics import windowdiff, pk\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pickle\n",
    "import os, sys\n",
    "import config\n",
    "\n",
    "config.root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, config.root_path)\n",
    "\n",
    "from src.dataset.dataset import RawData\n",
    "from src.dataset.wikisection_preprocessing import (\n",
    "    tokenize,\n",
    "    clean_sentence,\n",
    "    preprocess_text_segmentation,\n",
    "    format_data_for_db_insertion,\n",
    ")\n",
    "from src.dataset.utils import truncate_by_token\n",
    "from db.dbv2 import Table, AugmentedTable, TrainTestTable\n",
    "import pprint\n",
    "\n",
    "from utils.metrics import windowdiff, pk\n",
    "\n",
    "from src.bertkeywords.src.similarities import Embedding, Similarities\n",
    "from src.bertkeywords.src.keywords import Keywords\n",
    "from src.encoders.coherence import Coherence\n",
    "from src.dataset.utils import flatten, dedupe_list, truncate_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb2458b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_formatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_type = \"city\"\n",
    "table = Table(dataset_type)\n",
    "augmented_table = AugmentedTable(dataset_type)\n",
    "train_test_table = TrainTestTable(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd59c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_formatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = table.get_all()\n",
    "\n",
    "text_data = [x[1] for x in data]\n",
    "text_labels = [x[2] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcccad5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_formatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_segments = table.get_all_segments()\n",
    "text_segments = [[y[1] for y in x] for x in all_segments]\n",
    "segments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "295657fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_formatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = 5\n",
    "max_tokens = 400\n",
    "\n",
    "for i, (segment, labels) in enumerate(\n",
    "    zip(text_segments[:samples], segments_labels[:samples])\n",
    "):\n",
    "    for sentence, label in zip(segment, labels):\n",
    "        # this is the training case. During inference, we will have no idea\n",
    "        # when segments start and when they end.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "467789d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"text_labels[:25]\";\n",
       "                var nbb_formatted_code = \"text_labels[:25]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_labels[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c7beb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-04-16 01:27:22.643016: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-04-16 01:27:23.686457: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 45;\n",
       "                var nbb_unformatted_code = \"# initialize the coherence library\\nmax_words_per_step = 4\\ncoherence = Coherence(max_words_per_step=max_words_per_step)\";\n",
       "                var nbb_formatted_code = \"# initialize the coherence library\\nmax_words_per_step = 4\\ncoherence = Coherence(max_words_per_step=max_words_per_step)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the coherence library\n",
    "max_words_per_step = 4\n",
    "coherence = Coherence(max_words_per_step=max_words_per_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "f6870992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 417;\n",
       "                var nbb_unformatted_code = \"def get_weighted_average(weighted_similarities, weights):\\n    return sum(weighted_similarities) / sum(weights)\\n\\n\\n# importance testing\\ndef compare_coherent_words(\\n    coherence_map,\\n    keywords_current,\\n    suppress_errors=False,\\n    same_word_multiplier=3,  # if set to 1, don't amplify the same words found\\n):\\n    word_comparisons = []\\n    weights = []\\n    for i, keywords in enumerate(coherence_map[::-1]):\\n        for word_tuple in keywords:\\n            word = word_tuple[0]\\n            for second_word_tuple in keywords_current:\\n                second_word = second_word_tuple[0]\\n                second_word_importance = second_word_tuple[1]\\n\\n                try:\\n                    word_one_emb = word_tuple[2]\\n                    word_two_emb = second_word_tuple[2]\\n\\n                    if same_word_multiplier > 1:\\n                        flattened_coherence_words_only = [\\n                            element[0]\\n                            for sublist in coherence_map\\n                            for element in sublist\\n                        ]\\n\\n                        num_occurrences = flattened_coherence_words_only.count(\\n                            second_word\\n                        )\\n\\n                        if num_occurrences > 0:\\n                            # amplify words that are found as duplicates in the coherence map\\n                            # if the word shows up 1 time, amplify the weight by 2 times\\n                            weighting_multiplier = flattened_coherence_words_only.count(\\n                                second_word\\n                            ) + (same_word_multiplier - 1)\\n                        else:\\n                            #                             weighting_multiplier = 1\\n                            weighting_multiplier = (\\n                                1 / same_word_multiplier\\n                            )  # reduce the importance of this word\\n\\n                    else:\\n                        weighting_multiplier = 1  # set to 1 in case this is turned off.\\n\\n                    # this weight is a recipricol function that will grow smaller the further the keywords are away\\n                    # we want to put more importance on the current words, so we apply twice as much weight.\\n                    if i == 0:\\n                        weight = (weighting_multiplier * 2) / (i + 1)\\n                    else:\\n                        weight = (weighting_multiplier * 1) / (i + 1)\\n\\n                    # multiply the weighting factor by the importance of the second word\\n                    weight *= second_word_importance\\n\\n                    word_comparisons.append(\\n                        (\\n                            word,\\n                            second_word,\\n                            weight\\n                            * coherence.embedding_lib.get_similarity(\\n                                word_one_emb, word_two_emb\\n                            ),\\n                        )\\n                    )\\n                    weights.append(weight)\\n                except AssertionError as e:\\n                    if not suppress_errors:\\n                        print(e, word, second_word)\\n\\n    return word_comparisons, weights\\n\\n\\n# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\\ndef coherence_tester(\\n    text_data,\\n    text_labels,\\n    max_tokens=256,\\n    max_str_length=30,\\n    prediction_thresh=0.25,\\n    pruning=0,  # remove one sentence worth of keywords\\n    pruning_min=6,  # remove the first sentence in the coherence map once it grows passed 6\\n    dynamic_threshold=False,\\n    threshold_warmup=10,  # number of iterations before using dynamic threshold\\n    last_n_threshold=5,  # will only consider the last n thresholds for dynamic threshold\\n):\\n    coherence_map = []\\n    predictions = []\\n    thresholds = []\\n    for i, (row, label) in enumerate(zip(text_data, text_labels)):\\n        threshold = prediction_thresh\\n        if dynamic_threshold and (i + 1) > threshold_warmup:\\n            last_n_thresholds = thresholds[(0 - last_n_threshold) :]\\n            last_n_thresholds.sort()\\n            mid = len(last_n_thresholds) // 2\\n            threshold = (last_n_thresholds[mid] + last_n_thresholds[~mid]) / 2\\n            print(f\\\"median threshold: {threshold}\\\")\\n        # compare the current sentence to the previous one\\n        if i == 0:\\n            predictions.append((0, 0))\\n        else:\\n            prev_row = text_data[i - 1]\\n\\n            row = truncate_by_token(row, max_tokens)\\n            prev_row = truncate_by_token(prev_row, max_tokens)\\n\\n            cohesion, keywords_prev, keywords_current = coherence.get_coherence(\\n                [row, prev_row], coherence_threshold=0.3\\n            )\\n\\n            # add the keywords to the coherence map\\n            coherence_map.append(cohesion)\\n            if pruning > 0 and len(coherence_map) >= pruning_min:\\n                print(\\\"pruning...\\\", len(coherence_map))\\n                coherence_map = coherence_map[\\n                    pruning:\\n                ]  # remove the pruning amount from the beginning of the list\\n                print(\\\"done pruning...\\\", len(coherence_map))\\n\\n            # truncate the strings for printing\\n            truncated_row = truncate_string(row, max_str_length)\\n            truncated_prev_row = truncate_string(prev_row, max_str_length)\\n            print(\\n                f\\\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\\\"\\n            )\\n\\n            # compute the word comparisons between the previous (with the coherence map)\\n            # and the current (possibly the first sentence in a new segment)\\n            word_comparisons_with_coherence, weights = compare_coherent_words(\\n                [*coherence_map, keywords_prev], keywords_current\\n            )\\n\\n            similarities_with_coherence = [\\n                comparison[2] for comparison in word_comparisons_with_coherence\\n            ]\\n            avg_similarity_with_coherence = sum(similarities_with_coherence) / (\\n                len(similarities_with_coherence) or 1\\n            )\\n            weighted_avg_similarity_with_coherence = get_weighted_average(\\n                similarities_with_coherence, weights\\n            )\\n            print(f\\\"weighted: {weighted_avg_similarity_with_coherence}\\\")\\n\\n            # if the two sentences are similar, create a cohesive prediction\\n            # otherwise, predict a new segment\\n            if weighted_avg_similarity_with_coherence > threshold:\\n                print(\\n                    f\\\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 0))\\n            else:\\n                # start of a new segment, empty the map\\n                coherence_map = []\\n                print(\\n                    f\\\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 1))\\n\\n            thresholds.append(weighted_avg_similarity_with_coherence)\\n            print(\\\"===============================================\\\")\\n\\n    return predictions\";\n",
       "                var nbb_formatted_code = \"def get_weighted_average(weighted_similarities, weights):\\n    return sum(weighted_similarities) / sum(weights)\\n\\n\\n# importance testing\\ndef compare_coherent_words(\\n    coherence_map,\\n    keywords_current,\\n    suppress_errors=False,\\n    same_word_multiplier=3,  # if set to 1, don't amplify the same words found\\n):\\n    word_comparisons = []\\n    weights = []\\n    for i, keywords in enumerate(coherence_map[::-1]):\\n        for word_tuple in keywords:\\n            word = word_tuple[0]\\n            for second_word_tuple in keywords_current:\\n                second_word = second_word_tuple[0]\\n                second_word_importance = second_word_tuple[1]\\n\\n                try:\\n                    word_one_emb = word_tuple[2]\\n                    word_two_emb = second_word_tuple[2]\\n\\n                    if same_word_multiplier > 1:\\n                        flattened_coherence_words_only = [\\n                            element[0]\\n                            for sublist in coherence_map\\n                            for element in sublist\\n                        ]\\n\\n                        num_occurrences = flattened_coherence_words_only.count(\\n                            second_word\\n                        )\\n\\n                        if num_occurrences > 0:\\n                            # amplify words that are found as duplicates in the coherence map\\n                            # if the word shows up 1 time, amplify the weight by 2 times\\n                            weighting_multiplier = flattened_coherence_words_only.count(\\n                                second_word\\n                            ) + (same_word_multiplier - 1)\\n                        else:\\n                            #                             weighting_multiplier = 1\\n                            weighting_multiplier = (\\n                                1 / same_word_multiplier\\n                            )  # reduce the importance of this word\\n\\n                    else:\\n                        weighting_multiplier = 1  # set to 1 in case this is turned off.\\n\\n                    # this weight is a recipricol function that will grow smaller the further the keywords are away\\n                    # we want to put more importance on the current words, so we apply twice as much weight.\\n                    if i == 0:\\n                        weight = (weighting_multiplier * 2) / (i + 1)\\n                    else:\\n                        weight = (weighting_multiplier * 1) / (i + 1)\\n\\n                    # multiply the weighting factor by the importance of the second word\\n                    weight *= second_word_importance\\n\\n                    word_comparisons.append(\\n                        (\\n                            word,\\n                            second_word,\\n                            weight\\n                            * coherence.embedding_lib.get_similarity(\\n                                word_one_emb, word_two_emb\\n                            ),\\n                        )\\n                    )\\n                    weights.append(weight)\\n                except AssertionError as e:\\n                    if not suppress_errors:\\n                        print(e, word, second_word)\\n\\n    return word_comparisons, weights\\n\\n\\n# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\\ndef coherence_tester(\\n    text_data,\\n    text_labels,\\n    max_tokens=256,\\n    max_str_length=30,\\n    prediction_thresh=0.25,\\n    pruning=0,  # remove one sentence worth of keywords\\n    pruning_min=6,  # remove the first sentence in the coherence map once it grows passed 6\\n    dynamic_threshold=False,\\n    threshold_warmup=10,  # number of iterations before using dynamic threshold\\n    last_n_threshold=5,  # will only consider the last n thresholds for dynamic threshold\\n):\\n    coherence_map = []\\n    predictions = []\\n    thresholds = []\\n    for i, (row, label) in enumerate(zip(text_data, text_labels)):\\n        threshold = prediction_thresh\\n        if dynamic_threshold and (i + 1) > threshold_warmup:\\n            last_n_thresholds = thresholds[(0 - last_n_threshold) :]\\n            last_n_thresholds.sort()\\n            mid = len(last_n_thresholds) // 2\\n            threshold = (last_n_thresholds[mid] + last_n_thresholds[~mid]) / 2\\n            print(f\\\"median threshold: {threshold}\\\")\\n        # compare the current sentence to the previous one\\n        if i == 0:\\n            predictions.append((0, 0))\\n        else:\\n            prev_row = text_data[i - 1]\\n\\n            row = truncate_by_token(row, max_tokens)\\n            prev_row = truncate_by_token(prev_row, max_tokens)\\n\\n            cohesion, keywords_prev, keywords_current = coherence.get_coherence(\\n                [row, prev_row], coherence_threshold=0.3\\n            )\\n\\n            # add the keywords to the coherence map\\n            coherence_map.append(cohesion)\\n            if pruning > 0 and len(coherence_map) >= pruning_min:\\n                print(\\\"pruning...\\\", len(coherence_map))\\n                coherence_map = coherence_map[\\n                    pruning:\\n                ]  # remove the pruning amount from the beginning of the list\\n                print(\\\"done pruning...\\\", len(coherence_map))\\n\\n            # truncate the strings for printing\\n            truncated_row = truncate_string(row, max_str_length)\\n            truncated_prev_row = truncate_string(prev_row, max_str_length)\\n            print(\\n                f\\\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\\\"\\n            )\\n\\n            # compute the word comparisons between the previous (with the coherence map)\\n            # and the current (possibly the first sentence in a new segment)\\n            word_comparisons_with_coherence, weights = compare_coherent_words(\\n                [*coherence_map, keywords_prev], keywords_current\\n            )\\n\\n            similarities_with_coherence = [\\n                comparison[2] for comparison in word_comparisons_with_coherence\\n            ]\\n            avg_similarity_with_coherence = sum(similarities_with_coherence) / (\\n                len(similarities_with_coherence) or 1\\n            )\\n            weighted_avg_similarity_with_coherence = get_weighted_average(\\n                similarities_with_coherence, weights\\n            )\\n            print(f\\\"weighted: {weighted_avg_similarity_with_coherence}\\\")\\n\\n            # if the two sentences are similar, create a cohesive prediction\\n            # otherwise, predict a new segment\\n            if weighted_avg_similarity_with_coherence > threshold:\\n                print(\\n                    f\\\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 0))\\n            else:\\n                # start of a new segment, empty the map\\n                coherence_map = []\\n                print(\\n                    f\\\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 1))\\n\\n            thresholds.append(weighted_avg_similarity_with_coherence)\\n            print(\\\"===============================================\\\")\\n\\n    return predictions\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_weighted_average(weighted_similarities, weights):\n",
    "    return sum(weighted_similarities) / sum(weights)\n",
    "\n",
    "\n",
    "# importance testing\n",
    "def compare_coherent_words(\n",
    "    coherence_map,\n",
    "    keywords_current,\n",
    "    suppress_errors=False,\n",
    "    same_word_multiplier=3,  # if set to 1, don't amplify the same words found\n",
    "):\n",
    "    word_comparisons = []\n",
    "    weights = []\n",
    "    for i, keywords in enumerate(coherence_map[::-1]):\n",
    "        for word_tuple in keywords:\n",
    "            word = word_tuple[0]\n",
    "            for second_word_tuple in keywords_current:\n",
    "                second_word = second_word_tuple[0]\n",
    "                second_word_importance = second_word_tuple[1]\n",
    "\n",
    "                try:\n",
    "                    word_one_emb = word_tuple[2]\n",
    "                    word_two_emb = second_word_tuple[2]\n",
    "\n",
    "                    if same_word_multiplier > 1:\n",
    "                        flattened_coherence_words_only = [\n",
    "                            element[0]\n",
    "                            for sublist in coherence_map\n",
    "                            for element in sublist\n",
    "                        ]\n",
    "\n",
    "                        num_occurrences = flattened_coherence_words_only.count(\n",
    "                            second_word\n",
    "                        )\n",
    "\n",
    "                        if num_occurrences > 0:\n",
    "                            # amplify words that are found as duplicates in the coherence map\n",
    "                            # if the word shows up 1 time, amplify the weight by 2 times\n",
    "                            weighting_multiplier = flattened_coherence_words_only.count(\n",
    "                                second_word\n",
    "                            ) + (same_word_multiplier - 1)\n",
    "                        else:\n",
    "                            #                             weighting_multiplier = 1\n",
    "                            weighting_multiplier = (\n",
    "                                1 / same_word_multiplier\n",
    "                            )  # reduce the importance of this word\n",
    "\n",
    "                    else:\n",
    "                        weighting_multiplier = 1  # set to 1 in case this is turned off.\n",
    "\n",
    "                    # this weight is a recipricol function that will grow smaller the further the keywords are away\n",
    "                    # we want to put more importance on the current words, so we apply twice as much weight.\n",
    "                    if i == 0:\n",
    "                        weight = (weighting_multiplier * 2) / (i + 1)\n",
    "                    else:\n",
    "                        weight = (weighting_multiplier * 1) / (i + 1)\n",
    "\n",
    "                    # multiply the weighting factor by the importance of the second word\n",
    "                    weight *= second_word_importance\n",
    "\n",
    "                    word_comparisons.append(\n",
    "                        (\n",
    "                            word,\n",
    "                            second_word,\n",
    "                            weight\n",
    "                            * coherence.embedding_lib.get_similarity(\n",
    "                                word_one_emb, word_two_emb\n",
    "                            ),\n",
    "                        )\n",
    "                    )\n",
    "                    weights.append(weight)\n",
    "                except AssertionError as e:\n",
    "                    if not suppress_errors:\n",
    "                        print(e, word, second_word)\n",
    "\n",
    "    return word_comparisons, weights\n",
    "\n",
    "\n",
    "# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\n",
    "def coherence_tester(\n",
    "    text_data,\n",
    "    text_labels,\n",
    "    max_tokens=256,\n",
    "    max_str_length=30,\n",
    "    prediction_thresh=0.25,\n",
    "    pruning=0,  # remove one sentence worth of keywords\n",
    "    pruning_min=6,  # remove the first sentence in the coherence map once it grows passed 6\n",
    "    dynamic_threshold=False,\n",
    "    threshold_warmup=10,  # number of iterations before using dynamic threshold\n",
    "    last_n_threshold=5,  # will only consider the last n thresholds for dynamic threshold\n",
    "):\n",
    "    coherence_map = []\n",
    "    predictions = []\n",
    "    thresholds = []\n",
    "    for i, (row, label) in enumerate(zip(text_data, text_labels)):\n",
    "        threshold = prediction_thresh\n",
    "        if dynamic_threshold and (i + 1) > threshold_warmup:\n",
    "            last_n_thresholds = thresholds[(0 - last_n_threshold) :]\n",
    "            last_n_thresholds.sort()\n",
    "            mid = len(last_n_thresholds) // 2\n",
    "            threshold = (last_n_thresholds[mid] + last_n_thresholds[~mid]) / 2\n",
    "            print(f\"median threshold: {threshold}\")\n",
    "        # compare the current sentence to the previous one\n",
    "        if i == 0:\n",
    "            predictions.append((0, 0))\n",
    "        else:\n",
    "            prev_row = text_data[i - 1]\n",
    "\n",
    "            row = truncate_by_token(row, max_tokens)\n",
    "            prev_row = truncate_by_token(prev_row, max_tokens)\n",
    "\n",
    "            cohesion, keywords_prev, keywords_current = coherence.get_coherence(\n",
    "                [row, prev_row], coherence_threshold=0.3\n",
    "            )\n",
    "\n",
    "            # add the keywords to the coherence map\n",
    "            coherence_map.append(cohesion)\n",
    "            if pruning > 0 and len(coherence_map) >= pruning_min:\n",
    "                print(\"pruning...\", len(coherence_map))\n",
    "                coherence_map = coherence_map[\n",
    "                    pruning:\n",
    "                ]  # remove the pruning amount from the beginning of the list\n",
    "                print(\"done pruning...\", len(coherence_map))\n",
    "\n",
    "            # truncate the strings for printing\n",
    "            truncated_row = truncate_string(row, max_str_length)\n",
    "            truncated_prev_row = truncate_string(prev_row, max_str_length)\n",
    "            print(\n",
    "                f\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\"\n",
    "            )\n",
    "\n",
    "            # compute the word comparisons between the previous (with the coherence map)\n",
    "            # and the current (possibly the first sentence in a new segment)\n",
    "            word_comparisons_with_coherence, weights = compare_coherent_words(\n",
    "                [*coherence_map, keywords_prev], keywords_current\n",
    "            )\n",
    "\n",
    "            similarities_with_coherence = [\n",
    "                comparison[2] for comparison in word_comparisons_with_coherence\n",
    "            ]\n",
    "            avg_similarity_with_coherence = sum(similarities_with_coherence) / (\n",
    "                len(similarities_with_coherence) or 1\n",
    "            )\n",
    "            weighted_avg_similarity_with_coherence = get_weighted_average(\n",
    "                similarities_with_coherence, weights\n",
    "            )\n",
    "            print(f\"weighted: {weighted_avg_similarity_with_coherence}\")\n",
    "\n",
    "            # if the two sentences are similar, create a cohesive prediction\n",
    "            # otherwise, predict a new segment\n",
    "            if weighted_avg_similarity_with_coherence > threshold:\n",
    "                print(\n",
    "                    f\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\"\n",
    "                )\n",
    "                predictions.append((weighted_avg_similarity_with_coherence, 0))\n",
    "            else:\n",
    "                # start of a new segment, empty the map\n",
    "                coherence_map = []\n",
    "                print(\n",
    "                    f\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\"\n",
    "                )\n",
    "                predictions.append((weighted_avg_similarity_with_coherence, 1))\n",
    "\n",
    "            thresholds.append(weighted_avg_similarity_with_coherence)\n",
    "            print(\"===============================================\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca71b4cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['donostia', 'sebastian', 'sebastián', 'sebastiane']\n",
      "['pasaia', 'urumea', 'biscay', 'adarra']\n",
      "Got the keywords in 0.2830 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['sebastián', 'biscay']], KW Curr: ['donostia', 'sebastian', 'sebastián', 'sebastiane']\n",
      "weighted: tensor([0.2756])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2756])\n",
      "===============================================\n",
      "['pasaia', 'urumea', 'biscay', 'adarra']\n",
      "['climate', 'temperatures', 'winters', 'sebastián']\n",
      "Got the keywords in 0.2695 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['sebastián', 'biscay'], ['pasaia', 'urumea', 'temperatures', 'winters']], KW Curr: ['pasaia', 'urumea', 'biscay', 'adarra']\n",
      "weighted: tensor([0.2337])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2337])\n",
      "===============================================\n",
      "['climate', 'temperatures', 'winters', 'sebastián']\n",
      "['paleolithic', 'ametzagaña', 'settlers', 'sapiens']\n",
      "Got the keywords in 0.2023 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['paleolithic', 'ametzagaña', 'climate', 'settlers']], KW Curr: ['climate', 'temperatures', 'winters', 'sebastián']\n",
      "weighted: tensor([0.3490])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3490])\n",
      "===============================================\n",
      "['paleolithic', 'ametzagaña', 'settlers', 'sapiens']\n",
      "['sebastián', 'san', 'basque', 'varduli']\n",
      "Got the keywords in 0.1647 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['paleolithic', 'ametzagaña', 'climate', 'settlers'], ['paleolithic', 'san', 'basque', 'sapiens']], KW Curr: ['paleolithic', 'ametzagaña', 'settlers', 'sapiens']\n",
      "weighted: tensor([0.2993])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2993])\n",
      "===============================================\n",
      "['sebastián', 'san', 'basque', 'varduli']\n",
      "['navarre', 'fuero', 'monastery', 'sebastián']\n",
      "Got the keywords in 0.2744 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['paleolithic', 'ametzagaña', 'climate', 'settlers'], ['paleolithic', 'san', 'basque', 'sapiens'], []], KW Curr: ['sebastián', 'san', 'basque', 'varduli']\n",
      "weighted: tensor([0.1844])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.1844])\n",
      "===============================================\n",
      "['navarre', 'fuero', 'monastery', 'sebastián']\n",
      "['donostia', 'navarre', 'spain', 'hondarribia']\n",
      "Got the keywords in 0.4172 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero']], KW Curr: ['navarre', 'fuero', 'monastery', 'sebastián']\n",
      "weighted: tensor([0.3494])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3494])\n",
      "===============================================\n",
      "['donostia', 'navarre', 'spain', 'hondarribia']\n",
      "['gipuzkoa', 'tolosa', 'construction', '1833']\n",
      "Got the keywords in 0.4483 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain']], KW Curr: ['donostia', 'navarre', 'spain', 'hondarribia']\n",
      "weighted: tensor([0.4572])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4572])\n",
      "===============================================\n",
      "['gipuzkoa', 'tolosa', 'construction', '1833']\n",
      "['cortazar', 'donostia', 'zurriola', 'town']\n",
      "Got the keywords in 0.4034 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain'], ['cortazar', 'gipuzkoa', 'donostia', 'tolosa']], KW Curr: ['gipuzkoa', 'tolosa', 'construction', '1833']\n",
      "weighted: tensor([0.5532])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5532])\n",
      "===============================================\n",
      "['cortazar', 'donostia', 'zurriola', 'town']\n",
      "['koxkeroak', 'vieja', 'joxemaritarrak', 'parte']\n",
      "Got the keywords in 0.3602 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain'], ['cortazar', 'gipuzkoa', 'donostia', 'tolosa'], ['cortazar', 'koxkeroak', 'vieja', 'joxemaritarrak']], KW Curr: ['cortazar', 'donostia', 'zurriola', 'town']\n",
      "weighted: tensor([0.6303])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.6303])\n",
      "===============================================\n",
      "['koxkeroak', 'vieja', 'joxemaritarrak', 'parte']\n",
      "['miramar', 'león', 'el', 'antiguo']\n",
      "Got the keywords in 0.2883 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain'], ['cortazar', 'gipuzkoa', 'donostia', 'tolosa'], ['cortazar', 'koxkeroak', 'vieja', 'joxemaritarrak'], ['koxkeroak', 'vieja', 'miramar', 'león']], KW Curr: ['koxkeroak', 'vieja', 'joxemaritarrak', 'parte']\n",
      "weighted: tensor([0.4422])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4422])\n",
      "===============================================\n",
      "['miramar', 'león', 'el', 'antiguo']\n",
      "['amara', 'euskotren', 'marshes', 'urumea']\n",
      "Got the keywords in 0.2064 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain'], ['cortazar', 'gipuzkoa', 'donostia', 'tolosa'], ['cortazar', 'koxkeroak', 'vieja', 'joxemaritarrak'], ['koxkeroak', 'vieja', 'miramar', 'león'], ['euskotren', 'miramar']], KW Curr: ['miramar', 'león', 'el', 'antiguo']\n",
      "weighted: tensor([0.3245])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3245])\n",
      "===============================================\n",
      "['amara', 'euskotren', 'marshes', 'urumea']\n",
      "['amara', 'district', 'city', 'madrid']\n",
      "Got the keywords in 0.2101 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain'], ['cortazar', 'gipuzkoa', 'donostia', 'tolosa'], ['cortazar', 'koxkeroak', 'vieja', 'joxemaritarrak'], ['koxkeroak', 'vieja', 'miramar', 'león'], ['euskotren', 'miramar'], ['amara', 'madrid']], KW Curr: ['amara', 'euskotren', 'marshes', 'urumea']\n",
      "weighted: tensor([0.2946])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2946])\n",
      "===============================================\n",
      "['amara', 'district', 'city', 'madrid']\n",
      "['district', 'area', 'gros', 'built']\n",
      "Got the keywords in 0.2542 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain'], ['cortazar', 'gipuzkoa', 'donostia', 'tolosa'], ['cortazar', 'koxkeroak', 'vieja', 'joxemaritarrak'], ['koxkeroak', 'vieja', 'miramar', 'león'], ['euskotren', 'miramar'], ['amara', 'madrid'], ['amara', 'district', 'city', 'built']], KW Curr: ['amara', 'district', 'city', 'madrid']\n",
      "weighted: tensor([0.2760])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2760])\n",
      "===============================================\n",
      "['district', 'area', 'gros', 'built']\n",
      "['palace', 'residence', 'aiete', 'franco']\n",
      "Got the keywords in 0.2177 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain'], ['cortazar', 'gipuzkoa', 'donostia', 'tolosa'], ['cortazar', 'koxkeroak', 'vieja', 'joxemaritarrak'], ['koxkeroak', 'vieja', 'miramar', 'león'], ['euskotren', 'miramar'], ['amara', 'madrid'], ['amara', 'district', 'city', 'built'], ['palace', 'district', 'residence', 'area']], KW Curr: ['district', 'area', 'gros', 'built']\n",
      "weighted: tensor([0.4008])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4008])\n",
      "===============================================\n",
      "['palace', 'residence', 'aiete', 'franco']\n",
      "['egia', 'donostia', 'urumea', 'anoeta']\n",
      "Got the keywords in 0.2753 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain'], ['cortazar', 'gipuzkoa', 'donostia', 'tolosa'], ['cortazar', 'koxkeroak', 'vieja', 'joxemaritarrak'], ['koxkeroak', 'vieja', 'miramar', 'león'], ['euskotren', 'miramar'], ['amara', 'madrid'], ['amara', 'district', 'city', 'built'], ['palace', 'district', 'residence', 'area'], ['egia', 'donostia', 'palace', 'anoeta']], KW Curr: ['palace', 'residence', 'aiete', 'franco']\n",
      "weighted: tensor([0.3921])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3921])\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['egia', 'donostia', 'urumea', 'anoeta']\n",
      "['intxaurrondo', 'walnut', 'basque', 'situated']\n",
      "Got the keywords in 0.2918 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain'], ['cortazar', 'gipuzkoa', 'donostia', 'tolosa'], ['cortazar', 'koxkeroak', 'vieja', 'joxemaritarrak'], ['koxkeroak', 'vieja', 'miramar', 'león'], ['euskotren', 'miramar'], ['amara', 'madrid'], ['amara', 'district', 'city', 'built'], ['palace', 'district', 'residence', 'area'], ['egia', 'donostia', 'palace', 'anoeta'], ['egia', 'intxaurrondo', 'donostia', 'walnut']], KW Curr: ['egia', 'donostia', 'urumea', 'anoeta']\n",
      "weighted: tensor([0.4195])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4195])\n",
      "===============================================\n",
      "['intxaurrondo', 'walnut', 'basque', 'situated']\n",
      "['altza', 'basque', 'sebastián', 'san']\n",
      "Got the keywords in 0.2652 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain'], ['cortazar', 'gipuzkoa', 'donostia', 'tolosa'], ['cortazar', 'koxkeroak', 'vieja', 'joxemaritarrak'], ['koxkeroak', 'vieja', 'miramar', 'león'], ['euskotren', 'miramar'], ['amara', 'madrid'], ['amara', 'district', 'city', 'built'], ['palace', 'district', 'residence', 'area'], ['egia', 'donostia', 'palace', 'anoeta'], ['egia', 'intxaurrondo', 'donostia', 'walnut'], ['intxaurrondo', 'altza', 'basque', 'sebastián']], KW Curr: ['intxaurrondo', 'walnut', 'basque', 'situated']\n",
      "weighted: tensor([0.4402])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4402])\n",
      "===============================================\n",
      "['altza', 'basque', 'sebastián', 'san']\n",
      "['ibaeta', 'cervezas', 'el', 'buildings']\n",
      "Got the keywords in 0.2451 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain'], ['cortazar', 'gipuzkoa', 'donostia', 'tolosa'], ['cortazar', 'koxkeroak', 'vieja', 'joxemaritarrak'], ['koxkeroak', 'vieja', 'miramar', 'león'], ['euskotren', 'miramar'], ['amara', 'madrid'], ['amara', 'district', 'city', 'built'], ['palace', 'district', 'residence', 'area'], ['egia', 'donostia', 'palace', 'anoeta'], ['egia', 'intxaurrondo', 'donostia', 'walnut'], ['intxaurrondo', 'altza', 'basque', 'sebastián'], ['ibaeta', 'basque', 'buildings', 'san']], KW Curr: ['altza', 'basque', 'sebastián', 'san']\n",
      "weighted: tensor([0.3509])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3509])\n",
      "===============================================\n",
      "['ibaeta', 'cervezas', 'el', 'buildings']\n",
      "['ciudad', 'hernani', 'astigarraga', 'district']\n",
      "Got the keywords in 0.2531 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain'], ['cortazar', 'gipuzkoa', 'donostia', 'tolosa'], ['cortazar', 'koxkeroak', 'vieja', 'joxemaritarrak'], ['koxkeroak', 'vieja', 'miramar', 'león'], ['euskotren', 'miramar'], ['amara', 'madrid'], ['amara', 'district', 'city', 'built'], ['palace', 'district', 'residence', 'area'], ['egia', 'donostia', 'palace', 'anoeta'], ['egia', 'intxaurrondo', 'donostia', 'walnut'], ['intxaurrondo', 'altza', 'basque', 'sebastián'], ['ibaeta', 'basque', 'buildings', 'san'], ['ibaeta', 'ciudad', 'cervezas', 'hernani']], KW Curr: ['ibaeta', 'cervezas', 'el', 'buildings']\n",
      "weighted: tensor([0.3776])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3776])\n",
      "===============================================\n",
      "['ciudad', 'hernani', 'astigarraga', 'district']\n",
      "['urumea', 'district', 'donostia', 'river']\n",
      "Got the keywords in 0.1708 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain'], ['cortazar', 'gipuzkoa', 'donostia', 'tolosa'], ['cortazar', 'koxkeroak', 'vieja', 'joxemaritarrak'], ['koxkeroak', 'vieja', 'miramar', 'león'], ['euskotren', 'miramar'], ['amara', 'madrid'], ['amara', 'district', 'city', 'built'], ['palace', 'district', 'residence', 'area'], ['egia', 'donostia', 'palace', 'anoeta'], ['egia', 'intxaurrondo', 'donostia', 'walnut'], ['intxaurrondo', 'altza', 'basque', 'sebastián'], ['ibaeta', 'basque', 'buildings', 'san'], ['ibaeta', 'ciudad', 'cervezas', 'hernani'], ['ciudad', 'astigarraga', 'urumea', 'river']], KW Curr: ['ciudad', 'hernani', 'astigarraga', 'district']\n",
      "weighted: tensor([0.3227])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3227])\n",
      "===============================================\n",
      "['urumea', 'district', 'donostia', 'river']\n",
      "['astigarraga', 'loiola', 'martutene', 'municipality']\n",
      "Got the keywords in 0.2023 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['navarre', 'donostia', 'navarre', 'fuero'], ['gipuzkoa', 'donostia', 'navarre', 'spain'], ['cortazar', 'gipuzkoa', 'donostia', 'tolosa'], ['cortazar', 'koxkeroak', 'vieja', 'joxemaritarrak'], ['koxkeroak', 'vieja', 'miramar', 'león'], ['euskotren', 'miramar'], ['amara', 'madrid'], ['amara', 'district', 'city', 'built'], ['palace', 'district', 'residence', 'area'], ['egia', 'donostia', 'palace', 'anoeta'], ['egia', 'intxaurrondo', 'donostia', 'walnut'], ['intxaurrondo', 'altza', 'basque', 'sebastián'], ['ibaeta', 'basque', 'buildings', 'san'], ['ibaeta', 'ciudad', 'cervezas', 'hernani'], ['ciudad', 'astigarraga', 'urumea', 'river'], ['astigarraga', 'district', 'river', 'municipality']], KW Curr: ['urumea', 'district', 'donostia', 'river']\n",
      "weighted: tensor([0.2378])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2378])\n",
      "===============================================\n",
      "['astigarraga', 'loiola', 'martutene', 'municipality']\n",
      "['ulia', 'donostia', 'pasaia', 'gardens']\n",
      "Got the keywords in 0.2630 seconds\n",
      "Got the embeddings and comparisons in 0.0009 seconds\n",
      "Coherence Map: [[]], KW Curr: ['astigarraga', 'loiola', 'martutene', 'municipality']\n",
      "weighted: tensor([0.1914])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.1914])\n",
      "===============================================\n",
      "['ulia', 'donostia', 'pasaia', 'gardens']\n",
      "['exclave', 'zubieta', 'donostia', 'village']\n",
      "Got the keywords in 0.2368 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['ulia', 'exclave']], KW Curr: ['ulia', 'donostia', 'pasaia', 'gardens']\n",
      "weighted: tensor([0.3102])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3102])\n",
      "===============================================\n",
      "['exclave', 'zubieta', 'donostia', 'village']\n",
      "['festivals', 'wrocław', 'sebastián', 'festival']\n",
      "Got the keywords in 0.3336 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['ulia', 'exclave'], ['exclave', 'festivals']], KW Curr: ['exclave', 'zubieta', 'donostia', 'village']\n",
      "weighted: tensor([0.3893])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3893])\n",
      "===============================================\n",
      "['festivals', 'wrocław', 'sebastián', 'festival']\n",
      "['festival', 'tamborrada', 'celebrations', 'feast']\n",
      "Got the keywords in 0.4504 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['ulia', 'exclave'], ['exclave', 'festivals'], ['festival', 'tamborrada', 'celebrations', 'festivals']], KW Curr: ['festivals', 'wrocław', 'sebastián', 'festival']\n",
      "weighted: tensor([0.4343])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4343])\n",
      "===============================================\n",
      "['festival', 'tamborrada', 'celebrations', 'feast']\n",
      "['festival', 'fireworks', 'semana', 'grande']\n",
      "Got the keywords in 0.2811 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['ulia', 'exclave'], ['exclave', 'festivals'], ['festival', 'tamborrada', 'celebrations', 'festivals'], ['festival', 'festival', 'tamborrada', 'semana']], KW Curr: ['festival', 'tamborrada', 'celebrations', 'feast']\n",
      "weighted: tensor([0.4352])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4352])\n",
      "===============================================\n",
      "['festival', 'fireworks', 'semana', 'grande']\n",
      "['festival', 'biscay', 'rowing', 'basque']\n",
      "Got the keywords in 0.2827 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['ulia', 'exclave'], ['exclave', 'festivals'], ['festival', 'tamborrada', 'celebrations', 'festivals'], ['festival', 'festival', 'tamborrada', 'semana'], ['festival', 'festival', 'biscay', 'grande']], KW Curr: ['festival', 'fireworks', 'semana', 'grande']\n",
      "weighted: tensor([0.3506])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3506])\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['festival', 'biscay', 'rowing', 'basque']\n",
      "['agatha', 'saint', 'carnival', 'eve']\n",
      "Got the keywords in 0.2731 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['ulia', 'exclave'], ['exclave', 'festivals'], ['festival', 'tamborrada', 'celebrations', 'festivals'], ['festival', 'festival', 'tamborrada', 'semana'], ['festival', 'festival', 'biscay', 'grande'], ['biscay', 'rowing', 'saint', 'carnival']], KW Curr: ['festival', 'biscay', 'rowing', 'basque']\n",
      "weighted: tensor([0.2798])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2798])\n",
      "===============================================\n",
      "['agatha', 'saint', 'carnival', 'eve']\n",
      "['festival', 'gypsy', 'romani', 'carnival']\n",
      "Got the keywords in 0.2211 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['ulia', 'exclave'], ['exclave', 'festivals'], ['festival', 'tamborrada', 'celebrations', 'festivals'], ['festival', 'festival', 'tamborrada', 'semana'], ['festival', 'festival', 'biscay', 'grande'], ['biscay', 'rowing', 'saint', 'carnival'], ['festival', 'agatha', 'gypsy', 'saint']], KW Curr: ['agatha', 'saint', 'carnival', 'eve']\n",
      "weighted: tensor([0.2851])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2851])\n",
      "===============================================\n",
      "['festival', 'gypsy', 'romani', 'carnival']\n",
      "['festival', 'gipuzkoa', 'chorizo', 'konstituzio']\n",
      "Got the keywords in 0.2451 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['ulia', 'exclave'], ['exclave', 'festivals'], ['festival', 'tamborrada', 'celebrations', 'festivals'], ['festival', 'festival', 'tamborrada', 'semana'], ['festival', 'festival', 'biscay', 'grande'], ['biscay', 'rowing', 'saint', 'carnival'], ['festival', 'agatha', 'gypsy', 'saint'], ['festival', 'festival', 'romani', 'chorizo']], KW Curr: ['festival', 'gypsy', 'romani', 'carnival']\n",
      "weighted: tensor([0.3501])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3501])\n",
      "===============================================\n",
      "['festival', 'gipuzkoa', 'chorizo', 'konstituzio']\n",
      "['christmas', 'basque', 'villages', 'streets']\n",
      "Got the keywords in 0.2174 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['ulia', 'exclave'], ['exclave', 'festivals'], ['festival', 'tamborrada', 'celebrations', 'festivals'], ['festival', 'festival', 'tamborrada', 'semana'], ['festival', 'festival', 'biscay', 'grande'], ['biscay', 'rowing', 'saint', 'carnival'], ['festival', 'agatha', 'gypsy', 'saint'], ['festival', 'festival', 'romani', 'chorizo'], ['festival', 'gipuzkoa', 'christmas', 'streets']], KW Curr: ['festival', 'gipuzkoa', 'chorizo', 'konstituzio']\n",
      "weighted: tensor([0.2872])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2872])\n",
      "===============================================\n",
      "['christmas', 'basque', 'villages', 'streets']\n",
      "['tourism', 'sebastián', 'destinations', 'spain']\n",
      "Got the keywords in 0.1457 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['ulia', 'exclave'], ['exclave', 'festivals'], ['festival', 'tamborrada', 'celebrations', 'festivals'], ['festival', 'festival', 'tamborrada', 'semana'], ['festival', 'festival', 'biscay', 'grande'], ['biscay', 'rowing', 'saint', 'carnival'], ['festival', 'agatha', 'gypsy', 'saint'], ['festival', 'festival', 'romani', 'chorizo'], ['festival', 'gipuzkoa', 'christmas', 'streets'], ['tourism', 'spain', 'basque', 'streets']], KW Curr: ['christmas', 'basque', 'villages', 'streets']\n",
      "weighted: tensor([0.2755])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2755])\n",
      "===============================================\n",
      "['tourism', 'sebastián', 'destinations', 'spain']\n",
      "['donostia', 'cuisine', 'restaurants', 'culinary']\n",
      "Got the keywords in 0.2078 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['ulia', 'exclave'], ['exclave', 'festivals'], ['festival', 'tamborrada', 'celebrations', 'festivals'], ['festival', 'festival', 'tamborrada', 'semana'], ['festival', 'festival', 'biscay', 'grande'], ['biscay', 'rowing', 'saint', 'carnival'], ['festival', 'agatha', 'gypsy', 'saint'], ['festival', 'festival', 'romani', 'chorizo'], ['festival', 'gipuzkoa', 'christmas', 'streets'], ['tourism', 'spain', 'basque', 'streets'], []], KW Curr: ['tourism', 'sebastián', 'destinations', 'spain']\n",
      "weighted: tensor([0.2178])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2178])\n",
      "===============================================\n",
      "['donostia', 'cuisine', 'restaurants', 'culinary']\n",
      "['universidad', 'donostia', 'university', 'universities']\n",
      "Got the keywords in 0.3104 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['donostia', 'universidad']], KW Curr: ['donostia', 'cuisine', 'restaurants', 'culinary']\n",
      "weighted: tensor([0.2926])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2926])\n",
      "===============================================\n",
      "['universidad', 'donostia', 'university', 'universities']\n",
      "['sociedad', 'liga', 'anoeta', 'stadium']\n",
      "Got the keywords in 0.3409 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['donostia', 'universidad'], ['sociedad', 'universidad', 'liga', 'university']], KW Curr: ['universidad', 'donostia', 'university', 'universities']\n",
      "weighted: tensor([0.3685])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3685])\n",
      "===============================================\n",
      "['sociedad', 'liga', 'anoeta', 'stadium']\n",
      "['hospers', 'sioux', 'railroad', 'founded']\n",
      "Got the keywords in 0.1952 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['donostia', 'universidad'], ['sociedad', 'universidad', 'liga', 'university'], ['sociedad', 'hospers']], KW Curr: ['sociedad', 'liga', 'anoeta', 'stadium']\n",
      "weighted: tensor([0.3483])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.3483])\n",
      "===============================================\n",
      "['hospers', 'sioux', 'railroad', 'founded']\n",
      "['hospers', 'located', 'area', 'city']\n",
      "Got the keywords in 0.1132 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['donostia', 'universidad'], ['sociedad', 'universidad', 'liga', 'university'], ['sociedad', 'hospers'], ['hospers', 'located']], KW Curr: ['hospers', 'sioux', 'railroad', 'founded']\n",
      "weighted: tensor([0.3368])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3368])\n",
      "===============================================\n",
      "['hospers', 'located', 'area', 'city']\n",
      "['census', 'population', 'households', 'families']\n",
      "Got the keywords in 0.1978 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['donostia', 'universidad'], ['sociedad', 'universidad', 'liga', 'university'], ['sociedad', 'hospers'], ['hospers', 'located'], ['hospers', 'census', 'population', 'households']], KW Curr: ['hospers', 'located', 'area', 'city']\n",
      "weighted: tensor([0.3205])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3205])\n",
      "===============================================\n",
      "['census', 'population', 'households', 'families']\n",
      "['population', 'census', 'households', '280']\n",
      "Got the keywords in 0.3290 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['donostia', 'universidad'], ['sociedad', 'universidad', 'liga', 'university'], ['sociedad', 'hospers'], ['hospers', 'located'], ['hospers', 'census', 'population', 'households'], ['census', 'population', 'population', 'households']], KW Curr: ['census', 'population', 'households', 'families']\n",
      "weighted: tensor([0.5334])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5334])\n",
      "===============================================\n",
      "['population', 'census', 'households', '280']\n",
      "['hospers', 'school', 'moc', 'students']\n",
      "Got the keywords in 0.2156 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['donostia', 'universidad'], ['sociedad', 'universidad', 'liga', 'university'], ['sociedad', 'hospers'], ['hospers', 'located'], ['hospers', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'hospers', 'census', '280']], KW Curr: ['population', 'census', 'households', '280']\n",
      "weighted: tensor([0.4027])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4027])\n",
      "===============================================\n",
      "['hospers', 'school', 'moc', 'students']\n",
      "['izyum', 'izium', 'ukraine', 'tatars']\n",
      "Got the keywords in 0.2067 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['donostia', 'universidad'], ['sociedad', 'universidad', 'liga', 'university'], ['sociedad', 'hospers'], ['hospers', 'located'], ['hospers', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'hospers', 'census', '280'], ['izium', 'moc']], KW Curr: ['hospers', 'school', 'moc', 'students']\n",
      "weighted: tensor([0.3009])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.3009])\n",
      "===============================================\n",
      "['izyum', 'izium', 'ukraine', 'tatars']\n",
      "['izium', '1943', 'bridgehead', '1942']\n",
      "Got the keywords in 0.1963 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['donostia', 'universidad'], ['sociedad', 'universidad', 'liga', 'university'], ['sociedad', 'hospers'], ['hospers', 'located'], ['hospers', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'hospers', 'census', '280'], ['izium', 'moc'], ['izium', 'izium', 'bridgehead', 'ukraine']], KW Curr: ['izyum', 'izium', 'ukraine', 'tatars']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted: tensor([0.3032])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3032])\n",
      "===============================================\n",
      "['izium', '1943', 'bridgehead', '1942']\n",
      "['sloviansk', 'ukraine', 'izium', 'russia']\n",
      "Got the keywords in 0.2837 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['donostia', 'universidad'], ['sociedad', 'universidad', 'liga', 'university'], ['sociedad', 'hospers'], ['hospers', 'located'], ['hospers', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'hospers', 'census', '280'], ['izium', 'moc'], ['izium', 'izium', 'bridgehead', 'ukraine'], ['sloviansk', 'ukraine', 'izium', 'izium']], KW Curr: ['izium', '1943', 'bridgehead', '1942']\n",
      "weighted: tensor([0.4422])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4422])\n",
      "===============================================\n",
      "['sloviansk', 'ukraine', 'izium', 'russia']\n",
      "['lennon', 'square', 'soviet', 'decommunization']\n",
      "Got the keywords in 0.2523 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['donostia', 'universidad'], ['sociedad', 'universidad', 'liga', 'university'], ['sociedad', 'hospers'], ['hospers', 'located'], ['hospers', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'hospers', 'census', '280'], ['izium', 'moc'], ['izium', 'izium', 'bridgehead', 'ukraine'], ['sloviansk', 'ukraine', 'izium', 'izium'], ['sloviansk', 'ukraine', 'lennon', 'decommunization']], KW Curr: ['sloviansk', 'ukraine', 'izium', 'russia']\n",
      "weighted: tensor([0.4159])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4159])\n",
      "===============================================\n",
      "['lennon', 'square', 'soviet', 'decommunization']\n",
      "['köppen', 'climate', 'warm', 'dfb']\n",
      "Got the keywords in 0.0851 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['donostia', 'universidad'], ['sociedad', 'universidad', 'liga', 'university'], ['sociedad', 'hospers'], ['hospers', 'located'], ['hospers', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'hospers', 'census', '280'], ['izium', 'moc'], ['izium', 'izium', 'bridgehead', 'ukraine'], ['sloviansk', 'ukraine', 'izium', 'izium'], ['sloviansk', 'ukraine', 'lennon', 'decommunization'], ['köppen', 'soviet']], KW Curr: ['lennon', 'square', 'soviet', 'decommunization']\n",
      "weighted: tensor([0.2605])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2605])\n",
      "===============================================\n",
      "['köppen', 'climate', 'warm', 'dfb']\n",
      "['wallingford', 'area', 'located', 'city']\n",
      "Got the keywords in 0.1115 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['donostia', 'universidad'], ['sociedad', 'universidad', 'liga', 'university'], ['sociedad', 'hospers'], ['hospers', 'located'], ['hospers', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'hospers', 'census', '280'], ['izium', 'moc'], ['izium', 'izium', 'bridgehead', 'ukraine'], ['sloviansk', 'ukraine', 'izium', 'izium'], ['sloviansk', 'ukraine', 'lennon', 'decommunization'], ['köppen', 'soviet'], []], KW Curr: ['köppen', 'climate', 'warm', 'dfb']\n",
      "weighted: tensor([0.2123])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.2123])\n",
      "===============================================\n",
      "['wallingford', 'area', 'located', 'city']\n",
      "['census', 'population', 'households', 'household']\n",
      "Got the keywords in 0.1384 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households']], KW Curr: ['wallingford', 'area', 'located', 'city']\n",
      "weighted: tensor([0.2773])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2773])\n",
      "===============================================\n",
      "['census', 'population', 'households', 'household']\n",
      "['census', 'population', 'households', 'families']\n",
      "Got the keywords in 0.2496 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population']], KW Curr: ['census', 'population', 'households', 'household']\n",
      "weighted: tensor([0.5813])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5813])\n",
      "===============================================\n",
      "['census', 'population', 'households', 'families']\n",
      "['banya', 'gora', 'resort', 'valley']\n",
      "Got the keywords in 0.2569 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], []], KW Curr: ['census', 'population', 'households', 'families']\n",
      "weighted: tensor([0.2557])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.2557])\n",
      "===============================================\n",
      "['banya', 'gora', 'resort', 'valley']\n",
      "['hydrotherapy', 'hydrothermal', 'village', 'ancient']\n",
      "Got the keywords in 0.1679 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], [], ['hydrotherapy', 'banya', 'ancient', 'gora']], KW Curr: ['banya', 'gora', 'resort', 'valley']\n",
      "weighted: tensor([0.2647])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2647])\n",
      "===============================================\n",
      "['hydrotherapy', 'hydrothermal', 'village', 'ancient']\n",
      "['boris', 'palace', 'banya', 'bagarov']\n",
      "Got the keywords in 0.1314 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], [], ['hydrotherapy', 'banya', 'ancient', 'gora'], ['hydrotherapy', 'boris']], KW Curr: ['hydrotherapy', 'hydrothermal', 'village', 'ancient']\n",
      "weighted: tensor([0.2741])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2741])\n",
      "===============================================\n",
      "['boris', 'palace', 'banya', 'bagarov']\n",
      "['harvard', 'railroad', '1871', 'massachusetts']\n",
      "Got the keywords in 0.1369 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], [], ['hydrotherapy', 'banya', 'ancient', 'gora'], ['hydrotherapy', 'boris'], ['harvard', 'boris']], KW Curr: ['boris', 'palace', 'banya', 'bagarov']\n",
      "weighted: tensor([0.3041])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.3041])\n",
      "===============================================\n",
      "['harvard', 'railroad', '1871', 'massachusetts']\n",
      "['harvard', 'area', 'located', '096554']\n",
      "Got the keywords in 0.1006 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], [], ['hydrotherapy', 'banya', 'ancient', 'gora'], ['hydrotherapy', 'boris'], ['harvard', 'boris'], ['harvard', '096554']], KW Curr: ['harvard', 'railroad', '1871', 'massachusetts']\n",
      "weighted: tensor([0.2735])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2735])\n",
      "===============================================\n",
      "['harvard', 'area', 'located', '096554']\n",
      "['census', 'population', 'households', 'families']\n",
      "Got the keywords in 0.1798 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], [], ['hydrotherapy', 'banya', 'ancient', 'gora'], ['hydrotherapy', 'boris'], ['harvard', 'boris'], ['harvard', '096554'], ['harvard', 'census', 'population', 'households']], KW Curr: ['harvard', 'area', 'located', '096554']\n",
      "weighted: tensor([0.3256])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3256])\n",
      "===============================================\n",
      "['census', 'population', 'households', 'families']\n",
      "['population', 'census', 'households', 'families']\n",
      "Got the keywords in 0.2685 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], [], ['hydrotherapy', 'banya', 'ancient', 'gora'], ['hydrotherapy', 'boris'], ['harvard', 'boris'], ['harvard', '096554'], ['harvard', 'census', 'population', 'households'], ['population', 'census', 'census', 'population']], KW Curr: ['census', 'population', 'households', 'families']\n",
      "weighted: tensor([0.4741])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4741])\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['population', 'census', 'households', 'families']\n",
      "['suva', 'fiji', 'polynesia', 'islands']\n",
      "Got the keywords in 0.3807 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], [], ['hydrotherapy', 'banya', 'ancient', 'gora'], ['hydrotherapy', 'boris'], ['harvard', 'boris'], ['harvard', '096554'], ['harvard', 'census', 'population', 'households'], ['population', 'census', 'census', 'population'], ['suva', 'census', 'fiji', 'polynesia']], KW Curr: ['population', 'census', 'households', 'families']\n",
      "weighted: tensor([0.3657])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.3657])\n",
      "===============================================\n",
      "['suva', 'fiji', 'polynesia', 'islands']\n",
      "['suva', 'fiji', 'pacific', 'viti']\n",
      "Got the keywords in 0.3665 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], [], ['hydrotherapy', 'banya', 'ancient', 'gora'], ['hydrotherapy', 'boris'], ['harvard', 'boris'], ['harvard', '096554'], ['harvard', 'census', 'population', 'households'], ['population', 'census', 'census', 'population'], ['suva', 'census', 'fiji', 'polynesia'], ['suva', 'suva', 'fiji', 'fiji']], KW Curr: ['suva', 'fiji', 'polynesia', 'islands']\n",
      "weighted: tensor([0.4252])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4252])\n",
      "===============================================\n",
      "['suva', 'fiji', 'pacific', 'viti']\n",
      "['suva', 'central', 'wards', 'area']\n",
      "Got the keywords in 0.2972 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], [], ['hydrotherapy', 'banya', 'ancient', 'gora'], ['hydrotherapy', 'boris'], ['harvard', 'boris'], ['harvard', '096554'], ['harvard', 'census', 'population', 'households'], ['population', 'census', 'census', 'population'], ['suva', 'census', 'fiji', 'polynesia'], ['suva', 'suva', 'fiji', 'fiji'], ['central', 'area', 'pacific', 'viti']], KW Curr: ['suva', 'fiji', 'pacific', 'viti']\n",
      "weighted: tensor([0.3110])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3110])\n",
      "===============================================\n",
      "['suva', 'central', 'wards', 'area']\n",
      "['areas', 'muanikau', 'zones', 'tamavua']\n",
      "Got the keywords in 0.1481 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], [], ['hydrotherapy', 'banya', 'ancient', 'gora'], ['hydrotherapy', 'boris'], ['harvard', 'boris'], ['harvard', '096554'], ['harvard', 'census', 'population', 'households'], ['population', 'census', 'census', 'population'], ['suva', 'census', 'fiji', 'polynesia'], ['suva', 'suva', 'fiji', 'fiji'], ['central', 'area', 'pacific', 'viti'], ['suva', 'central', 'areas', 'muanikau']], KW Curr: ['suva', 'central', 'wards', 'area']\n",
      "weighted: tensor([0.3970])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3970])\n",
      "===============================================\n",
      "['areas', 'muanikau', 'zones', 'tamavua']\n",
      "['suva', 'areas', 'rewa', 'corridor']\n",
      "Got the keywords in 0.2050 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], [], ['hydrotherapy', 'banya', 'ancient', 'gora'], ['hydrotherapy', 'boris'], ['harvard', 'boris'], ['harvard', '096554'], ['harvard', 'census', 'population', 'households'], ['population', 'census', 'census', 'population'], ['suva', 'census', 'fiji', 'polynesia'], ['suva', 'suva', 'fiji', 'fiji'], ['central', 'area', 'pacific', 'viti'], ['suva', 'central', 'areas', 'muanikau'], ['areas', 'muanikau', 'rewa', 'corridor']], KW Curr: ['areas', 'muanikau', 'zones', 'tamavua']\n",
      "weighted: tensor([0.5121])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5121])\n",
      "===============================================\n",
      "['suva', 'areas', 'rewa', 'corridor']\n",
      "['suva', 'climate', 'weather', 'rainfall']\n",
      "Got the keywords in 0.2615 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], [], ['hydrotherapy', 'banya', 'ancient', 'gora'], ['hydrotherapy', 'boris'], ['harvard', 'boris'], ['harvard', '096554'], ['harvard', 'census', 'population', 'households'], ['population', 'census', 'census', 'population'], ['suva', 'census', 'fiji', 'polynesia'], ['suva', 'suva', 'fiji', 'fiji'], ['central', 'area', 'pacific', 'viti'], ['suva', 'central', 'areas', 'muanikau'], ['areas', 'muanikau', 'rewa', 'corridor'], ['suva', 'suva']], KW Curr: ['suva', 'areas', 'rewa', 'corridor']\n",
      "weighted: tensor([0.3369])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3369])\n",
      "===============================================\n",
      "['suva', 'climate', 'weather', 'rainfall']\n",
      "['suva', 'fijian', 'fijians', 'fiji']\n",
      "Got the keywords in 0.2808 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], [], ['hydrotherapy', 'banya', 'ancient', 'gora'], ['hydrotherapy', 'boris'], ['harvard', 'boris'], ['harvard', '096554'], ['harvard', 'census', 'population', 'households'], ['population', 'census', 'census', 'population'], ['suva', 'census', 'fiji', 'polynesia'], ['suva', 'suva', 'fiji', 'fiji'], ['central', 'area', 'pacific', 'viti'], ['suva', 'central', 'areas', 'muanikau'], ['areas', 'muanikau', 'rewa', 'corridor'], ['suva', 'suva'], []], KW Curr: ['suva', 'climate', 'weather', 'rainfall']\n",
      "weighted: tensor([0.2833])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2833])\n",
      "===============================================\n",
      "['suva', 'fijian', 'fijians', 'fiji']\n",
      "['suva', 'fiji', 'councillors', 'municipal']\n",
      "Got the keywords in 0.2436 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['wallingford', 'census', 'population', 'households'], ['census', 'census', 'population', 'population'], [], ['hydrotherapy', 'banya', 'ancient', 'gora'], ['hydrotherapy', 'boris'], ['harvard', 'boris'], ['harvard', '096554'], ['harvard', 'census', 'population', 'households'], ['population', 'census', 'census', 'population'], ['suva', 'census', 'fiji', 'polynesia'], ['suva', 'suva', 'fiji', 'fiji'], ['central', 'area', 'pacific', 'viti'], ['suva', 'central', 'areas', 'muanikau'], ['areas', 'muanikau', 'rewa', 'corridor'], ['suva', 'suva'], [], ['fijian', 'municipal']], KW Curr: ['suva', 'fijian', 'fijians', 'fiji']\n",
      "weighted: tensor([0.2047])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2047])\n",
      "===============================================\n",
      "['suva', 'fiji', 'councillors', 'municipal']\n",
      "['fiji', 'fijian', 'suva', 'buildings']\n",
      "Got the keywords in 0.3048 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['suva', 'fiji', 'fijian', 'suva']], KW Curr: ['suva', 'fiji', 'councillors', 'municipal']\n",
      "weighted: tensor([0.4500])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4500])\n",
      "===============================================\n",
      "['fiji', 'fijian', 'suva', 'buildings']\n",
      "['suva', 'fiji', 'fijians', 'pacific']\n",
      "Got the keywords in 0.3962 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['suva', 'fiji', 'fijian', 'suva'], []], KW Curr: ['fiji', 'fijian', 'suva', 'buildings']\n",
      "weighted: tensor([0.2743])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2743])\n",
      "===============================================\n",
      "['suva', 'fiji', 'fijians', 'pacific']\n",
      "['suva', 'fiji', 'oceania', 'pacific']\n",
      "Got the keywords in 0.3825 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['suva', 'fiji', 'fijian', 'suva'], [], ['suva', 'fiji']], KW Curr: ['suva', 'fiji', 'fijians', 'pacific']\n",
      "weighted: tensor([0.2557])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2557])\n",
      "===============================================\n",
      "['suva', 'fiji', 'oceania', 'pacific']\n",
      "['suva', 'oceania', 'regional', 'infrastructure']\n",
      "Got the keywords in 0.2636 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['suva', 'fiji', 'fijian', 'suva'], [], ['suva', 'fiji'], ['suva', 'suva', 'pacific', 'infrastructure']], KW Curr: ['suva', 'fiji', 'oceania', 'pacific']\n",
      "weighted: tensor([0.3506])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3506])\n",
      "===============================================\n",
      "['suva', 'oceania', 'regional', 'infrastructure']\n",
      "['suva', 'seats', 'seating', 'venues']\n",
      "Got the keywords in 0.1496 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['suva', 'fiji', 'fijian', 'suva'], [], ['suva', 'fiji'], ['suva', 'suva', 'pacific', 'infrastructure'], []], KW Curr: ['suva', 'oceania', 'regional', 'infrastructure']\n",
      "weighted: tensor([0.2844])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2844])\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['suva', 'seats', 'seating', 'venues']\n",
      "['suva', 'fiji', 'gardens', 'garden']\n",
      "Got the keywords in 0.1976 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['suva', 'fiji', 'fijian', 'suva'], [], ['suva', 'fiji'], ['suva', 'suva', 'pacific', 'infrastructure'], [], []], KW Curr: ['suva', 'seats', 'seating', 'venues']\n",
      "weighted: tensor([0.1885])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.1885])\n",
      "===============================================\n",
      "['suva', 'fiji', 'gardens', 'garden']\n",
      "['suva', 'concerts', 'performances', 'singers']\n",
      "Got the keywords in 0.2432 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['suva', 'suva', 'concerts', 'fiji']], KW Curr: ['suva', 'fiji', 'gardens', 'garden']\n",
      "weighted: tensor([0.3086])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3086])\n",
      "===============================================\n",
      "['suva', 'concerts', 'performances', 'singers']\n",
      "['suva', 'fijian', 'fijians', 'fiji']\n",
      "Got the keywords in 0.2314 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['suva', 'suva', 'concerts', 'fiji'], ['suva', 'suva', 'fijian', 'concerts']], KW Curr: ['suva', 'concerts', 'performances', 'singers']\n",
      "weighted: tensor([0.3285])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3285])\n",
      "===============================================\n",
      "['suva', 'fijian', 'fijians', 'fiji']\n",
      "['festivals', 'festival', 'suva', 'carnival']\n",
      "Got the keywords in 0.1883 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['suva', 'suva', 'concerts', 'fiji'], ['suva', 'suva', 'fijian', 'concerts'], ['fijian', 'suva']], KW Curr: ['suva', 'fijian', 'fijians', 'fiji']\n",
      "weighted: tensor([0.2582])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2582])\n",
      "===============================================\n",
      "['festivals', 'festival', 'suva', 'carnival']\n",
      "['suva', 'nightlife', 'nightclubs', 'lounges']\n",
      "Got the keywords in 0.1974 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['suva', 'suva', 'concerts', 'fiji'], ['suva', 'suva', 'fijian', 'concerts'], ['fijian', 'suva'], ['suva', 'suva', 'carnival', 'nightlife']], KW Curr: ['festivals', 'festival', 'suva', 'carnival']\n",
      "weighted: tensor([0.3389])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3389])\n",
      "===============================================\n",
      "['suva', 'nightlife', 'nightclubs', 'lounges']\n",
      "['suva', 'theatres', 'laucala', 'cinema']\n",
      "Got the keywords in 0.2060 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['suva', 'suva', 'concerts', 'fiji'], ['suva', 'suva', 'fijian', 'concerts'], ['fijian', 'suva'], ['suva', 'suva', 'carnival', 'nightlife'], ['suva', 'suva', 'nightlife', 'nightclubs']], KW Curr: ['suva', 'nightlife', 'nightclubs', 'lounges']\n",
      "weighted: tensor([0.3258])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3258])\n",
      "===============================================\n",
      "['suva', 'theatres', 'laucala', 'cinema']\n",
      "['suva', 'fiji', 'pacific', 'stadium']\n",
      "Got the keywords in 0.2045 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['suva', 'suva', 'concerts', 'fiji'], ['suva', 'suva', 'fijian', 'concerts'], ['fijian', 'suva'], ['suva', 'suva', 'carnival', 'nightlife'], ['suva', 'suva', 'nightlife', 'nightclubs'], ['fiji', 'theatres', 'pacific', 'cinema']], KW Curr: ['suva', 'theatres', 'laucala', 'cinema']\n",
      "weighted: tensor([0.2536])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2536])\n",
      "===============================================\n",
      "['suva', 'fiji', 'pacific', 'stadium']\n",
      "['suva', 'fiji', 'stations', 'broadcasters']\n",
      "Got the keywords in 0.2337 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['suva', 'suva', 'concerts', 'fiji'], ['suva', 'suva', 'fijian', 'concerts'], ['fijian', 'suva'], ['suva', 'suva', 'carnival', 'nightlife'], ['suva', 'suva', 'nightlife', 'nightclubs'], ['fiji', 'theatres', 'pacific', 'cinema'], ['suva', 'suva']], KW Curr: ['suva', 'fiji', 'pacific', 'stadium']\n",
      "weighted: tensor([0.3195])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3195])\n",
      "===============================================\n",
      "['suva', 'fiji', 'stations', 'broadcasters']\n",
      "['suva', 'shopping', 'areas', 'stores']\n",
      "Got the keywords in 0.4000 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['suva', 'suva', 'concerts', 'fiji'], ['suva', 'suva', 'fijian', 'concerts'], ['fijian', 'suva'], ['suva', 'suva', 'carnival', 'nightlife'], ['suva', 'suva', 'nightlife', 'nightclubs'], ['fiji', 'theatres', 'pacific', 'cinema'], ['suva', 'suva'], ['suva', 'suva']], KW Curr: ['suva', 'fiji', 'stations', 'broadcasters']\n",
      "weighted: tensor([0.3747])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3747])\n",
      "===============================================\n",
      "['suva', 'shopping', 'areas', 'stores']\n",
      "['auckland', 'suva', 'fiji', 'airport']\n",
      "Got the keywords in 0.4221 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['suva', 'suva', 'concerts', 'fiji'], ['suva', 'suva', 'fijian', 'concerts'], ['fijian', 'suva'], ['suva', 'suva', 'carnival', 'nightlife'], ['suva', 'suva', 'nightlife', 'nightclubs'], ['fiji', 'theatres', 'pacific', 'cinema'], ['suva', 'suva'], ['suva', 'suva'], ['suva', 'auckland', 'suva', 'shopping']], KW Curr: ['suva', 'shopping', 'areas', 'stores']\n",
      "weighted: tensor([0.4534])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4534])\n",
      "===============================================\n",
      "['auckland', 'suva', 'fiji', 'airport']\n",
      "['suva', 'fiji', 'actress', 'australian']\n",
      "Got the keywords in 0.3628 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['suva', 'suva', 'concerts', 'fiji'], ['suva', 'suva', 'fijian', 'concerts'], ['fijian', 'suva'], ['suva', 'suva', 'carnival', 'nightlife'], ['suva', 'suva', 'nightlife', 'nightclubs'], ['fiji', 'theatres', 'pacific', 'cinema'], ['suva', 'suva'], ['suva', 'suva'], ['suva', 'auckland', 'suva', 'shopping'], []], KW Curr: ['auckland', 'suva', 'fiji', 'airport']\n",
      "weighted: tensor([0.3096])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3096])\n",
      "===============================================\n",
      "['suva', 'fiji', 'actress', 'australian']\n",
      "['osaka', 'kashiwara', 'prefecture', 'fujidera']\n",
      "Got the keywords in 0.2704 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['suva', 'suva', 'concerts', 'fiji'], ['suva', 'suva', 'fijian', 'concerts'], ['fijian', 'suva'], ['suva', 'suva', 'carnival', 'nightlife'], ['suva', 'suva', 'nightlife', 'nightclubs'], ['fiji', 'theatres', 'pacific', 'cinema'], ['suva', 'suva'], ['suva', 'suva'], ['suva', 'auckland', 'suva', 'shopping'], [], ['australian', 'fujidera']], KW Curr: ['suva', 'fiji', 'actress', 'australian']\n",
      "weighted: tensor([0.2164])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.2164])\n",
      "===============================================\n",
      "['osaka', 'kashiwara', 'prefecture', 'fujidera']\n",
      "['kashiwara', 'kawachi', 'kokubu', 'kansai']\n",
      "Got the keywords in 0.1720 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [[]], KW Curr: ['osaka', 'kashiwara', 'prefecture', 'fujidera']\n",
      "weighted: tensor([0.2025])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2025])\n",
      "===============================================\n",
      "['kashiwara', 'kawachi', 'kokubu', 'kansai']\n",
      "['havana', 'area', 'located', 'city']\n",
      "Got the keywords in 0.1444 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['kashiwara', 'kawachi', 'area', 'city']], KW Curr: ['kashiwara', 'kawachi', 'kokubu', 'kansai']\n",
      "weighted: tensor([0.3508])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.3508])\n",
      "===============================================\n",
      "['havana', 'area', 'located', 'city']\n",
      "['greenville', 'havana', 'marvinville', 'station']\n",
      "Got the keywords in 0.1503 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['kashiwara', 'kawachi', 'area', 'city'], ['havana', 'greenville', 'havana', 'city']], KW Curr: ['havana', 'area', 'located', 'city']\n",
      "weighted: tensor([0.2377])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.2377])\n",
      "===============================================\n",
      "['greenville', 'havana', 'marvinville', 'station']\n",
      "['population', 'census', 'households', 'families']\n",
      "Got the keywords in 0.2033 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census']], KW Curr: ['greenville', 'havana', 'marvinville', 'station']\n",
      "weighted: tensor([0.3794])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3794])\n",
      "===============================================\n",
      "['population', 'census', 'households', 'families']\n",
      "['alvarado', 'mexico', 'city', 'office']\n",
      "Got the keywords in 0.1826 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population']], KW Curr: ['population', 'census', 'households', 'families']\n",
      "weighted: tensor([0.3647])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.3647])\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alvarado', 'mexico', 'city', 'office']\n",
      "['area', 'census', 'land', 'city']\n",
      "Got the keywords in 0.0909 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office']], KW Curr: ['alvarado', 'mexico', 'city', 'office']\n",
      "weighted: tensor([0.2909])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2909])\n",
      "===============================================\n",
      "['area', 'census', 'land', 'city']\n",
      "['census', 'population', 'households', 'families']\n",
      "Got the keywords in 0.1647 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city']], KW Curr: ['area', 'census', 'land', 'city']\n",
      "weighted: tensor([0.2769])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2769])\n",
      "===============================================\n",
      "['census', 'population', 'households', 'families']\n",
      "['population', 'census', 'households', 'household']\n",
      "Got the keywords in 0.2545 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census']], KW Curr: ['census', 'population', 'households', 'families']\n",
      "weighted: tensor([0.4890])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4890])\n",
      "===============================================\n",
      "['population', 'census', 'households', 'household']\n",
      "['hanska', 'incorporated', 'village', '1890']\n",
      "Got the keywords in 0.1937 seconds\n",
      "Got the embeddings and comparisons in 0.0002 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households']], KW Curr: ['population', 'census', 'households', 'household']\n",
      "weighted: tensor([0.4015])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4015])\n",
      "===============================================\n",
      "['hanska', 'incorporated', 'village', '1890']\n",
      "['area', 'census', 'land', 'city']\n",
      "Got the keywords in 0.0862 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city']], KW Curr: ['hanska', 'incorporated', 'village', '1890']\n",
      "weighted: tensor([0.3392])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3392])\n",
      "===============================================\n",
      "['area', 'census', 'land', 'city']\n",
      "['census', 'population', 'households', 'household']\n",
      "Got the keywords in 0.1595 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city']], KW Curr: ['area', 'census', 'land', 'city']\n",
      "weighted: tensor([0.2854])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2854])\n",
      "===============================================\n",
      "['census', 'population', 'households', 'household']\n",
      "['population', 'census', 'households', 'families']\n",
      "Got the keywords in 0.2660 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city'], ['population', 'census', 'census', 'population']], KW Curr: ['census', 'population', 'households', 'household']\n",
      "weighted: tensor([0.5920])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5920])\n",
      "===============================================\n",
      "['population', 'census', 'households', 'families']\n",
      "['washta', 'sioux', 'located', 'area']\n",
      "Got the keywords in 0.2126 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city'], ['population', 'census', 'census', 'population'], ['washta', 'population', 'census', 'households']], KW Curr: ['population', 'census', 'households', 'families']\n",
      "weighted: tensor([0.4799])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4799])\n",
      "===============================================\n",
      "['washta', 'sioux', 'located', 'area']\n",
      "['census', 'population', 'households', 'families']\n",
      "Got the keywords in 0.1874 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city'], ['population', 'census', 'census', 'population'], ['washta', 'population', 'census', 'households'], ['washta', 'census', 'population', 'households']], KW Curr: ['washta', 'sioux', 'located', 'area']\n",
      "weighted: tensor([0.5025])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5025])\n",
      "===============================================\n",
      "['census', 'population', 'households', 'families']\n",
      "['population', 'census', 'households', '282']\n",
      "Got the keywords in 0.2597 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city'], ['population', 'census', 'census', 'population'], ['washta', 'population', 'census', 'households'], ['washta', 'census', 'population', 'households'], ['census', 'population', 'population', 'households']], KW Curr: ['census', 'population', 'households', 'families']\n",
      "weighted: tensor([0.5326])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5326])\n",
      "===============================================\n",
      "['population', 'census', 'households', '282']\n",
      "['prairie', 'reed', 'ada', 'daggs']\n",
      "Got the keywords in 0.3304 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city'], ['population', 'census', 'census', 'population'], ['washta', 'population', 'census', 'households'], ['washta', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'prairie', 'reed', 'ada']], KW Curr: ['population', 'census', 'households', '282']\n",
      "weighted: tensor([0.3952])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.3952])\n",
      "===============================================\n",
      "['prairie', 'reed', 'ada', 'daggs']\n",
      "['ada', 'park', 'camp', 'bebee']\n",
      "Got the keywords in 0.2725 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city'], ['population', 'census', 'census', 'population'], ['washta', 'population', 'census', 'households'], ['washta', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'prairie', 'reed', 'ada'], []], KW Curr: ['prairie', 'reed', 'ada', 'daggs']\n",
      "weighted: tensor([0.2924])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.2924])\n",
      "===============================================\n",
      "['ada', 'park', 'camp', 'bebee']\n",
      "['ada', 'oklahoma', 'tulsa', 'texas']\n",
      "Got the keywords in 0.1524 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city'], ['population', 'census', 'census', 'population'], ['washta', 'population', 'census', 'households'], ['washta', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'prairie', 'reed', 'ada'], [], ['tulsa', 'ada', 'texas', 'camp']], KW Curr: ['ada', 'park', 'camp', 'bebee']\n",
      "weighted: tensor([0.3245])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3245])\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ada', 'oklahoma', 'tulsa', 'texas']\n",
      "['ada', 'census', 'population', 'households']\n",
      "Got the keywords in 0.2298 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city'], ['population', 'census', 'census', 'population'], ['washta', 'population', 'census', 'households'], ['washta', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'prairie', 'reed', 'ada'], [], ['tulsa', 'ada', 'texas', 'camp'], ['ada', 'ada']], KW Curr: ['ada', 'oklahoma', 'tulsa', 'texas']\n",
      "weighted: tensor([0.3950])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3950])\n",
      "===============================================\n",
      "['ada', 'census', 'population', 'households']\n",
      "['ada', 'headquartered', 'headquarters', 'companies']\n",
      "Got the keywords in 0.4269 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city'], ['population', 'census', 'census', 'population'], ['washta', 'population', 'census', 'households'], ['washta', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'prairie', 'reed', 'ada'], [], ['tulsa', 'ada', 'texas', 'camp'], ['ada', 'ada'], ['ada', 'ada', 'census', 'headquartered']], KW Curr: ['ada', 'census', 'population', 'households']\n",
      "weighted: tensor([0.4657])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4657])\n",
      "===============================================\n",
      "['ada', 'headquartered', 'headquarters', 'companies']\n",
      "['ecu', 'university', 'central', 'accredited']\n",
      "Got the keywords in 0.2851 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city'], ['population', 'census', 'census', 'population'], ['washta', 'population', 'census', 'households'], ['washta', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'prairie', 'reed', 'ada'], [], ['tulsa', 'ada', 'texas', 'camp'], ['ada', 'ada'], ['ada', 'ada', 'census', 'headquartered'], ['ecu', 'ada', 'university', 'central']], KW Curr: ['ada', 'headquartered', 'headquarters', 'companies']\n",
      "weighted: tensor([0.4552])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4552])\n",
      "===============================================\n",
      "['ecu', 'university', 'central', 'accredited']\n",
      "['ada', 'schools', 'school', 'glenwood']\n",
      "Got the keywords in 0.1544 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city'], ['population', 'census', 'census', 'population'], ['washta', 'population', 'census', 'households'], ['washta', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'prairie', 'reed', 'ada'], [], ['tulsa', 'ada', 'texas', 'camp'], ['ada', 'ada'], ['ada', 'ada', 'census', 'headquartered'], ['ecu', 'ada', 'university', 'central'], ['ecu', 'school']], KW Curr: ['ecu', 'university', 'central', 'accredited']\n",
      "weighted: tensor([0.3355])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3355])\n",
      "===============================================\n",
      "['ada', 'schools', 'school', 'glenwood']\n",
      "['pontotoc', 'located', 'tech', 'area']\n",
      "Got the keywords in 0.1367 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city'], ['population', 'census', 'census', 'population'], ['washta', 'population', 'census', 'households'], ['washta', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'prairie', 'reed', 'ada'], [], ['tulsa', 'ada', 'texas', 'camp'], ['ada', 'ada'], ['ada', 'ada', 'census', 'headquartered'], ['ecu', 'ada', 'university', 'central'], ['ecu', 'school'], ['pontotoc', 'ada', 'schools', 'located']], KW Curr: ['ada', 'schools', 'school', 'glenwood']\n",
      "weighted: tensor([0.3387])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3387])\n",
      "===============================================\n",
      "['pontotoc', 'located', 'tech', 'area']\n",
      "['ada', 'oklahoma', 'convicted', 'prosecutor']\n",
      "Got the keywords in 0.2090 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city'], ['population', 'census', 'census', 'population'], ['washta', 'population', 'census', 'households'], ['washta', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'prairie', 'reed', 'ada'], [], ['tulsa', 'ada', 'texas', 'camp'], ['ada', 'ada'], ['ada', 'ada', 'census', 'headquartered'], ['ecu', 'ada', 'university', 'central'], ['ecu', 'school'], ['pontotoc', 'ada', 'schools', 'located'], ['pontotoc', 'prosecutor']], KW Curr: ['pontotoc', 'located', 'tech', 'area']\n",
      "weighted: tensor([0.3490])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3490])\n",
      "===============================================\n",
      "['ada', 'oklahoma', 'convicted', 'prosecutor']\n",
      "['comodoro', 'jorge', 'tehuelche', 'rivadavia']\n",
      "Got the keywords in 0.4006 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['greenville', 'havana', 'population', 'census'], ['alvarado', 'population'], ['census', 'office'], ['census', 'city'], ['census', 'population', 'population', 'census'], ['hanska', 'population', 'census', 'households'], ['hanska', 'city'], ['population', 'city'], ['population', 'census', 'census', 'population'], ['washta', 'population', 'census', 'households'], ['washta', 'census', 'population', 'households'], ['census', 'population', 'population', 'households'], ['population', 'prairie', 'reed', 'ada'], [], ['tulsa', 'ada', 'texas', 'camp'], ['ada', 'ada'], ['ada', 'ada', 'census', 'headquartered'], ['ecu', 'ada', 'university', 'central'], ['ecu', 'school'], ['pontotoc', 'ada', 'schools', 'located'], ['pontotoc', 'prosecutor'], ['ada', 'comodoro', 'oklahoma', 'jorge']], KW Curr: ['ada', 'oklahoma', 'convicted', 'prosecutor']\n",
      "weighted: tensor([0.3836])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.3836])\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "num_samples = 250\n",
    "max_tokens = 256  # want to keep this under 512\n",
    "max_str_length = 30\n",
    "\n",
    "true_labels = text_labels[start : start + num_samples]\n",
    "\n",
    "predictions = coherence_tester(\n",
    "    text_data[start : start + num_samples],\n",
    "    true_labels,\n",
    "    max_tokens=max_tokens,\n",
    "    max_str_length=max_str_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a93727",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x[1] for x in predictions])\n",
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e7863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_string = \"\".join(str([x[1] for x in predictions]))\n",
    "true_string = \"\".join(str(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af16ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db43c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_score = windowdiff(pred_string, true_string, avg_k)\n",
    "pk_score = pk(pred_string, true_string, avg_k)\n",
    "\n",
    "print(f\"k = {avg_k}\")\n",
    "print(f\"wd = {wd_score}\")\n",
    "print(f\"pk = {pk_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb16194",
   "metadata": {},
   "source": [
    "## Prediction Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7148b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_thresholds = [0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af208503",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_thresh = 0.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cebcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_thresh in pred_thresholds:\n",
    "    modified_predictions = [\n",
    "        1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\n",
    "    ]\n",
    "\n",
    "    pred_string = \"\".join(str(modified_predictions))\n",
    "    true_string = \"\".join(str(true_labels))\n",
    "\n",
    "    avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\n",
    "\n",
    "    wd_score = windowdiff(pred_string, true_string, avg_k)\n",
    "    pk_score = pk(pred_string, true_string, avg_k)\n",
    "\n",
    "    print(f\"pred_thresh = {pred_thresh}\")\n",
    "    print(f\"k = {avg_k}\")\n",
    "    print(f\"wd = {wd_score}\")\n",
    "    print(f\"pk = {pk_score}\")\n",
    "    print(\"===========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "6a2f65af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 389;\n",
       "                var nbb_unformatted_code = \"print(pred_string)\\nprint(true_string)\";\n",
       "                var nbb_formatted_code = \"print(pred_string)\\nprint(true_string)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pred_string)\n",
    "print(true_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b9dbb752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 6\n",
      "wd = 0.4684563758389262\n",
      "pk = 0.4214765100671141\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 375;\n",
       "                var nbb_unformatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_formatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wd_score = windowdiff(pred_string, true_string, avg_k)\n",
    "pk_score = pk(pred_string, true_string, avg_k)\n",
    "\n",
    "print(f\"k = {avg_k}\")\n",
    "print(f\"wd = {wd_score}\")\n",
    "print(f\"pk = {pk_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34585627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "462a8434",
   "metadata": {},
   "source": [
    "## KeyBERT Embedding Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d15e7648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 172;\n",
       "                var nbb_unformatted_code = \"curr = 230\\nprev = curr - 1\";\n",
       "                var nbb_formatted_code = \"curr = 230\\nprev = curr - 1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "curr = 230\n",
    "prev = curr - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629634b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the keywords and embeddings library\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "similarities_lib = Similarities(\"bert-base-uncased\")\n",
    "keywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\n",
    "embedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8c6434ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the keywords in 0.6567 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "['cantonese', 'languages', 'vietnamese', 'communes']\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 205;\n",
       "                var nbb_unformatted_code = \"cohesion = coherence.get_coherence(\\n    [text_data[curr], text_data[prev]], coherence_threshold=0.25\\n)\\nprint([k[0] for k in cohesion])\";\n",
       "                var nbb_formatted_code = \"cohesion = coherence.get_coherence(\\n    [text_data[curr], text_data[prev]], coherence_threshold=0.25\\n)\\nprint([k[0] for k in cohesion])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cohesion = coherence.get_coherence(\n",
    "    [text_data[curr], text_data[prev]], coherence_threshold=0.25\n",
    ")\n",
    "print([k[0] for k in cohesion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "357c0021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 206;\n",
       "                var nbb_unformatted_code = \"# get the keywords for the current sentences\\nkeywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\\nkeywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\\n\\n# compute the word comparisons between the previous (with the coherence map)\\n# and the current (possibly the first sentence in a new segment)\\nword_comparisons_with_coherence, weights = compare_coherent_words(\\n    [keywords_prev], keywords_current\\n)\";\n",
       "                var nbb_formatted_code = \"# get the keywords for the current sentences\\nkeywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\\nkeywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\\n\\n# compute the word comparisons between the previous (with the coherence map)\\n# and the current (possibly the first sentence in a new segment)\\nword_comparisons_with_coherence, weights = compare_coherent_words(\\n    [keywords_prev], keywords_current\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the keywords for the current sentences\n",
    "keywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\n",
    "keywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\n",
    "\n",
    "# compute the word comparisons between the previous (with the coherence map)\n",
    "# and the current (possibly the first sentence in a new segment)\n",
    "word_comparisons_with_coherence, weights = compare_coherent_words(\n",
    "    [keywords_prev], keywords_current\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "dd52c9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('township', 0.2304),\n",
       "  ('communes', 0.1857),\n",
       "  ('hải', 0.1399),\n",
       "  ('wards', 0.1397),\n",
       "  ('đông', 0.1224)],\n",
       " [('cantonese', 0.5038),\n",
       "  ('mandarin', 0.464),\n",
       "  ('languages', 0.3483),\n",
       "  ('language', 0.343),\n",
       "  ('vietnamese', 0.3184)])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 207;\n",
       "                var nbb_unformatted_code = \"[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]\";\n",
       "                var nbb_formatted_code = \"[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121f953",
   "metadata": {},
   "source": [
    "# KeyBERT Embedding Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "559ab602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 679;\n",
       "                var nbb_unformatted_code = \"docs = [\\n        \\\"Hi my name is Devarsh\\\",\\n        \\\"Devarsh likes to play Basketball.\\\",\\n    \\\"I love to watch Cricket.\\\",\\n        \\\"I am a strong programmer. And my name is Devarsh\\\",\\n]\";\n",
       "                var nbb_formatted_code = \"docs = [\\n    \\\"Hi my name is Devarsh\\\",\\n    \\\"Devarsh likes to play Basketball.\\\",\\n    \\\"I love to watch Cricket.\\\",\\n    \\\"I am a strong programmer. And my name is Devarsh\\\",\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = [\n",
    "    \"Hi my name is Devarsh\",\n",
    "    \"Devarsh likes to play Basketball.\",\n",
    "    \"I love to watch Cricket.\",\n",
    "    \"I am a strong programmer. And my name is Devarsh\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "00458200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 680;\n",
       "                var nbb_unformatted_code = \"from keybert import KeyBERT\\n\\nkw_model = KeyBERT()\\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(\\n    docs, min_df=1, stop_words=\\\"english\\\"\\n)\\nkeywords = kw_model.extract_keywords(\\n    docs,\\n    min_df=1,\\n    stop_words=\\\"english\\\",\\n    doc_embeddings=doc_embeddings,\\n    word_embeddings=word_embeddings,\\n)\";\n",
       "                var nbb_formatted_code = \"from keybert import KeyBERT\\n\\nkw_model = KeyBERT()\\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(\\n    docs, min_df=1, stop_words=\\\"english\\\"\\n)\\nkeywords = kw_model.extract_keywords(\\n    docs,\\n    min_df=1,\\n    stop_words=\\\"english\\\",\\n    doc_embeddings=doc_embeddings,\\n    word_embeddings=word_embeddings,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "doc_embeddings, word_embeddings = kw_model.extract_embeddings(\n",
    "    docs, min_df=1, stop_words=\"english\"\n",
    ")\n",
    "keywords = kw_model.extract_keywords(\n",
    "    docs,\n",
    "    min_df=1,\n",
    "    stop_words=\"english\",\n",
    "    doc_embeddings=doc_embeddings,\n",
    "    word_embeddings=word_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "7d30bae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 681;\n",
       "                var nbb_unformatted_code = \"len(doc_embeddings)\";\n",
       "                var nbb_formatted_code = \"len(doc_embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "018ee52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 682;\n",
       "                var nbb_unformatted_code = \"len(word_embeddings)\";\n",
       "                var nbb_formatted_code = \"len(word_embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "80cbdc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('devarsh', 0.6267), ('hi', 0.5216)],\n",
       " [('devarsh', 0.6549),\n",
       "  ('basketball', 0.5558),\n",
       "  ('play', 0.3787),\n",
       "  ('likes', 0.2284)],\n",
       " [('cricket', 0.7118), ('watch', 0.3656), ('love', 0.307)],\n",
       " [('programmer', 0.5942), ('devarsh', 0.5528), ('strong', 0.3452)]]"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 683;\n",
       "                var nbb_unformatted_code = \"keywords\";\n",
       "                var nbb_formatted_code = \"keywords\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "fd1ac50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 701;\n",
       "                var nbb_unformatted_code = \"kw_model = KeyBERT()\\nimport torch\\n\\n\\ndef get_keywords_with_embeddings_test(\\n    data,\\n) -> list[tuple[str, float, torch.Tensor]]:\\n    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\\n\\n    keywords = kw_model.extract_keywords(\\n        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\\n    )\\n\\n    keywords_with_embeddings = []\\n    count = 0\\n    print(len(word_embeddings))\\n    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\\n        for j, words in enumerate(kw):\\n            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\\n            count += 1\\n\\n    return keywords_with_embeddings\";\n",
       "                var nbb_formatted_code = \"kw_model = KeyBERT()\\nimport torch\\n\\n\\ndef get_keywords_with_embeddings_test(\\n    data,\\n) -> list[tuple[str, float, torch.Tensor]]:\\n    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\\n\\n    keywords = kw_model.extract_keywords(\\n        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\\n    )\\n\\n    keywords_with_embeddings = []\\n    count = 0\\n    print(len(word_embeddings))\\n    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\\n        for j, words in enumerate(kw):\\n            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\\n            count += 1\\n\\n    return keywords_with_embeddings\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kw_model = KeyBERT()\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_keywords_with_embeddings_test(\n",
    "    data,\n",
    ") -> list[tuple[str, float, torch.Tensor]]:\n",
    "    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\n",
    "\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\n",
    "    )\n",
    "\n",
    "    keywords_with_embeddings = []\n",
    "    count = 0\n",
    "    print(len(word_embeddings))\n",
    "    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\n",
    "        for j, words in enumerate(kw):\n",
    "            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\n",
    "            count += 1\n",
    "\n",
    "    return keywords_with_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "d1bbf3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 702;\n",
       "                var nbb_unformatted_code = \"embeddings = get_keywords_with_embeddings_test(docs)\";\n",
       "                var nbb_formatted_code = \"embeddings = get_keywords_with_embeddings_test(docs)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = get_keywords_with_embeddings_test(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "f1ea7b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 703;\n",
       "                var nbb_unformatted_code = \"len(embeddings)\";\n",
       "                var nbb_formatted_code = \"len(embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883ef15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
