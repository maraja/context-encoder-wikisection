{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc7697a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run if working locally\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ee84b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom utils.metrics import windowdiff, pk\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_formatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom utils.metrics import windowdiff, pk\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pickle\n",
    "import os, sys\n",
    "import config\n",
    "\n",
    "config.root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, config.root_path)\n",
    "\n",
    "from src.dataset.dataset import RawData\n",
    "from src.dataset.wikisection_preprocessing import (\n",
    "    tokenize,\n",
    "    clean_sentence,\n",
    "    preprocess_text_segmentation,\n",
    "    format_data_for_db_insertion,\n",
    ")\n",
    "from src.dataset.utils import truncate_by_token\n",
    "from db.dbv2 import Table, AugmentedTable, TrainTestTable\n",
    "import pprint\n",
    "\n",
    "from utils.metrics import windowdiff, pk\n",
    "\n",
    "from src.bertkeywords.src.similarities import Embedding, Similarities\n",
    "from src.bertkeywords.src.keywords import Keywords\n",
    "from src.encoders.coherence import Coherence\n",
    "from src.dataset.utils import flatten, dedupe_list, truncate_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e4dcd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-04-14 13:28:26.971936: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-04-14 13:28:29.703951: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# initialize the coherence library\\nmax_words_per_step = 4\\ncoherence = Coherence(max_words_per_step=max_words_per_step)\";\n",
       "                var nbb_formatted_code = \"# initialize the coherence library\\nmax_words_per_step = 4\\ncoherence = Coherence(max_words_per_step=max_words_per_step)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the coherence library\n",
    "max_words_per_step = 4\n",
    "coherence = Coherence(max_words_per_step=max_words_per_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "226021b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-04-14 13:30:34.417238: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-04-14 13:30:37.415622: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# initialize the keywords and embeddings library\\npp = pprint.PrettyPrinter(indent=4)\\nsimilarities_lib = Similarities(\\\"bert-base-uncased\\\")\\nkeywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\\nembedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)\";\n",
       "                var nbb_formatted_code = \"# initialize the keywords and embeddings library\\npp = pprint.PrettyPrinter(indent=4)\\nsimilarities_lib = Similarities(\\\"bert-base-uncased\\\")\\nkeywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\\nembedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the keywords and embeddings library\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "similarities_lib = Similarities(\"bert-base-uncased\")\n",
    "keywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\n",
    "embedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb2458b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_formatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_type = \"city\"\n",
    "table = Table(dataset_type)\n",
    "augmented_table = AugmentedTable(dataset_type)\n",
    "train_test_table = TrainTestTable(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd59c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_formatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = table.get_all()\n",
    "\n",
    "text_data = [x[1] for x in data]\n",
    "text_labels = [x[2] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcccad5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_formatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_segments = table.get_all_segments()\n",
    "text_segments = [[y[1] for y in x] for x in all_segments]\n",
    "segments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "295657fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_formatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = 5\n",
    "max_tokens = 400\n",
    "\n",
    "for i, (segment, labels) in enumerate(\n",
    "    zip(text_segments[:samples], segments_labels[:samples])\n",
    "):\n",
    "    for sentence, label in zip(segment, labels):\n",
    "        # this is the training case. During inference, we will have no idea\n",
    "        # when segments start and when they end.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "467789d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"text_labels[:25]\";\n",
       "                var nbb_formatted_code = \"text_labels[:25]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_labels[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f6870992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 170;\n",
       "                var nbb_unformatted_code = \"pruning = 0  # remove the lowest n important words from coherence map\\npruning_min = 10  # only prune after n words in the coherence map\\n\\n\\ndef get_weighted_average(weighted_similarities, weights):\\n    return sum(weighted_similarities) / sum(weights)\\n\\n\\n# importance testing\\ndef compare_coherent_words(\\n    coherence_map, keywords_current, suppress_errors=False, same_word_multiplier=True\\n):\\n    word_comparisons = []\\n    weights = []\\n    for i, keywords in enumerate(coherence_map[::-1]):\\n        for word_tuple in keywords:\\n            word = word_tuple[0]\\n            for second_word_tuple in keywords_current:\\n                second_word = second_word_tuple[0]\\n\\n                try:\\n                    word_one_emb = word_tuple[2]\\n                    word_two_emb = second_word_tuple[2]\\n\\n                    if same_word_multiplier:\\n                        flattened_coherence_words_only = [\\n                            element[0]\\n                            for sublist in coherence_map\\n                            for element in sublist\\n                        ]\\n\\n                        num_occurrences = flattened_coherence_words_only.count(\\n                            second_word\\n                        )\\n\\n                        if num_occurrences > 0:\\n                            # amplify words that are found as duplicates in the coherence map\\n                            # if the word shows up 1 time, amplify the weight by 2 times\\n                            weighting_multiplier = (\\n                                flattened_coherence_words_only.count(second_word) + 2\\n                            )\\n                        else:\\n                            weighting_multiplier = (\\n                                1 / 3\\n                            )  # reduce the importance of this word\\n\\n                    else:\\n                        weighting_multiplier = 1  # set to 1 in case this is turned off.\\n\\n                    # this weight is a recipricol function that will grow smaller the further the keywords are away\\n                    # we want to put more importance on the current words, so we apply twice as much weight.\\n                    if i == 0:\\n                        weight = (weighting_multiplier * 2) / (i + 1)\\n                    else:\\n                        weight = (weighting_multiplier * 1) / (i + 1)\\n\\n                    word_comparisons.append(\\n                        (\\n                            word,\\n                            second_word,\\n                            weight\\n                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\\n                        )\\n                    )\\n                    weights.append(weight)\\n                except AssertionError as e:\\n                    if not suppress_errors:\\n                        print(e, word, second_word)\\n\\n    return word_comparisons, weights\\n\\n\\n# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\\ndef coherence_tester(\\n    text_data,\\n    text_labels,\\n    max_tokens=400,\\n    max_str_length=30,\\n    prediction_thresh=0.7,\\n    dynamic_threshold=True,\\n    threshold_warmup=10,  # number of iterations before using dynamic threshold\\n    last_n_threshold=5,  # will only consider the last n thresholds for dynamic threshold\\n):\\n    coherence_map = []\\n    predictions = []\\n    thresholds = []\\n    for i, (row, label) in enumerate(zip(text_data, text_labels)):\\n        threshold = prediction_thresh\\n        if dynamic_threshold and (i + 1) > threshold_warmup:\\n            last_n_thresholds = thresholds[(0 - last_n_threshold) :]\\n            last_n_thresholds.sort()\\n            mid = len(last_n_thresholds) // 2\\n            threshold = (last_n_thresholds[mid] + last_n_thresholds[~mid]) / 2\\n            print(f\\\"median threshold: {threshold}\\\")\\n        # compare the current sentence to the previous one\\n        if i == 0:\\n            predictions.append((0, 0))\\n        else:\\n            prev_row = text_data[i - 1]\\n\\n            row = truncate_by_token(row, max_tokens)\\n            prev_row = truncate_by_token(prev_row, max_tokens)\\n\\n            cohesion, keywords_prev, keywords_current = coherence.get_coherence(\\n                [row, prev_row], coherence_threshold=0.2\\n            )\\n\\n            pruning = 1\\n            pruning_min = 6\\n\\n            # add the keywords to the coherence map\\n            coherence_map.append(cohesion)\\n            if pruning > 0 and len(coherence_map) >= pruning_min:\\n                print(\\\"pruning...\\\", len(coherence_map))\\n                coherence_map = coherence_map[::-1][pruning:][\\n                    ::-1\\n                ]  # get the last n - pruning values and reverse the list\\n                print(\\\"done pruning...\\\", len(coherence_map))\\n\\n            # truncate the strings for printing\\n            truncated_row = truncate_string(row, max_str_length)\\n            truncated_prev_row = truncate_string(prev_row, max_str_length)\\n            print(\\n                f\\\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\\\"\\n            )\\n\\n            # compute the word comparisons between the previous (with the coherence map)\\n            # and the current (possibly the first sentence in a new segment)\\n            word_comparisons_with_coherence, weights = compare_coherent_words(\\n                [*coherence_map, keywords_prev], keywords_current\\n            )\\n\\n            similarities_with_coherence = [\\n                comparison[2] for comparison in word_comparisons_with_coherence\\n            ]\\n            avg_similarity_with_coherence = sum(similarities_with_coherence) / (\\n                len(similarities_with_coherence) or 1\\n            )\\n            weighted_avg_similarity_with_coherence = get_weighted_average(\\n                similarities_with_coherence, weights\\n            )\\n            print(f\\\"weighted: {weighted_avg_similarity_with_coherence}\\\")\\n\\n            # if the two sentences are similar, create a cohesive prediction\\n            # otherwise, predict a new segment\\n            if weighted_avg_similarity_with_coherence > threshold:\\n                print(\\n                    f\\\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 0))\\n            else:\\n                # start of a new segment, empty the map\\n                coherence_map = []\\n                print(\\n                    f\\\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 1))\\n\\n            thresholds.append(weighted_avg_similarity_with_coherence)\\n            print(\\\"===============================================\\\")\\n\\n    return predictions\";\n",
       "                var nbb_formatted_code = \"pruning = 0  # remove the lowest n important words from coherence map\\npruning_min = 10  # only prune after n words in the coherence map\\n\\n\\ndef get_weighted_average(weighted_similarities, weights):\\n    return sum(weighted_similarities) / sum(weights)\\n\\n\\n# importance testing\\ndef compare_coherent_words(\\n    coherence_map, keywords_current, suppress_errors=False, same_word_multiplier=True\\n):\\n    word_comparisons = []\\n    weights = []\\n    for i, keywords in enumerate(coherence_map[::-1]):\\n        for word_tuple in keywords:\\n            word = word_tuple[0]\\n            for second_word_tuple in keywords_current:\\n                second_word = second_word_tuple[0]\\n\\n                try:\\n                    word_one_emb = word_tuple[2]\\n                    word_two_emb = second_word_tuple[2]\\n\\n                    if same_word_multiplier:\\n                        flattened_coherence_words_only = [\\n                            element[0]\\n                            for sublist in coherence_map\\n                            for element in sublist\\n                        ]\\n\\n                        num_occurrences = flattened_coherence_words_only.count(\\n                            second_word\\n                        )\\n\\n                        if num_occurrences > 0:\\n                            # amplify words that are found as duplicates in the coherence map\\n                            # if the word shows up 1 time, amplify the weight by 2 times\\n                            weighting_multiplier = (\\n                                flattened_coherence_words_only.count(second_word) + 2\\n                            )\\n                        else:\\n                            weighting_multiplier = (\\n                                1 / 3\\n                            )  # reduce the importance of this word\\n\\n                    else:\\n                        weighting_multiplier = 1  # set to 1 in case this is turned off.\\n\\n                    # this weight is a recipricol function that will grow smaller the further the keywords are away\\n                    # we want to put more importance on the current words, so we apply twice as much weight.\\n                    if i == 0:\\n                        weight = (weighting_multiplier * 2) / (i + 1)\\n                    else:\\n                        weight = (weighting_multiplier * 1) / (i + 1)\\n\\n                    word_comparisons.append(\\n                        (\\n                            word,\\n                            second_word,\\n                            weight\\n                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\\n                        )\\n                    )\\n                    weights.append(weight)\\n                except AssertionError as e:\\n                    if not suppress_errors:\\n                        print(e, word, second_word)\\n\\n    return word_comparisons, weights\\n\\n\\n# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\\ndef coherence_tester(\\n    text_data,\\n    text_labels,\\n    max_tokens=400,\\n    max_str_length=30,\\n    prediction_thresh=0.7,\\n    dynamic_threshold=True,\\n    threshold_warmup=10,  # number of iterations before using dynamic threshold\\n    last_n_threshold=5,  # will only consider the last n thresholds for dynamic threshold\\n):\\n    coherence_map = []\\n    predictions = []\\n    thresholds = []\\n    for i, (row, label) in enumerate(zip(text_data, text_labels)):\\n        threshold = prediction_thresh\\n        if dynamic_threshold and (i + 1) > threshold_warmup:\\n            last_n_thresholds = thresholds[(0 - last_n_threshold) :]\\n            last_n_thresholds.sort()\\n            mid = len(last_n_thresholds) // 2\\n            threshold = (last_n_thresholds[mid] + last_n_thresholds[~mid]) / 2\\n            print(f\\\"median threshold: {threshold}\\\")\\n        # compare the current sentence to the previous one\\n        if i == 0:\\n            predictions.append((0, 0))\\n        else:\\n            prev_row = text_data[i - 1]\\n\\n            row = truncate_by_token(row, max_tokens)\\n            prev_row = truncate_by_token(prev_row, max_tokens)\\n\\n            cohesion, keywords_prev, keywords_current = coherence.get_coherence(\\n                [row, prev_row], coherence_threshold=0.2\\n            )\\n\\n            pruning = 1\\n            pruning_min = 6\\n\\n            # add the keywords to the coherence map\\n            coherence_map.append(cohesion)\\n            if pruning > 0 and len(coherence_map) >= pruning_min:\\n                print(\\\"pruning...\\\", len(coherence_map))\\n                coherence_map = coherence_map[::-1][pruning:][\\n                    ::-1\\n                ]  # get the last n - pruning values and reverse the list\\n                print(\\\"done pruning...\\\", len(coherence_map))\\n\\n            # truncate the strings for printing\\n            truncated_row = truncate_string(row, max_str_length)\\n            truncated_prev_row = truncate_string(prev_row, max_str_length)\\n            print(\\n                f\\\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\\\"\\n            )\\n\\n            # compute the word comparisons between the previous (with the coherence map)\\n            # and the current (possibly the first sentence in a new segment)\\n            word_comparisons_with_coherence, weights = compare_coherent_words(\\n                [*coherence_map, keywords_prev], keywords_current\\n            )\\n\\n            similarities_with_coherence = [\\n                comparison[2] for comparison in word_comparisons_with_coherence\\n            ]\\n            avg_similarity_with_coherence = sum(similarities_with_coherence) / (\\n                len(similarities_with_coherence) or 1\\n            )\\n            weighted_avg_similarity_with_coherence = get_weighted_average(\\n                similarities_with_coherence, weights\\n            )\\n            print(f\\\"weighted: {weighted_avg_similarity_with_coherence}\\\")\\n\\n            # if the two sentences are similar, create a cohesive prediction\\n            # otherwise, predict a new segment\\n            if weighted_avg_similarity_with_coherence > threshold:\\n                print(\\n                    f\\\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 0))\\n            else:\\n                # start of a new segment, empty the map\\n                coherence_map = []\\n                print(\\n                    f\\\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 1))\\n\\n            thresholds.append(weighted_avg_similarity_with_coherence)\\n            print(\\\"===============================================\\\")\\n\\n    return predictions\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pruning = 0  # remove the lowest n important words from coherence map\n",
    "pruning_min = 10  # only prune after n words in the coherence map\n",
    "\n",
    "\n",
    "def get_weighted_average(weighted_similarities, weights):\n",
    "    return sum(weighted_similarities) / sum(weights)\n",
    "\n",
    "\n",
    "# importance testing\n",
    "def compare_coherent_words(\n",
    "    coherence_map, keywords_current, suppress_errors=False, same_word_multiplier=True\n",
    "):\n",
    "    word_comparisons = []\n",
    "    weights = []\n",
    "    for i, keywords in enumerate(coherence_map[::-1]):\n",
    "        for word_tuple in keywords:\n",
    "            word = word_tuple[0]\n",
    "            for second_word_tuple in keywords_current:\n",
    "                second_word = second_word_tuple[0]\n",
    "\n",
    "                try:\n",
    "                    word_one_emb = word_tuple[2]\n",
    "                    word_two_emb = second_word_tuple[2]\n",
    "\n",
    "                    if same_word_multiplier:\n",
    "                        flattened_coherence_words_only = [\n",
    "                            element[0]\n",
    "                            for sublist in coherence_map\n",
    "                            for element in sublist\n",
    "                        ]\n",
    "\n",
    "                        num_occurrences = flattened_coherence_words_only.count(\n",
    "                            second_word\n",
    "                        )\n",
    "\n",
    "                        if num_occurrences > 0:\n",
    "                            # amplify words that are found as duplicates in the coherence map\n",
    "                            # if the word shows up 1 time, amplify the weight by 2 times\n",
    "                            weighting_multiplier = (\n",
    "                                flattened_coherence_words_only.count(second_word) + 2\n",
    "                            )\n",
    "                        else:\n",
    "                            weighting_multiplier = (\n",
    "                                1 / 3\n",
    "                            )  # reduce the importance of this word\n",
    "\n",
    "                    else:\n",
    "                        weighting_multiplier = 1  # set to 1 in case this is turned off.\n",
    "\n",
    "                    # this weight is a recipricol function that will grow smaller the further the keywords are away\n",
    "                    # we want to put more importance on the current words, so we apply twice as much weight.\n",
    "                    if i == 0:\n",
    "                        weight = (weighting_multiplier * 2) / (i + 1)\n",
    "                    else:\n",
    "                        weight = (weighting_multiplier * 1) / (i + 1)\n",
    "\n",
    "                    word_comparisons.append(\n",
    "                        (\n",
    "                            word,\n",
    "                            second_word,\n",
    "                            weight\n",
    "                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\n",
    "                        )\n",
    "                    )\n",
    "                    weights.append(weight)\n",
    "                except AssertionError as e:\n",
    "                    if not suppress_errors:\n",
    "                        print(e, word, second_word)\n",
    "\n",
    "    return word_comparisons, weights\n",
    "\n",
    "\n",
    "# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\n",
    "def coherence_tester(\n",
    "    text_data,\n",
    "    text_labels,\n",
    "    max_tokens=400,\n",
    "    max_str_length=30,\n",
    "    prediction_thresh=0.7,\n",
    "    dynamic_threshold=True,\n",
    "    threshold_warmup=10,  # number of iterations before using dynamic threshold\n",
    "    last_n_threshold=5,  # will only consider the last n thresholds for dynamic threshold\n",
    "):\n",
    "    coherence_map = []\n",
    "    predictions = []\n",
    "    thresholds = []\n",
    "    for i, (row, label) in enumerate(zip(text_data, text_labels)):\n",
    "        threshold = prediction_thresh\n",
    "        if dynamic_threshold and (i + 1) > threshold_warmup:\n",
    "            last_n_thresholds = thresholds[(0 - last_n_threshold) :]\n",
    "            last_n_thresholds.sort()\n",
    "            mid = len(last_n_thresholds) // 2\n",
    "            threshold = (last_n_thresholds[mid] + last_n_thresholds[~mid]) / 2\n",
    "            print(f\"median threshold: {threshold}\")\n",
    "        # compare the current sentence to the previous one\n",
    "        if i == 0:\n",
    "            predictions.append((0, 0))\n",
    "        else:\n",
    "            prev_row = text_data[i - 1]\n",
    "\n",
    "            row = truncate_by_token(row, max_tokens)\n",
    "            prev_row = truncate_by_token(prev_row, max_tokens)\n",
    "\n",
    "            cohesion, keywords_prev, keywords_current = coherence.get_coherence(\n",
    "                [row, prev_row], coherence_threshold=0.2\n",
    "            )\n",
    "\n",
    "            pruning = 1\n",
    "            pruning_min = 6\n",
    "\n",
    "            # add the keywords to the coherence map\n",
    "            coherence_map.append(cohesion)\n",
    "            if pruning > 0 and len(coherence_map) >= pruning_min:\n",
    "                print(\"pruning...\", len(coherence_map))\n",
    "                coherence_map = coherence_map[::-1][pruning:][\n",
    "                    ::-1\n",
    "                ]  # get the last n - pruning values and reverse the list\n",
    "                print(\"done pruning...\", len(coherence_map))\n",
    "\n",
    "            # truncate the strings for printing\n",
    "            truncated_row = truncate_string(row, max_str_length)\n",
    "            truncated_prev_row = truncate_string(prev_row, max_str_length)\n",
    "            print(\n",
    "                f\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\"\n",
    "            )\n",
    "\n",
    "            # compute the word comparisons between the previous (with the coherence map)\n",
    "            # and the current (possibly the first sentence in a new segment)\n",
    "            word_comparisons_with_coherence, weights = compare_coherent_words(\n",
    "                [*coherence_map, keywords_prev], keywords_current\n",
    "            )\n",
    "\n",
    "            similarities_with_coherence = [\n",
    "                comparison[2] for comparison in word_comparisons_with_coherence\n",
    "            ]\n",
    "            avg_similarity_with_coherence = sum(similarities_with_coherence) / (\n",
    "                len(similarities_with_coherence) or 1\n",
    "            )\n",
    "            weighted_avg_similarity_with_coherence = get_weighted_average(\n",
    "                similarities_with_coherence, weights\n",
    "            )\n",
    "            print(f\"weighted: {weighted_avg_similarity_with_coherence}\")\n",
    "\n",
    "            # if the two sentences are similar, create a cohesive prediction\n",
    "            # otherwise, predict a new segment\n",
    "            if weighted_avg_similarity_with_coherence > threshold:\n",
    "                print(\n",
    "                    f\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\"\n",
    "                )\n",
    "                predictions.append((weighted_avg_similarity_with_coherence, 0))\n",
    "            else:\n",
    "                # start of a new segment, empty the map\n",
    "                coherence_map = []\n",
    "                print(\n",
    "                    f\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\"\n",
    "                )\n",
    "                predictions.append((weighted_avg_similarity_with_coherence, 1))\n",
    "\n",
    "            thresholds.append(weighted_avg_similarity_with_coherence)\n",
    "            print(\"===============================================\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ca71b4cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hydrotherapy', 'hydrothermal', 'village', 'mineral', 'sources']\n",
      "['bagarov', 'rheumatism', 'summerhouse', 'verdure', 'banya']\n",
      "Got the keywords in 0.2709 seconds\n",
      "Got the embeddings and comparisons in 0.0085 seconds\n",
      "Coherence Map: [['bagarov', 'hydrotherapy', 'rheumatism', 'hydrothermal']], KW Curr: ['hydrotherapy', 'hydrothermal', 'village', 'mineral', 'sources']\n",
      "weighted: tensor([0.6876])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.6876])\n",
      "===============================================\n",
      "['bagarov', 'rheumatism', 'summerhouse', 'verdure', 'banya']\n",
      "['massachusetts', 'railroad', 'harvard', '1871', 'university']\n",
      "Got the keywords in 0.6209 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['massachusetts', 'bagarov', 'railroad', 'rheumatism']], KW Curr: ['bagarov', 'rheumatism', 'summerhouse', 'verdure', 'banya']\n",
      "weighted: tensor([0.7757])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.7757])\n",
      "===============================================\n",
      "['massachusetts', 'railroad', 'harvard', '1871', 'university']\n",
      "['620276', '096554', 'census', 'city', 'land']\n",
      "Got the keywords in 0.1593 seconds\n",
      "Got the embeddings and comparisons in 0.0002 seconds\n",
      "Coherence Map: [['massachusetts', 'bagarov', 'railroad', 'rheumatism'], ['census', 'massachusetts', 'city', 'railroad']], KW Curr: ['massachusetts', 'railroad', 'harvard', '1871', 'university']\n",
      "weighted: tensor([0.6945])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.6945])\n",
      "===============================================\n",
      "['620276', '096554', 'census', 'city', 'land']\n",
      "['372', '453', 'male', 'census', 'female']\n",
      "Got the keywords in 0.4672 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['male', 'census', 'census', 'city']], KW Curr: ['620276', '096554', 'census', 'city', 'land']\n",
      "weighted: tensor([0.6748])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.6748])\n",
      "===============================================\n",
      "['372', '453', 'male', 'census', 'female']\n",
      "['602', '667', '558', '702', 'males']\n",
      "Got the keywords in 0.9761 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['males', 'male']], KW Curr: ['372', '453', 'male', 'census', 'female']\n",
      "weighted: tensor([0.8449])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.8449])\n",
      "===============================================\n",
      "['602', '667', '558', '702', 'males']\n",
      "['lomaiviti', 'muanikau', 'sprawl', 'cakobau', 'samabula']\n",
      "Got the keywords in 1.1858 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['males', 'male'], ['lomaiviti', 'males']], KW Curr: ['602', '667', '558', '702', 'males']\n",
      "weighted: tensor([0.7972])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.7972])\n",
      "===============================================\n",
      "['lomaiviti', 'muanikau', 'sprawl', 'cakobau', 'samabula']\n",
      "['mangroves', 'laucala', 'levu', 'viti', 'buildings']\n",
      "Got the keywords in 1.0621 seconds\n",
      "Got the embeddings and comparisons in 0.0008 seconds\n",
      "Coherence Map: [['males', 'male'], ['lomaiviti', 'males'], ['mangroves', 'lomaiviti', 'laucala', 'muanikau']], KW Curr: ['lomaiviti', 'muanikau', 'sprawl', 'cakobau', 'samabula']\n",
      "weighted: tensor([0.7727])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7727])\n",
      "===============================================\n",
      "['mangroves', 'laucala', 'levu', 'viti', 'buildings']\n",
      "['suva', 'area', 'western', 'wards', 'ward']\n",
      "Got the keywords in 0.6954 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['males', 'male'], ['lomaiviti', 'males'], ['mangroves', 'lomaiviti', 'laucala', 'muanikau'], ['suva', 'mangroves', 'area', 'laucala']], KW Curr: ['mangroves', 'laucala', 'levu', 'viti', 'buildings']\n",
      "weighted: tensor([0.7838])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7838])\n",
      "===============================================\n",
      "['suva', 'area', 'western', 'wards', 'ward']\n",
      "['muanikau', 'nabua', 'samabula', 'tamavua', 'cbd']\n",
      "Got the keywords in 0.3390 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "Coherence Map: [['males', 'male'], ['lomaiviti', 'males'], ['mangroves', 'lomaiviti', 'laucala', 'muanikau'], ['suva', 'mangroves', 'area', 'laucala'], ['muanikau', 'suva', 'nabua', 'area']], KW Curr: ['suva', 'area', 'western', 'wards', 'ward']\n",
      "weighted: tensor([0.8279])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.8279])\n",
      "===============================================\n",
      "median threshold: tensor([0.7972])\n",
      "['muanikau', 'nabua', 'samabula', 'tamavua', 'cbd']\n",
      "['nausori', 'nasinu', 'conurbation', 'lami', 'sawani']\n",
      "Got the keywords in 0.8249 seconds\n",
      "Got the embeddings and comparisons in 0.0009 seconds\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['males', 'male'], ['lomaiviti', 'males'], ['mangroves', 'lomaiviti', 'laucala', 'muanikau'], ['suva', 'mangroves', 'area', 'laucala'], ['muanikau', 'suva', 'nabua', 'area']], KW Curr: ['muanikau', 'nabua', 'samabula', 'tamavua', 'cbd']\n",
      "weighted: tensor([0.8422])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.8422])\n",
      "===============================================\n",
      "median threshold: tensor([0.7972])\n",
      "['nausori', 'nasinu', 'conurbation', 'lami', 'sawani']\n",
      "['precipitation', 'copious', 'drier', 'weather', 'averages']\n",
      "Got the keywords in 0.9108 seconds\n",
      "Got the embeddings and comparisons in 0.0008 seconds\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['males', 'male'], ['lomaiviti', 'males'], ['mangroves', 'lomaiviti', 'laucala', 'muanikau'], ['suva', 'mangroves', 'area', 'laucala'], ['muanikau', 'suva', 'nabua', 'area']], KW Curr: ['nausori', 'nasinu', 'conurbation', 'lami', 'sawani']\n",
      "weighted: tensor([0.7767])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.7767])\n",
      "===============================================\n",
      "median threshold: tensor([0.7838])\n",
      "['precipitation', 'copious', 'drier', 'weather', 'averages']\n",
      "['fijians', 'kailoma', 'rotumans', 'fijian', 'hindustani']\n",
      "Got the keywords in 0.9225 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "Coherence Map: [['fijians', 'precipitation', 'kailoma', 'copious']], KW Curr: ['precipitation', 'copious', 'drier', 'weather', 'averages']\n",
      "weighted: tensor([0.7663])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.7663])\n",
      "===============================================\n",
      "median threshold: tensor([0.7838])\n",
      "['fijians', 'kailoma', 'rotumans', 'fijian', 'hindustani']\n",
      "['nasinu', 'mayor', 'governments', 'chandu', 'interim']\n",
      "Got the keywords in 0.8606 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['nasinu', 'fijians', 'mayor', 'kailoma']], KW Curr: ['fijians', 'kailoma', 'rotumans', 'fijian', 'hindustani']\n",
      "weighted: tensor([0.7945])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7945])\n",
      "===============================================\n",
      "median threshold: tensor([0.7945])\n",
      "['nasinu', 'mayor', 'governments', 'chandu', 'interim']\n",
      "['sukuna', 'pilings', 'thurston', 'fijian', 'ratu']\n",
      "Got the keywords in 1.0411 seconds\n",
      "Got the embeddings and comparisons in 0.0019 seconds\n",
      "Coherence Map: [['nasinu', 'fijians', 'mayor', 'kailoma'], ['sukuna', 'nasinu', 'pilings', 'mayor']], KW Curr: ['nasinu', 'mayor', 'governments', 'chandu', 'interim']\n",
      "weighted: tensor([0.8093])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.8093])\n",
      "===============================================\n",
      "median threshold: tensor([0.7945])\n",
      "['sukuna', 'pilings', 'thurston', 'fijian', 'ratu']\n",
      "['fijians', 'raiwaqa', 'vatuwaqa', 'printeries', 'shipyards']\n",
      "Got the keywords in 1.2254 seconds\n",
      "Got the embeddings and comparisons in 0.0008 seconds\n",
      "Coherence Map: [['nasinu', 'fijians', 'mayor', 'kailoma'], ['sukuna', 'nasinu', 'pilings', 'mayor'], ['fijians', 'sukuna', 'raiwaqa', 'pilings']], KW Curr: ['sukuna', 'pilings', 'thurston', 'fijian', 'ratu']\n",
      "weighted: tensor([0.7730])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.7730])\n",
      "===============================================\n",
      "median threshold: tensor([0.7767])\n",
      "['fijians', 'raiwaqa', 'vatuwaqa', 'printeries', 'shipyards']\n",
      "['tuvalu', 'kiribati', 'nauru', 'micronesia', 'geoscience']\n",
      "Got the keywords in 1.2083 seconds\n",
      "Got the embeddings and comparisons in 0.0012 seconds\n",
      "Coherence Map: [['tuvalu', 'fijians', 'kiribati', 'raiwaqa']], KW Curr: ['fijians', 'raiwaqa', 'vatuwaqa', 'printeries', 'shipyards']\n",
      "weighted: tensor([0.8080])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.8080])\n",
      "===============================================\n",
      "median threshold: tensor([0.7945])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tuvalu', 'kiribati', 'nauru', 'micronesia', 'geoscience']\n",
      "['infrastructure', 'events', 'suva', 'capital', 'event']\n",
      "Got the keywords in 0.7894 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['tuvalu', 'fijians', 'kiribati', 'raiwaqa'], ['infrastructure', 'tuvalu', 'events', 'kiribati']], KW Curr: ['tuvalu', 'kiribati', 'nauru', 'micronesia', 'geoscience']\n",
      "weighted: tensor([0.8028])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.8028])\n",
      "===============================================\n",
      "median threshold: tensor([0.8028])\n",
      "['infrastructure', 'events', 'suva', 'capital', 'event']\n",
      "['grandstand', 'vodafone', 'multipurpose', 'arena', 'gymnasium']\n",
      "Got the keywords in 0.7337 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['tuvalu', 'fijians', 'kiribati', 'raiwaqa'], ['infrastructure', 'tuvalu', 'events', 'kiribati'], ['grandstand', 'infrastructure', 'vodafone', 'events']], KW Curr: ['infrastructure', 'events', 'suva', 'capital', 'event']\n",
      "weighted: tensor([0.7546])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.7546])\n",
      "===============================================\n",
      "median threshold: tensor([0.8028])\n",
      "['grandstand', 'vodafone', 'multipurpose', 'arena', 'gymnasium']\n",
      "['kingsford', 'sukuna', 'thurston', 'foreshore', 'carnivals']\n",
      "Got the keywords in 0.8132 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['kingsford', 'grandstand', 'sukuna', 'vodafone']], KW Curr: ['grandstand', 'vodafone', 'multipurpose', 'arena', 'gymnasium']\n",
      "weighted: tensor([0.7261])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.7261])\n",
      "===============================================\n",
      "median threshold: tensor([0.7730])\n",
      "['kingsford', 'sukuna', 'thurston', 'foreshore', 'carnivals']\n",
      "['chauhan', 'sunidhi', 'nigam', 'sonu', 'priyanka']\n",
      "Got the keywords in 0.8873 seconds\n",
      "Got the embeddings and comparisons in 0.0100 seconds\n",
      "Coherence Map: [['chauhan', 'kingsford', 'sunidhi', 'sukuna']], KW Curr: ['kingsford', 'sukuna', 'thurston', 'foreshore', 'carnivals']\n",
      "weighted: tensor([0.7560])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.7560])\n",
      "===============================================\n",
      "median threshold: tensor([0.7560])\n",
      "['chauhan', 'sunidhi', 'nigam', 'sonu', 'priyanka']\n",
      "['fijians', 'indentured', 'cuisines', 'chilies', 'fijian']\n",
      "Got the keywords in 0.7877 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['fijians', 'chauhan', 'indentured', 'sunidhi']], KW Curr: ['chauhan', 'sunidhi', 'nigam', 'sonu', 'priyanka']\n",
      "weighted: tensor([0.7368])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.7368])\n",
      "===============================================\n",
      "median threshold: tensor([0.7546])\n",
      "['fijians', 'indentured', 'cuisines', 'chilies', 'fijian']\n",
      "['tradeshow', 'hibiscus', 'festivals', 'carnival', 'years']\n",
      "Got the keywords in 0.7534 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['tradeshow', 'fijians', 'hibiscus', 'indentured']], KW Curr: ['fijians', 'indentured', 'cuisines', 'chilies', 'fijian']\n",
      "weighted: tensor([0.7397])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.7397])\n",
      "===============================================\n",
      "median threshold: tensor([0.7397])\n",
      "['tradeshow', 'hibiscus', 'festivals', 'carnival', 'years']\n",
      "['nightlife', 'caters', 'lounges', 'nightclubs', 'policed']\n",
      "Got the keywords in 0.7398 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "Coherence Map: [['nightlife', 'tradeshow', 'caters', 'hibiscus']], KW Curr: ['tradeshow', 'hibiscus', 'festivals', 'carnival', 'years']\n",
      "weighted: tensor([0.7872])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7872])\n",
      "===============================================\n",
      "median threshold: tensor([0.7397])\n",
      "['nightlife', 'caters', 'lounges', 'nightclubs', 'policed']\n",
      "['haunts', 'sharan', 'laucala', 'damodar', 'theatres']\n",
      "Got the keywords in 0.7762 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "Coherence Map: [['nightlife', 'tradeshow', 'caters', 'hibiscus'], ['haunts', 'nightlife', 'sharan', 'caters']], KW Curr: ['nightlife', 'caters', 'lounges', 'nightclubs', 'policed']\n",
      "weighted: tensor([0.8080])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.8080])\n",
      "===============================================\n",
      "median threshold: tensor([0.7560])\n",
      "['haunts', 'sharan', 'laucala', 'damodar', 'theatres']\n",
      "['events', 'years', 'athletics', 'capital', 'rugby']\n",
      "Got the keywords in 0.7383 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "Coherence Map: [['nightlife', 'tradeshow', 'caters', 'hibiscus'], ['haunts', 'nightlife', 'sharan', 'caters'], ['events', 'haunts', 'years', 'sharan']], KW Curr: ['haunts', 'sharan', 'laucala', 'damodar', 'theatres']\n",
      "weighted: tensor([0.7933])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7933])\n",
      "===============================================\n",
      "median threshold: tensor([0.7872])\n",
      "['events', 'years', 'athletics', 'capital', 'rugby']\n",
      "['itaukei', 'lalakai', 'shanti', 'newspapers', 'magazines']\n",
      "Got the keywords in 0.8794 seconds\n",
      "Got the embeddings and comparisons in 0.0105 seconds\n",
      "Coherence Map: [['nightlife', 'tradeshow', 'caters', 'hibiscus'], ['haunts', 'nightlife', 'sharan', 'caters'], ['events', 'haunts', 'years', 'sharan'], ['itaukei', 'events', 'lalakai', 'years']], KW Curr: ['events', 'years', 'athletics', 'capital', 'rugby']\n",
      "weighted: tensor([0.7794])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.7794])\n",
      "===============================================\n",
      "median threshold: tensor([0.7872])\n",
      "['itaukei', 'lalakai', 'shanti', 'newspapers', 'magazines']\n",
      "['handicrafts', 'extravaganza', 'denarau', 'supermodel', 'shopper']\n",
      "Got the keywords in 1.1307 seconds\n",
      "Got the embeddings and comparisons in 0.0008 seconds\n",
      "Coherence Map: [['handicrafts', 'itaukei', 'extravaganza', 'lalakai']], KW Curr: ['itaukei', 'lalakai', 'shanti', 'newspapers', 'magazines']\n",
      "weighted: tensor([0.7630])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.7630])\n",
      "===============================================\n",
      "median threshold: tensor([0.7872])\n",
      "['handicrafts', 'extravaganza', 'denarau', 'supermodel', 'shopper']\n",
      "['tuvalu', 'nausori', 'nasinu', 'levu', 'vanua']\n",
      "Got the keywords in 1.1716 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "Coherence Map: [['tuvalu', 'handicrafts', 'nausori', 'extravaganza']], KW Curr: ['handicrafts', 'extravaganza', 'denarau', 'supermodel', 'shopper']\n",
      "weighted: tensor([0.8246])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.8246])\n",
      "===============================================\n",
      "median threshold: tensor([0.7933])\n",
      "['tuvalu', 'nausori', 'nasinu', 'levu', 'vanua']\n",
      "['koroibulu', 'tuiloa', 'yalimaiwai', 'civoniceva', 'josua']\n",
      "Got the keywords in 1.0702 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "Coherence Map: [['tuvalu', 'handicrafts', 'nausori', 'extravaganza'], ['koroibulu', 'tuvalu', 'tuiloa', 'nausori']], KW Curr: ['tuvalu', 'nausori', 'nasinu', 'levu', 'vanua']\n",
      "weighted: tensor([0.7340])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.7340])\n",
      "===============================================\n",
      "median threshold: tensor([0.7794])\n",
      "['koroibulu', 'tuiloa', 'yalimaiwai', 'civoniceva', 'josua']\n",
      "['habikino', 'kashiwara', 'yamato', 'fujidera', 'prefectural']\n",
      "Got the keywords in 0.9037 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "Coherence Map: [['habikino', 'koroibulu', 'kashiwara', 'tuiloa']], KW Curr: ['koroibulu', 'tuiloa', 'yalimaiwai', 'civoniceva', 'josua']\n",
      "weighted: tensor([0.7419])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.7419])\n",
      "===============================================\n",
      "median threshold: tensor([0.7630])\n",
      "['habikino', 'kashiwara', 'yamato', 'fujidera', 'prefectural']\n",
      "['kashiwara', 'katakami', 'katashimo', 'kokubu', 'kawachi']\n",
      "Got the keywords in 0.7110 seconds\n",
      "Got the embeddings and comparisons in 0.0018 seconds\n",
      "Coherence Map: [['kashiwara', 'habikino', 'katakami', 'kashiwara']], KW Curr: ['habikino', 'kashiwara', 'yamato', 'fujidera', 'prefectural']\n",
      "weighted: tensor([0.7641])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7641])\n",
      "===============================================\n",
      "median threshold: tensor([0.7630])\n",
      "['kashiwara', 'katakami', 'katashimo', 'kokubu', 'kawachi']\n",
      "['529064', '111021', 'census', 'city', 'states']\n",
      "Got the keywords in 0.6634 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['kashiwara', 'habikino', 'katakami', 'kashiwara'], ['census', 'kashiwara', 'city', 'katakami']], KW Curr: ['kashiwara', 'katakami', 'katashimo', 'kokubu', 'kawachi']\n",
      "weighted: tensor([0.6904])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.6904])\n",
      "===============================================\n",
      "median threshold: tensor([0.7419])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['529064', '111021', 'census', 'city', 'states']\n",
      "['marvinville', 'confusion', 'story', 'changed', 'happened']\n",
      "Got the keywords in 0.2642 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['marvinville', 'census', 'confusion', 'city']], KW Curr: ['529064', '111021', 'census', 'city', 'states']\n",
      "weighted: tensor([0.6217])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.6217])\n",
      "===============================================\n",
      "median threshold: tensor([0.7340])\n",
      "['marvinville', 'confusion', 'story', 'changed', 'happened']\n",
      "['392', '822', '558', 'males', 'census']\n",
      "Got the keywords in 0.8141 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['males', 'marvinville', 'census', 'confusion']], KW Curr: ['marvinville', 'confusion', 'story', 'changed', 'happened']\n",
      "weighted: tensor([0.7599])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7599])\n",
      "===============================================\n",
      "median threshold: tensor([0.7419])\n",
      "['392', '822', '558', 'males', 'census']\n",
      "['alvarado', 'city', '1905', 'called', 'post']\n",
      "Got the keywords in 0.7838 seconds\n",
      "Got the embeddings and comparisons in 0.0002 seconds\n",
      "Coherence Map: [['males', 'marvinville', 'census', 'confusion'], ['alvarado', 'males', 'city', 'census']], KW Curr: ['392', '822', '558', 'males', 'census']\n",
      "weighted: tensor([0.7136])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.7136])\n",
      "===============================================\n",
      "median threshold: tensor([0.7136])\n",
      "['alvarado', 'city', '1905', 'called', 'post']\n",
      "['census', 'land', 'city', 'states', 'total']\n",
      "Got the keywords in 0.1545 seconds\n",
      "Got the embeddings and comparisons in 0.0002 seconds\n",
      "Coherence Map: [['census', 'alvarado', 'land', 'city']], KW Curr: ['alvarado', 'city', '1905', 'called', 'post']\n",
      "weighted: tensor([0.6166])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.6166])\n",
      "===============================================\n",
      "median threshold: tensor([0.6904])\n",
      "['census', 'land', 'city', 'states', 'total']\n",
      "['363', 'male', 'census', 'female', 'population']\n",
      "Got the keywords in 0.3348 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "Coherence Map: [['male', 'census', 'census', 'land']], KW Curr: ['census', 'land', 'city', 'states', 'total']\n",
      "weighted: tensor([0.7422])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7422])\n",
      "===============================================\n",
      "median threshold: tensor([0.7136])\n",
      "['363', 'male', 'census', 'female', 'population']\n",
      "['682', '688', '371', '838', '738']\n",
      "Got the keywords in 0.9689 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['male', 'census', 'census', 'land'], []], KW Curr: ['363', 'male', 'census', 'female', 'population']\n",
      "weighted: tensor([0.8251])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.8251])\n",
      "===============================================\n",
      "median threshold: tensor([0.7422])\n",
      "['682', '688', '371', '838', '738']\n",
      "['village', 'platted', '1890', '1899', '1901']\n",
      "Got the keywords in 0.7443 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['male', 'census', 'census', 'land'], [], []], KW Curr: ['682', '688', '371', '838', '738']\n",
      "weighted: tensor([0.7398])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.7398])\n",
      "===============================================\n",
      "median threshold: tensor([0.7398])\n",
      "['village', 'platted', '1890', '1899', '1901']\n",
      "['census', 'land', 'city', 'states', 'total']\n",
      "Got the keywords in 0.1630 seconds\n",
      "Got the embeddings and comparisons in 0.0001 seconds\n",
      "Coherence Map: [['census', 'village', 'land', 'platted']], KW Curr: ['village', 'platted', '1890', '1899', '1901']\n",
      "weighted: tensor([0.7492])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7492])\n",
      "===============================================\n",
      "median threshold: tensor([0.7422])\n",
      "['census', 'land', 'city', 'states', 'total']\n",
      "['male', 'census', 'female', 'population', '18']\n",
      "Got the keywords in 0.3271 seconds\n",
      "Got the embeddings and comparisons in 0.0010 seconds\n",
      "Coherence Map: [['census', 'village', 'land', 'platted'], ['male', 'census', 'census', 'land']], KW Curr: ['census', 'land', 'city', 'states', 'total']\n",
      "weighted: tensor([0.7625])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7625])\n",
      "===============================================\n",
      "median threshold: tensor([0.7492])\n",
      "['male', 'census', 'female', 'population', '18']\n",
      "['856', '611', '443', '803', '821']\n",
      "Got the keywords in 1.0266 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['census', 'village', 'land', 'platted'], ['male', 'census', 'census', 'land'], []], KW Curr: ['male', 'census', 'female', 'population', '18']\n",
      "weighted: tensor([0.8436])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.8436])\n",
      "===============================================\n",
      "median threshold: tensor([0.7625])\n",
      "['856', '611', '443', '803', '821']\n",
      "['575896', '718051', 'census', 'sioux', 'washta']\n",
      "Got the keywords in 0.8281 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['census', 'village', 'land', 'platted'], ['male', 'census', 'census', 'land'], [], []], KW Curr: ['856', '611', '443', '803', '821']\n",
      "weighted: tensor([0.7099])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.7099])\n",
      "===============================================\n",
      "median threshold: tensor([0.7492])\n",
      "['575896', '718051', 'census', 'sioux', 'washta']\n",
      "['male', 'census', 'female', 'population', '19']\n",
      "Got the keywords in 0.7565 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['male', 'census', 'census', 'sioux']], KW Curr: ['575896', '718051', 'census', 'sioux', 'washta']\n",
      "weighted: tensor([0.6767])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.6767])\n",
      "===============================================\n",
      "median threshold: tensor([0.7492])\n",
      "['male', 'census', 'female', 'population', '19']\n",
      "['673', '455', '639', '025', 'males']\n",
      "Got the keywords in 0.9649 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['males', 'male']], KW Curr: ['male', 'census', 'female', 'population', '19']\n",
      "weighted: tensor([0.8347])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.8347])\n",
      "===============================================\n",
      "median threshold: tensor([0.7625])\n",
      "['673', '455', '639', '025', 'males']\n",
      "['ardmoreite', 'texan', 'stonewall', 'freemason', 'bloodiest']\n",
      "Got the keywords in 1.1138 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['males', 'male'], ['ardmoreite', 'males']], KW Curr: ['673', '455', '639', '025', 'males']\n",
      "weighted: tensor([0.7925])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.7925])\n",
      "===============================================\n",
      "median threshold: tensor([0.7925])\n",
      "['ardmoreite', 'texan', 'stonewall', 'freemason', 'bloodiest']\n",
      "['pontotoc', 'courthouse', 'wintersmith', 'mijo', 'sugg']\n",
      "Got the keywords in 0.9616 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['males', 'male'], ['ardmoreite', 'males'], ['pontotoc', 'ardmoreite', 'courthouse', 'texan']], KW Curr: ['ardmoreite', 'texan', 'stonewall', 'freemason', 'bloodiest']\n",
      "weighted: tensor([0.7675])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.7675])\n",
      "===============================================\n",
      "median threshold: tensor([0.7675])\n",
      "['pontotoc', 'courthouse', 'wintersmith', 'mijo', 'sugg']\n",
      "['texas', 'tulsa', 'oklahoma', 'census', 'city']\n",
      "Got the keywords in 0.6041 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "Coherence Map: [['texas', 'pontotoc', 'tulsa', 'courthouse']], KW Curr: ['pontotoc', 'courthouse', 'wintersmith', 'mijo', 'sugg']\n",
      "weighted: tensor([0.7978])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7978])\n",
      "===============================================\n",
      "median threshold: tensor([0.7925])\n",
      "['texas', 'tulsa', 'oklahoma', 'census', 'city']\n",
      "['862', 'disparity', '803', '805', '977']\n",
      "Got the keywords in 0.8385 seconds\n",
      "Got the embeddings and comparisons in 0.0001 seconds\n",
      "Coherence Map: [['texas', 'pontotoc', 'tulsa', 'courthouse'], ['disparity', 'texas']], KW Curr: ['texas', 'tulsa', 'oklahoma', 'census', 'city']\n",
      "weighted: tensor([0.7969])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7969])\n",
      "===============================================\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 172;\n",
       "                var nbb_unformatted_code = \"start = 50\\nnum_samples = 50\\nmax_tokens = 256  # want to keep this under 512\\nmax_str_length = 30\\n\\ntrue_labels = text_labels[start : start + num_samples]\\n\\npredictions = coherence_tester(\\n    text_data[start : start + num_samples],\\n    true_labels,\\n    max_tokens=max_tokens,\\n    max_str_length=max_str_length,\\n)\";\n",
       "                var nbb_formatted_code = \"start = 50\\nnum_samples = 50\\nmax_tokens = 256  # want to keep this under 512\\nmax_str_length = 30\\n\\ntrue_labels = text_labels[start : start + num_samples]\\n\\npredictions = coherence_tester(\\n    text_data[start : start + num_samples],\\n    true_labels,\\n    max_tokens=max_tokens,\\n    max_str_length=max_str_length,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = 50\n",
    "num_samples = 50\n",
    "max_tokens = 256  # want to keep this under 512\n",
    "max_str_length = 30\n",
    "\n",
    "true_labels = text_labels[start : start + num_samples]\n",
    "\n",
    "predictions = coherence_tester(\n",
    "    text_data[start : start + num_samples],\n",
    "    true_labels,\n",
    "    max_tokens=max_tokens,\n",
    "    max_str_length=max_str_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d3a93727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 173;\n",
       "                var nbb_unformatted_code = \"print([x[1] for x in predictions])\\nprint(true_labels)\";\n",
       "                var nbb_formatted_code = \"print([x[1] for x in predictions])\\nprint(true_labels)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print([x[1] for x in predictions])\n",
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "61e7863f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 174;\n",
       "                var nbb_unformatted_code = \"pred_string = \\\"\\\".join(str([x[1] for x in predictions]))\\ntrue_string = \\\"\\\".join(str(true_labels))\";\n",
       "                var nbb_formatted_code = \"pred_string = \\\"\\\".join(str([x[1] for x in predictions]))\\ntrue_string = \\\"\\\".join(str(true_labels))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_string = \"\".join(str([x[1] for x in predictions]))\n",
    "true_string = \"\".join(str(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "19af16ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 175;\n",
       "                var nbb_unformatted_code = \"avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\";\n",
       "                var nbb_formatted_code = \"avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "db43c420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 5\n",
      "wd = 0.541095890410959\n",
      "pk = 0.4863013698630137\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 176;\n",
       "                var nbb_unformatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_formatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wd_score = windowdiff(pred_string, true_string, avg_k)\n",
    "pk_score = pk(pred_string, true_string, avg_k)\n",
    "\n",
    "print(f\"k = {avg_k}\")\n",
    "print(f\"wd = {wd_score}\")\n",
    "print(f\"pk = {pk_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb16194",
   "metadata": {},
   "source": [
    "## Prediction Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "af208503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 177;\n",
       "                var nbb_unformatted_code = \"pred_thresh = 0.7\";\n",
       "                var nbb_formatted_code = \"pred_thresh = 0.7\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_thresh = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e9cebcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 178;\n",
       "                var nbb_unformatted_code = \"modified_predictions = [\\n    1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\\n]\\n\\npred_string = \\\"\\\".join(str(modified_predictions))\\ntrue_string = \\\"\\\".join(str(true_labels))\\n\\navg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\";\n",
       "                var nbb_formatted_code = \"modified_predictions = [\\n    1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\\n]\\n\\npred_string = \\\"\\\".join(str(modified_predictions))\\ntrue_string = \\\"\\\".join(str(true_labels))\\n\\navg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modified_predictions = [\n",
    "    1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\n",
    "]\n",
    "\n",
    "pred_string = \"\".join(str(modified_predictions))\n",
    "true_string = \"\".join(str(true_labels))\n",
    "\n",
    "avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6a2f65af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 179;\n",
       "                var nbb_unformatted_code = \"print(pred_string)\\nprint(true_string)\";\n",
       "                var nbb_formatted_code = \"print(pred_string)\\nprint(true_string)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pred_string)\n",
    "print(true_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b9dbb752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 5\n",
      "wd = 0.3219178082191781\n",
      "pk = 0.3082191780821918\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 180;\n",
       "                var nbb_unformatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_formatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wd_score = windowdiff(pred_string, true_string, avg_k)\n",
    "pk_score = pk(pred_string, true_string, avg_k)\n",
    "\n",
    "print(f\"k = {avg_k}\")\n",
    "print(f\"wd = {wd_score}\")\n",
    "print(f\"pk = {pk_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34585627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "462a8434",
   "metadata": {},
   "source": [
    "## KeyBERT Embedding Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d15e7648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 172;\n",
       "                var nbb_unformatted_code = \"curr = 230\\nprev = curr - 1\";\n",
       "                var nbb_formatted_code = \"curr = 230\\nprev = curr - 1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "curr = 230\n",
    "prev = curr - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8c6434ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the keywords in 0.6567 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "['cantonese', 'languages', 'vietnamese', 'communes']\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 205;\n",
       "                var nbb_unformatted_code = \"cohesion = coherence.get_coherence(\\n    [text_data[curr], text_data[prev]], coherence_threshold=0.25\\n)\\nprint([k[0] for k in cohesion])\";\n",
       "                var nbb_formatted_code = \"cohesion = coherence.get_coherence(\\n    [text_data[curr], text_data[prev]], coherence_threshold=0.25\\n)\\nprint([k[0] for k in cohesion])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cohesion = coherence.get_coherence(\n",
    "    [text_data[curr], text_data[prev]], coherence_threshold=0.25\n",
    ")\n",
    "print([k[0] for k in cohesion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "357c0021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 206;\n",
       "                var nbb_unformatted_code = \"# get the keywords for the current sentences\\nkeywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\\nkeywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\\n\\n# compute the word comparisons between the previous (with the coherence map)\\n# and the current (possibly the first sentence in a new segment)\\nword_comparisons_with_coherence, weights = compare_coherent_words(\\n    [keywords_prev], keywords_current\\n)\";\n",
       "                var nbb_formatted_code = \"# get the keywords for the current sentences\\nkeywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\\nkeywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\\n\\n# compute the word comparisons between the previous (with the coherence map)\\n# and the current (possibly the first sentence in a new segment)\\nword_comparisons_with_coherence, weights = compare_coherent_words(\\n    [keywords_prev], keywords_current\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the keywords for the current sentences\n",
    "keywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\n",
    "keywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\n",
    "\n",
    "# compute the word comparisons between the previous (with the coherence map)\n",
    "# and the current (possibly the first sentence in a new segment)\n",
    "word_comparisons_with_coherence, weights = compare_coherent_words(\n",
    "    [keywords_prev], keywords_current\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "dd52c9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('township', 0.2304),\n",
       "  ('communes', 0.1857),\n",
       "  ('hi', 0.1399),\n",
       "  ('wards', 0.1397),\n",
       "  ('ng', 0.1224)],\n",
       " [('cantonese', 0.5038),\n",
       "  ('mandarin', 0.464),\n",
       "  ('languages', 0.3483),\n",
       "  ('language', 0.343),\n",
       "  ('vietnamese', 0.3184)])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 207;\n",
       "                var nbb_unformatted_code = \"[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]\";\n",
       "                var nbb_formatted_code = \"[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121f953",
   "metadata": {},
   "source": [
    "# KeyBERT Embedding Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "559ab602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 679;\n",
       "                var nbb_unformatted_code = \"docs = [\\n        \\\"Hi my name is Devarsh\\\",\\n        \\\"Devarsh likes to play Basketball.\\\",\\n    \\\"I love to watch Cricket.\\\",\\n        \\\"I am a strong programmer. And my name is Devarsh\\\",\\n]\";\n",
       "                var nbb_formatted_code = \"docs = [\\n    \\\"Hi my name is Devarsh\\\",\\n    \\\"Devarsh likes to play Basketball.\\\",\\n    \\\"I love to watch Cricket.\\\",\\n    \\\"I am a strong programmer. And my name is Devarsh\\\",\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = [\n",
    "    \"Hi my name is Devarsh\",\n",
    "    \"Devarsh likes to play Basketball.\",\n",
    "    \"I love to watch Cricket.\",\n",
    "    \"I am a strong programmer. And my name is Devarsh\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "00458200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 680;\n",
       "                var nbb_unformatted_code = \"from keybert import KeyBERT\\n\\nkw_model = KeyBERT()\\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(\\n    docs, min_df=1, stop_words=\\\"english\\\"\\n)\\nkeywords = kw_model.extract_keywords(\\n    docs,\\n    min_df=1,\\n    stop_words=\\\"english\\\",\\n    doc_embeddings=doc_embeddings,\\n    word_embeddings=word_embeddings,\\n)\";\n",
       "                var nbb_formatted_code = \"from keybert import KeyBERT\\n\\nkw_model = KeyBERT()\\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(\\n    docs, min_df=1, stop_words=\\\"english\\\"\\n)\\nkeywords = kw_model.extract_keywords(\\n    docs,\\n    min_df=1,\\n    stop_words=\\\"english\\\",\\n    doc_embeddings=doc_embeddings,\\n    word_embeddings=word_embeddings,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "doc_embeddings, word_embeddings = kw_model.extract_embeddings(\n",
    "    docs, min_df=1, stop_words=\"english\"\n",
    ")\n",
    "keywords = kw_model.extract_keywords(\n",
    "    docs,\n",
    "    min_df=1,\n",
    "    stop_words=\"english\",\n",
    "    doc_embeddings=doc_embeddings,\n",
    "    word_embeddings=word_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "7d30bae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 681;\n",
       "                var nbb_unformatted_code = \"len(doc_embeddings)\";\n",
       "                var nbb_formatted_code = \"len(doc_embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "018ee52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 682;\n",
       "                var nbb_unformatted_code = \"len(word_embeddings)\";\n",
       "                var nbb_formatted_code = \"len(word_embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "80cbdc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('devarsh', 0.6267), ('hi', 0.5216)],\n",
       " [('devarsh', 0.6549),\n",
       "  ('basketball', 0.5558),\n",
       "  ('play', 0.3787),\n",
       "  ('likes', 0.2284)],\n",
       " [('cricket', 0.7118), ('watch', 0.3656), ('love', 0.307)],\n",
       " [('programmer', 0.5942), ('devarsh', 0.5528), ('strong', 0.3452)]]"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 683;\n",
       "                var nbb_unformatted_code = \"keywords\";\n",
       "                var nbb_formatted_code = \"keywords\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "fd1ac50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 701;\n",
       "                var nbb_unformatted_code = \"kw_model = KeyBERT()\\nimport torch\\n\\n\\ndef get_keywords_with_embeddings_test(\\n    data,\\n) -> list[tuple[str, float, torch.Tensor]]:\\n    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\\n\\n    keywords = kw_model.extract_keywords(\\n        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\\n    )\\n\\n    keywords_with_embeddings = []\\n    count = 0\\n    print(len(word_embeddings))\\n    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\\n        for j, words in enumerate(kw):\\n            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\\n            count += 1\\n\\n    return keywords_with_embeddings\";\n",
       "                var nbb_formatted_code = \"kw_model = KeyBERT()\\nimport torch\\n\\n\\ndef get_keywords_with_embeddings_test(\\n    data,\\n) -> list[tuple[str, float, torch.Tensor]]:\\n    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\\n\\n    keywords = kw_model.extract_keywords(\\n        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\\n    )\\n\\n    keywords_with_embeddings = []\\n    count = 0\\n    print(len(word_embeddings))\\n    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\\n        for j, words in enumerate(kw):\\n            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\\n            count += 1\\n\\n    return keywords_with_embeddings\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kw_model = KeyBERT()\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_keywords_with_embeddings_test(\n",
    "    data,\n",
    ") -> list[tuple[str, float, torch.Tensor]]:\n",
    "    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\n",
    "\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\n",
    "    )\n",
    "\n",
    "    keywords_with_embeddings = []\n",
    "    count = 0\n",
    "    print(len(word_embeddings))\n",
    "    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\n",
    "        for j, words in enumerate(kw):\n",
    "            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\n",
    "            count += 1\n",
    "\n",
    "    return keywords_with_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "d1bbf3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 702;\n",
       "                var nbb_unformatted_code = \"embeddings = get_keywords_with_embeddings_test(docs)\";\n",
       "                var nbb_formatted_code = \"embeddings = get_keywords_with_embeddings_test(docs)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = get_keywords_with_embeddings_test(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "f1ea7b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 703;\n",
       "                var nbb_unformatted_code = \"len(embeddings)\";\n",
       "                var nbb_formatted_code = \"len(embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883ef15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
