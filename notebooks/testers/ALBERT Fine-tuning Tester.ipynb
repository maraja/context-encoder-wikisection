{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b816077",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb492ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811f4ff7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7173540a0d98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext_encoder_bert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mContextEncoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mContextEncoderComplex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from src.encoders.context_encoder_bert import ContextEncoder\n",
    "\n",
    "from tensorflow.python import keras\n",
    "import toml\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from src.dataset.albert import AlbertDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457b646e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ALBERT_FINETUNE_SIMPLE': [{'bert_type': 'albert', 'dataset_type': 'clinical', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 200}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'clinical', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 200}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'clinical', 'pct_data': 1, 'augment_pct': 1, 'epochs': 200}, {'bert_type': 'albert', 'dataset_type': 'fiction', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 200}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'fiction', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 200}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'fiction', 'pct_data': 1, 'augment_pct': 1, 'epochs': 200}, {'bert_type': 'albert', 'dataset_type': 'wiki', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 200}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'wiki', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 200}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'wiki', 'pct_data': 1, 'augment_pct': 1, 'epochs': 200}], 'ALBERT_FINETUNE_COMPLEX': {'foo': 'bar'}, 'LDA_BERT_FINETUNE_SIMPLE': {'foo': 'bar'}, 'ALBERT_FINETUNE_TEST': [{'bert_type': 'albert', 'dataset_type': 'clinical', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 100}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'clinical', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 100}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'clinical', 'pct_data': 1, 'augment_pct': 1, 'epochs': 100}, {'bert_type': 'albert', 'dataset_type': 'fiction', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 100}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'fiction', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 100}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'fiction', 'pct_data': 1, 'augment_pct': 1, 'epochs': 100}, {'bert_type': 'albert', 'dataset_type': 'wiki', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 100}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'wiki', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 100}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'wiki', 'pct_data': 1, 'augment_pct': 1, 'epochs': 100}], 'DISTILBERT_FINETUNE_TEST': [{'bert_type': 'distilbert', 'dataset_type': 'clinical', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 100}, {'bert_type': 'distilbert', 'finetune_bert': True, 'dataset_type': 'clinical', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 100}, {'bert_type': 'distilbert', 'finetune_bert': True, 'dataset_type': 'clinical', 'pct_data': 1, 'augment_pct': 1, 'epochs': 100}, {'bert_type': 'distilbert', 'dataset_type': 'fiction', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 100}, {'bert_type': 'distilbert', 'finetune_bert': True, 'dataset_type': 'fiction', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 100}, {'bert_type': 'distilbert', 'finetune_bert': True, 'dataset_type': 'fiction', 'pct_data': 1, 'augment_pct': 1, 'epochs': 100}, {'bert_type': 'distilbert', 'dataset_type': 'wiki', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 100}, {'bert_type': 'distilbert', 'finetune_bert': True, 'dataset_type': 'wiki', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 100}, {'bert_type': 'distilbert', 'finetune_bert': True, 'dataset_type': 'wiki', 'pct_data': 1, 'augment_pct': 1, 'epochs': 100}]}\n"
     ]
    }
   ],
   "source": [
    "# Read local `config.toml` file.\n",
    "config = toml.load('../settings/experiments.toml')\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1491d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70d4ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pool_output(input_tensor):\n",
    "#     bert_full_output = tf.transpose(input_tensor, [0, 2, 1])\n",
    "#     bert_pooled_output = tf.reduce_mean(bert_full_output, 2)\n",
    "#     return bert_pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2296a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool_output(tf.constant([[[1,2,3,4], [5,6,7,8]], [[1,2,3,4], [5,6,7,8]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f409b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f45a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContextEncoder(final_dropout=0.5,\n",
    "                       dense_neurons=64,\n",
    "                       bert_trainable=True,\n",
    "                       bert_type=\"albert-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc6268d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_output = model(tf.constant([[[1,2]],[[3,4]],[[5,6]]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cca70c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.22578469]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30593b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"context_encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "albert (TFAlbertMainLayer)   multiple                  11683584  \n",
      "_________________________________________________________________\n",
      "dense_input_left (Dense)     multiple                  49216     \n",
      "_________________________________________________________________\n",
      "dense_input_mid (Dense)      multiple                  49216     \n",
      "_________________________________________________________________\n",
      "dense_input_right (Dense)    multiple                  49216     \n",
      "_________________________________________________________________\n",
      "dense_output (Dense)         multiple                  193       \n",
      "_________________________________________________________________\n",
      "final_dropout (Dropout)      multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,831,425\n",
      "Trainable params: 11,831,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5575373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f82d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AlbertDataset(dataset_type=\"clinical\",\n",
    "                       pct_data=0.001,\n",
    "                       max_segment_length=5,\n",
    "                       augment_pct=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0e7aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, tokenized_sentences, labels = dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f25364d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deb35b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(9, 256), dtype=int32, numpy=\n",
       " array([[    2,    53,    16, ...,     0,     0,     0],\n",
       "        [    2, 18084,    14, ...,     0,     0,     0],\n",
       "        [    2,   400,    19, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,   235,    31, ...,     0,     0,     0],\n",
       "        [    2,   273,   468, ...,     0,     0,     0],\n",
       "        [    2,    21,  1825, ...,     0,     0,     0]])>,\n",
       " <tf.Tensor: shape=(9, 256), dtype=int32, numpy=\n",
       " array([[    2, 18084,    14, ...,     0,     0,     0],\n",
       "        [    2,   400,    19, ...,     0,     0,     0],\n",
       "        [    2,   453,   133, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,   273,   468, ...,     0,     0,     0],\n",
       "        [    2,    21,  1825, ...,     0,     0,     0],\n",
       "        [    2,    53,    16, ...,     0,     0,     0]])>,\n",
       " <tf.Tensor: shape=(9, 256), dtype=int32, numpy=\n",
       " array([[    2,   400,    19, ...,     0,     0,     0],\n",
       "        [    2,   453,   133, ...,     0,     0,     0],\n",
       "        [    2,   469,   133, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,    21,  1825, ...,     0,     0,     0],\n",
       "        [    2,    53,    16, ...,     0,     0,     0],\n",
       "        [    2, 18084,    14, ...,     0,     0,     0]])>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.format_sentences_tri_input(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31d578f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b7411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bcd5721",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91574f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "021f90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "dense_output = 128\n",
    "\n",
    "# balanced = balanced binary crossentropy\n",
    "checkpoint_filepath = '../models/ALBERT/finetune/simple/{}-{}-{}-pct-{}-aug/checkpoint'.format(\n",
    "                        dataset.dataset_type,                    \n",
    "                        len(sentences), \n",
    "                        dataset.pct_data,\n",
    "                        dataset.augment_pct)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "#     early_stopping,\n",
    "    model_checkpoint_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0d0170c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models/ALBERT/finetune/simple/clinical-9-0.001-pct-0.001-aug/checkpoint'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d5e4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fb95835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint available.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "except:\n",
    "    print(\"No checkpoint available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7aedf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 42s 16s/step - loss: 0.9206 - accuracy: 0.5000 - val_loss: 0.0343 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.format_sentences_tri_input(tokenized_sentences), \n",
    "                    tf.convert_to_tensor(labels), \n",
    "                    epochs=EPOCHS,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    verbose=1, \n",
    "#                     class_weight=class_weight,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8edb58dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.8650528192520142],\n",
       " 'accuracy': [0.5],\n",
       " 'val_loss': [0.0343400277197361],\n",
       " 'val_accuracy': [1.0]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b480d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6ade9c3",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4a47d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from utils.experiments import get_experiments, save_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be0f1e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_type</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>finetune_bert</th>\n",
       "      <th>pct_data</th>\n",
       "      <th>augment_pct</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>clinical</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albert</td>\n",
       "      <td>clinical</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert</td>\n",
       "      <td>clinical</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>albert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>albert</td>\n",
       "      <td>wiki</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>albert</td>\n",
       "      <td>wiki</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>albert</td>\n",
       "      <td>wiki</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  bert_type dataset_type  finetune_bert  pct_data  augment_pct  epochs\n",
       "0    albert     clinical           True         1          0.1     200\n",
       "1    albert     clinical           True         1          0.5     200\n",
       "2    albert     clinical           True         1          1.0     200\n",
       "3    albert      fiction           True         1          0.1     200\n",
       "4    albert      fiction           True         1          0.5     200\n",
       "5    albert      fiction           True         1          1.0     200\n",
       "6    albert         wiki           True         1          0.1     200\n",
       "7    albert         wiki           True         1          0.5     200\n",
       "8    albert         wiki           True         1          1.0     200"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read local `config.toml` file.\n",
    "config = get_experiments('ALBERT_FINETUNE_SIMPLE')\n",
    "config_df = pd.DataFrame.from_dict(config)\n",
    "config_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4e7f80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_df.to_csv(r'../models/experiment.csv', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a65dfad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bcebcc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for experiment in config:\n",
    "    bert_type = config['bert_type']\n",
    "    dataset_type = config['dataset_type']\n",
    "    finetune_bert = config['finetune_bert']\n",
    "    pct_data = config['pct_data']\n",
    "    augment_pct = config['augment_pct']\n",
    "    epochs = config['epochs']\n",
    "    print(\"params:\", bert_type, dataset_type, finetune_bert, pct_data, augment_pct, epochs)\n",
    "    \n",
    "    # init model\n",
    "    print(\"initializing model...\")\n",
    "    model = ContextEncoder(final_dropout=0.5,\n",
    "                           dense_neurons=64,\n",
    "                           bert_trainable=finetune_bert,\n",
    "                           bert_type=\"albert-base-v2\")\n",
    "    \n",
    "    # init dataset\n",
    "    print(\"initializing dataset...\")\n",
    "    dataset = AlbertDataset(dataset_type=dataset_type,\n",
    "                           pct_data=pct_data,\n",
    "                           max_segment_length=5,\n",
    "                           augment_pct=augment_pct)\n",
    "    \n",
    "    # process dataset\n",
    "    print(\"processing dataset...\")\n",
    "    sentences, tokenized_sentences, labels = dataset.process()\n",
    "    \n",
    "    # create checkpoint path\n",
    "    checkpoint_filepath = '../models/ALBERT/finetune/simple/{}-{}-{}-pct-{}-aug/checkpoint'.format(\n",
    "                            dataset_type,                    \n",
    "                            len(sentences), \n",
    "                            pct_data,\n",
    "                            augment_pct)\n",
    "    print(checkpoint_filepath)\n",
    "    \n",
    "    # compiling model\n",
    "    print(\"compiling the model...\")\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=METRICS)\n",
    "    \n",
    "    try:\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "        print(\"model loaded.\")\n",
    "    except:\n",
    "        print(\"No checkpoint available.\")\n",
    "    \n",
    "    # \n",
    "    print(\"starting the training process...\")\n",
    "    history = model.fit(dataset.format_sentences_tri_input(tokenized_sentences), \n",
    "                        tf.convert_to_tensor(labels), \n",
    "                        epochs=EPOCHS,\n",
    "                        validation_split=0.1,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        verbose=1, \n",
    "                        # class_weight=class_weight,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    # assigning history to experiment object for saving.\n",
    "    experiment[\"history\"] = history\n",
    "    \n",
    "    print(\"saving results...\")\n",
    "    save_results(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323dd2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd] *",
   "language": "python",
   "name": "conda-env-phd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
